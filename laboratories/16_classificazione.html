

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Classificazione &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'laboratories/16_classificazione';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../lectures/index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../lectures/index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_setup.html">1. Introduzione ai laboratori e Installazione dell‚ÄôAmbiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_intro_python.html">2. Introduzione a Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_intro_numpy.html">3. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_intro_matplotlib.html">4. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_intro_pandas.html">5. Introduzione a Pandas</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/laboratories/16_classificazione.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flaboratories/16_classificazione.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/laboratories/16_classificazione.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classificazione</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misure-di-valutazione">Misure di Valutazione</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-e-testing-del-classificatore">Training e testing del classificatore</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#misure-di-valutazione-degli-errori">Misure di valutazione degli errori</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hit-true-positive-e-miss-false-negative">Hit (True Positive) e Miss (False Negative)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#errori-di-tipo-i-o-falsi-allarmi-false-positive">Errori di tipo I o falsi allarmi (False Positive)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#correct-rejection-true-negative">Correct Rejection (True Negative)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#riepilogo-tp-tn-fp-fn">Riepilogo: TP, TN, FP, FN</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrice-di-confusione">Matrice di Confusione</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizzazione-della-matrice-di-confusione-tnr-tpr-fnr-fpr">Normalizzazione della matrice di confusione - TNR, TPR, FNR, FPR</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-recall-e-f-measure">Precision, Recall e F-Measure</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy">Accuracy</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uso-delle-diverse-misure-di-performance">1.3 Uso delle diverse misure di performance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classificatore-sempre-positivo">Classificatore sempre positivo</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classificatore-sempre-negativo">Classificatore sempre negativo</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-e-sbilanciamento-delle-classi">Accuracy e sbilanciamento delle classi</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nota-sulle-diverse-misure-di-valutazione-di-performance">Nota sulle diverse misure di valutazione di performance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#errore-di-training-e-errore-di-generalizzazione">Errore di training e errore di generalizzazione</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classificazione-basata-su-soglia-e-curve-roc">Classificazione Basata su Soglia e Curve ROC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classificazione-nearest-neighbor-k-nn">Classificazione Nearest Neighbor (K-NN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trovare-il-valore-ottimale-di-k">Trovare il valore ottimale di K</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminante-lineare-di-fisher">Discriminante Lineare di Fisher</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#esempio-breast-cancer-dataset">Esempio: Breast Cancer Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretazione-geometrica-dei-coefficienti">Interpretazione geometrica dei coefficienti</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-discriminant-analysis">Linear Discriminant Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#riduzione-della-dimensionalita-con-lda">Riduzione della Dimensionalit√† con LDA</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classificazione-maximum-a-posteriori-map">Classificazione Maximum a Posteriori (MAP)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classificatore-basato-su-distanza-di-mahalanobis">Classificatore Basato su Distanza di Mahalanobis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementazione-map-di-scikit-learn">Implementazione MAP di Scikit-Learn</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#esercizi">Esercizi</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="classificazione">
<h1>Classificazione<a class="headerlink" href="#classificazione" title="Permalink to this heading">#</a></h1>
<section id="misure-di-valutazione">
<h2>Misure di Valutazione<a class="headerlink" href="#misure-di-valutazione" title="Permalink to this heading">#</a></h2>
<p>Finora abbiamo utilizzato regressori lineari e logistici per studiare le relazioni tra le variabili di un dataset. In linea di principio, una volta calcolati, questi due strumenti possono essere utilizzati per fare predizioni su dati mai visti prima. Ad esempio, date nuove misurazioni di tessuto, si potrebbe utilizzare il regressore logistico calcolato precedentemente per determinare in maniera automatica qual √® la probabilit√† che il tessuto contenga un tumore maligno.</p>
<p>Le misure di valutazione della bont√† dei regressori viste finora tuttavia, non fanno uso di dati ‚Äúmai visti‚Äù in fase di costruzione del modello. Pertanto, potrebbe accadere che il modello si comporti bene sui dati utilizzati per costruirlo e che poi abbia performance peggiori su dati nuovi. Questo fenomeno √® detto <strong>overfitting</strong> in quanto il modello si specializza ‚Äútroppo‚Äù sul set di dati utilizzato per calcolarlo. Quando il modello funziona bene anche su dati nuovi, si parla invece di <strong>generalizzazione</strong>.</p>
<p>Caricheremo il dataset <code class="docutils literal notranslate"><span class="pre">biopsy</span></code> come dataset di esempio:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.datasets</span> <span class="kn">import</span> <span class="n">get_rdataset</span>
<span class="n">biopsy</span> <span class="o">=</span> <span class="n">get_rdataset</span><span class="p">(</span><span class="s1">&#39;biopsy&#39;</span><span class="p">,</span><span class="n">package</span><span class="o">=</span><span class="s1">&#39;MASS&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Definiamo una nuova variabile <code class="docutils literal notranslate"><span class="pre">cl</span></code> come visto negli scorsi laboratori per poter calcolare un regressore logistico in seguito:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;cl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s1">&#39;benign&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;malignant&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<section id="training-e-testing-del-classificatore">
<h3>Training e testing del classificatore<a class="headerlink" href="#training-e-testing-del-classificatore" title="Permalink to this heading">#</a></h3>
<p>Per verificare quanto il modello √® generale e utilizzabile per fare predizioni su dati nuovi, in machine learning di suddivide il dataset in due parti:</p>
<ul class="simple">
<li><p>Training set: verr√† utilizzato per ‚Äúallenare‚Äù il modello. Nel caso di un regressore logistico o lineare, questa fase di training, o allenamento, consiste nel calcolo dei parametri del modello;</p></li>
<li><p>Testing set: verr√† utilizzato per misurare le performance del modello. Si tratta del set di dati ‚Äúnuovi‚Äù e non va mai utilizzato per calcolare i parametri del modello.</p></li>
</ul>
<p>Se il modello funziona bene sui dati di test dopo essere stato allenato sui dati di training, possiamo ipotizzare che esso funzioner√† bene anche nuovi dati mai visti prima.</p>
<p>La suddivisione dei dati in insiemi di training e testing viene in genere fatta in maniera casuale. Si tende inoltre a includere pi√π campioni nel training set. Possiamo utilizzare la funzione <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> della libreria a tale scopo. Effettuiamo uno split in cui il <span class="math notranslate nohighlight">\(25\%\)</span> dei dati √® incluso nel testing set, mentre il restante <span class="math notranslate nohighlight">\(75\%\)</span> √® incluso nel training set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1">#impostiamo un seed in modo che lo split sia ripetibile</span>
<span class="c1">#generalmente non √® necessario (n√© consigliabile!) farlo</span>
<span class="c1">#qui lo facciamo per questioni didattiche</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">biopsy_train</span><span class="p">,</span> <span class="n">biopsy_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">biopsy</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">biopsy_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">biopsy_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>524
175
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 1</strong></p>
<p>Perch√© si suddividono i dati in due set asimmetrici, scegliendo un numero maggiore di elementi di training? Quali sono i rischi di un test set troppo piccolo?</p>
</div></blockquote>
<p>Si procede dunque con la fase di training. Nel nostro caso, calcoleremo un regressore logistico utilizzando solo i dati di training. Usiamo il set di variabili ottimale individuato negli scorsi laboratori:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">logit</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="s1">&#39;cl ~ V1 + V3 + V4 + V6 + V7 + V8&#39;</span><span class="p">,</span><span class="n">biopsy_train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.084918
         Iterations 9
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>cl</td>        <th>  No. Observations:  </th>   <td>   509</td>  
</tr>
<tr>
  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   502</td>  
</tr>
<tr>
  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     6</td>  
</tr>
<tr>
  <th>Date:</th>          <td>Thu, 29 Nov 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.8680</td>  
</tr>
<tr>
  <th>Time:</th>              <td>12:08:23</td>     <th>  Log-Likelihood:    </th>  <td> -43.223</td> 
</tr>
<tr>
  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -327.56</td> 
</tr>
<tr>
  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.333e-119</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -9.9977</td> <td>    1.297</td> <td>   -7.708</td> <td> 0.000</td> <td>  -12.540</td> <td>   -7.455</td>
</tr>
<tr>
  <th>V1</th>        <td>    0.6750</td> <td>    0.154</td> <td>    4.384</td> <td> 0.000</td> <td>    0.373</td> <td>    0.977</td>
</tr>
<tr>
  <th>V3</th>        <td>    0.2576</td> <td>    0.177</td> <td>    1.457</td> <td> 0.145</td> <td>   -0.089</td> <td>    0.604</td>
</tr>
<tr>
  <th>V4</th>        <td>    0.4870</td> <td>    0.149</td> <td>    3.266</td> <td> 0.001</td> <td>    0.195</td> <td>    0.779</td>
</tr>
<tr>
  <th>V6</th>        <td>    0.3201</td> <td>    0.098</td> <td>    3.251</td> <td> 0.001</td> <td>    0.127</td> <td>    0.513</td>
</tr>
<tr>
  <th>V7</th>        <td>    0.4037</td> <td>    0.170</td> <td>    2.370</td> <td> 0.018</td> <td>    0.070</td> <td>    0.738</td>
</tr>
<tr>
  <th>V8</th>        <td>    0.3276</td> <td>    0.134</td> <td>    2.442</td> <td> 0.015</td> <td>    0.065</td> <td>    0.590</td>
</tr>
</table></div></div>
</div>
<p>Il coefficiente di <code class="docutils literal notranslate"><span class="pre">V3</span></code> non √® pi√π statisticamente rilevante (il set di dati √® cambiato rispetto a prima). Rimuoviamo la variabile:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="s1">&#39;cl ~ V1 + V4 + V6 + V7 + V8&#39;</span><span class="p">,</span><span class="n">biopsy_train</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.087157
         Iterations 10
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>cl</td>        <th>  No. Observations:  </th>   <td>   509</td>  
</tr>
<tr>
  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   503</td>  
</tr>
<tr>
  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>  
</tr>
<tr>
  <th>Date:</th>          <td>Thu, 29 Nov 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.8646</td>  
</tr>
<tr>
  <th>Time:</th>              <td>12:08:23</td>     <th>  Log-Likelihood:    </th>  <td> -44.363</td> 
</tr>
<tr>
  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -327.56</td> 
</tr>
<tr>
  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>3.690e-120</td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>  -10.3177</td> <td>    1.308</td> <td>   -7.885</td> <td> 0.000</td> <td>  -12.882</td> <td>   -7.753</td>
</tr>
<tr>
  <th>V1</th>        <td>    0.7604</td> <td>    0.149</td> <td>    5.116</td> <td> 0.000</td> <td>    0.469</td> <td>    1.052</td>
</tr>
<tr>
  <th>V4</th>        <td>    0.5434</td> <td>    0.147</td> <td>    3.691</td> <td> 0.000</td> <td>    0.255</td> <td>    0.832</td>
</tr>
<tr>
  <th>V6</th>        <td>    0.3649</td> <td>    0.094</td> <td>    3.887</td> <td> 0.000</td> <td>    0.181</td> <td>    0.549</td>
</tr>
<tr>
  <th>V7</th>        <td>    0.4723</td> <td>    0.158</td> <td>    2.988</td> <td> 0.003</td> <td>    0.163</td> <td>    0.782</td>
</tr>
<tr>
  <th>V8</th>        <td>    0.4147</td> <td>    0.120</td> <td>    3.445</td> <td> 0.001</td> <td>    0.179</td> <td>    0.651</td>
</tr>
</table></div></div>
</div>
<p>Tutti i coefficienti sono adesso statisticamente rilevanti.</p>
<p>Una volta calcolato il classificatore sul training set, possiamo ottenere le predizioni sul test set come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">biopsy_test</span><span class="o">.</span><span class="n">dropna</span><span class="p">())</span> <span class="c1">#applichiamo dropna per evitare i NaN</span>
<span class="n">test_probs</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>341    0.001093
532    0.001093
687    0.007106
83     0.045540
428    0.000682
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Il regressore logistico predice delle probabilit√†. Per ottenre le classi predette (<span class="math notranslate nohighlight">\(0\)</span> o <span class="math notranslate nohighlight">\(1\)</span>), possiamo arrotondare all‚Äôintero pi√π vicino come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_preds</span> <span class="o">=</span> <span class="n">test_probs</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">test_preds</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>341    0
532    0
687    0
83     0
428    0
dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Per valutare la bont√† del classificatore, dobbiamo confrontare le classi predette a quelle reali, che vengono in genere dette ‚Äúclassi di ground truth‚Äù (gt). Recuperiamo queste classi dal dataset originale:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_gt</span> <span class="o">=</span> <span class="n">biopsy_test</span><span class="o">.</span><span class="n">dropna</span><span class="p">()[</span><span class="s1">&#39;cl&#39;</span><span class="p">]</span>
<span class="n">test_gt</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>341    0
532    0
687    0
83     0
428    0
Name: cl, dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="misure-di-valutazione-degli-errori">
<h3>Misure di valutazione degli errori<a class="headerlink" href="#misure-di-valutazione-degli-errori" title="Permalink to this heading">#</a></h3>
<p>Abbiamo detto che per valutare quanto il regressore logistico sia ‚Äúbuono‚Äù, possiamo confrontare le etichette predette (<code class="docutils literal notranslate"><span class="pre">train_preds</span></code>) con quelle corrette (<code class="docutils literal notranslate"><span class="pre">train_gt</span></code>) contando ‚Äúquanti errori‚Äù vengono fatti dal modello. Nello specifico, esistono diversi modi per misurare le performance di un classificatore. In questo laboratorio vedremo:</p>
<ul class="simple">
<li><p>True Positive (o Hit);</p></li>
<li><p>False Negative (o Miss);</p></li>
<li><p>False Positive (o Errore di tipo I o falsi allarmi);</p></li>
<li><p>True Negative (o correct rejection);</p></li>
<li><p>Matrice di confusione e sua normalizzazione;</p></li>
<li><p>Precision;</p></li>
<li><p>Recall;</p></li>
<li><p><span class="math notranslate nohighlight">\(F_1\)</span> score;</p></li>
<li><p>Accuracy.</p></li>
</ul>
<section id="hit-true-positive-e-miss-false-negative">
<h4>Hit (True Positive) e Miss (False Negative)<a class="headerlink" href="#hit-true-positive-e-miss-false-negative" title="Permalink to this heading">#</a></h4>
<p>Le pi√π semplici misure di performance di un classificatore sono hit e miss. Una predizione pu√≤ essere classificata come un ‚Äúhit‚Äù o un ‚Äúmiss‚Äù come segue:</p>
<ul class="simple">
<li><p><strong>hit</strong>: se l‚Äôelemento √® stato classificato come positivo (variabile dipendente pari a <span class="math notranslate nohighlight">\(1\)</span>) ed era effettivamente positivo. Si dice anche che l‚Äôelemento √® un <strong>True Positive (TP)</strong>, ovvero un ‚Äúvero positivo‚Äù;</p></li>
<li><p><strong>miss</strong>: se l‚Äôelemento √® stato classificato come negativo (variabile dipendente pari a <span class="math notranslate nohighlight">\(0\)</span>), ma era in realt√† un positivo. L‚Äôelemento viene anche detto <strong>False Negative (FN)</strong>, ovvero ‚Äúfaslo negativo‚Äù. Equivalentemente si parla anche di <strong>errore di tipo II</strong>.</p></li>
</ul>
<p>Vediamo come classificare ogni predizione:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#np.logical_and mette in and due array di booleani</span>
<span class="n">hit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">test_gt</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_preds</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
<span class="n">miss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">test_gt</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_preds</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hit</span><span class="o">.</span><span class="n">head</span><span class="p">(),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">miss</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>341    False
532    False
687    False
83     False
428    False
Name: cl, dtype: bool 

341    False
532    False
687    False
83     False
428    False
Name: cl, dtype: bool
</pre></div>
</div>
</div>
</div>
<p>Possiamo contare i valori di hit e miss effettuando la somma dei valori (i valori <code class="docutils literal notranslate"><span class="pre">True</span></code> verranno considerati come <span class="math notranslate nohighlight">\(1\)</span>, mentre i <code class="docutils literal notranslate"><span class="pre">False</span></code> come <span class="math notranslate nohighlight">\(0\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di hit:&quot;</span><span class="p">,</span><span class="n">hit</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di miss:&quot;</span><span class="p">,</span><span class="n">miss</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numero di hit: 61
Numero di miss: 3
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 2</strong></p>
<p>Cosa possiamo dire del classificatore guardando al numero di hit e miss? I numeri ottenuti vanno rapportati alla dimensione del test set o possono essere interpretati in maniera indipendente da essa?</p>
</div></blockquote>
</section>
<section id="errori-di-tipo-i-o-falsi-allarmi-false-positive">
<h4>Errori di tipo I o falsi allarmi (False Positive)<a class="headerlink" href="#errori-di-tipo-i-o-falsi-allarmi-false-positive" title="Permalink to this heading">#</a></h4>
<p>Si parla di errori di tipo I o di falsi allarmi per tutti gli elementi che vengono classificati come positivi, ma erano in realt√† dei negativi. In questo caso, si parla anche di <strong>False Positive (FP)</strong>, ovvero ‚Äúfalsi positivi‚Äù. Calcoliamo il numero di falsi allarmi:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">false_alarms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">test_gt</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_preds</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di falsi allarmi:&quot;</span><span class="p">,</span><span class="n">false_alarms</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numero di falsi allarmi: 2
</pre></div>
</div>
</div>
</div>
</section>
<section id="correct-rejection-true-negative">
<h4>Correct Rejection (True Negative)<a class="headerlink" href="#correct-rejection-true-negative" title="Permalink to this heading">#</a></h4>
<p>Quando un elemento √® stato classificato come negativo ed era effettivamente un negativo si parla di <strong>True Negative (TN)</strong>, ovvero ‚ÄúVeri Negativi‚Äù o, alternativamente, di ‚Äúcorrect rejection‚Äù. Vediamo come calcolare il numero di veri negativi:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_negatives</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">test_gt</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_preds</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di veri negativi:&quot;</span><span class="p">,</span><span class="n">true_negatives</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numero di veri negativi: 108
</pre></div>
</div>
</div>
</div>
</section>
<section id="riepilogo-tp-tn-fp-fn">
<h4>Riepilogo: TP, TN, FP, FN<a class="headerlink" href="#riepilogo-tp-tn-fp-fn" title="Permalink to this heading">#</a></h4>
<p>Riepilogando, ogni elemento pu√≤ essere considerato come:</p>
<ul class="simple">
<li><p><strong>True Positive (TP)</strong>: se √® stato classificato come positivo ed era effettivamente un positivo;</p></li>
<li><p><strong>True Negative (TN)</strong>: se √® stato classificato come negativo ed era effettivamente un negativo;</p></li>
<li><p><strong>False Positive (FP)</strong>: se √® stato classificato come positivo ma era un negativo;</p></li>
<li><p><strong>False Negative (FN)</strong>: se √® stato classificato come negativo ma era un positivo.</p></li>
</ul>
<p>Una volta categorizzata ogni predizione in una delle quattro classi, le perfomance del classificatore possono essere riassunte contando il numero di <strong>TP</strong>, <strong>TN</strong>, <strong>FP</strong>, <strong>FN</strong>. I primi due numeri contano le classificazioni corrette, mentre gli ultimi due contano gli errori. Per un buon classificatore, ci aspettiamo dunque di ottenere un numero alto di <strong>TP</strong> e <strong>TN</strong> e numeri bassi di <strong>FP</strong> e <strong>FN</strong>.</p>
</section>
</section>
<section id="matrice-di-confusione">
<h3>Matrice di Confusione<a class="headerlink" href="#matrice-di-confusione" title="Permalink to this heading">#</a></h3>
<p>Una matrice di confusione riassume i quattro numeri appena visti in forma tabulare. La matrice di confusione per un classificatore su <span class="math notranslate nohighlight">\(N\)</span> classi pu√≤ essere definita come una matrice <span class="math notranslate nohighlight">\(C\)</span> di dimensioni <span class="math notranslate nohighlight">\(N \times N\)</span> in cui l‚Äôelemento generale <span class="math notranslate nohighlight">\(C_{ij}\)</span> √® pari al numero di osservazioni appartenenti alla classe <span class="math notranslate nohighlight">\(i\)</span> e predetti come appartenenti alla classe <span class="math notranslate nohighlight">\(j\)</span>. Considerato che la classe negativa ha indice <span class="math notranslate nohighlight">\(0\)</span> e la classe positiva ha indice <span class="math notranslate nohighlight">\(1\)</span>, nel caso della classificazione binaria, la matrice si presenta come segue:</p>
<br>
<center>
<table class='tab'>
    <tr></tr>
    <tr><td></td><th colspan=2>Predetti</th></tr>
<tr></tr>
<tr>
    <td rowspan=3><b>Reali</b></td>
<td>TN</td><td>FP</td>
</tr>
    <tr></tr>
<tr>
<td>FN</td><td>TP</td>
</tr>
</table>
</center>
<p>Dove TN, FP, FN e TP denotato il numero di True Negative, False Positive, False Negative e True Positive.</p>
<p><strong>NOTA</strong>: esistono diversi modi per definire una matrice di confusione. Noi ci rifacciamo al metodo usato da <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> per comodit√†. Pu√≤ capitare tuttavia di vedere matrici di confusione definite in maniera trasposta (valori predetti lungo le righe e reali lungo le colonne) e nel caso binario, pu√≤ capitare di avere i valori riordinati come TP, FN, FP, TN. Si noti che tutte queste rappresentazioni sono equivalenti. Nei laboratori considereremo sempre la matrice di confusione nella formulazione qui affrontata.</p>
<p>Possiamo calcolare la matrice di confusione mediante la funzione <code class="docutils literal notranslate"><span class="pre">confusion_matrix</span></code> della libreria <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[108,   2],
       [  3,  61]])
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 3</strong></p>
<p>Si studi la matrice di confusione. I numeri ottenuti corrispondono ai valori di TN, FP, FN, TP calcolati a mano in precedenza?</p>
</div></blockquote>
<p>Possiamo ottenere i valori separati di TN, FP, FN e TP semplicemente come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di True Negative:&quot;</span><span class="p">,</span><span class="n">tn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di False Positive:&quot;</span><span class="p">,</span><span class="n">fp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di False Negative:&quot;</span><span class="p">,</span><span class="n">fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di Ture Positive:&quot;</span><span class="p">,</span><span class="n">tp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numero di True Negative: 108
Numero di False Positive: 2
Numero di False Negative: 3
Numero di Ture Positive: 61
</pre></div>
</div>
</div>
</div>
<section id="normalizzazione-della-matrice-di-confusione-tnr-tpr-fnr-fpr">
<h4>Normalizzazione della matrice di confusione - TNR, TPR, FNR, FPR<a class="headerlink" href="#normalizzazione-della-matrice-di-confusione-tnr-tpr-fnr-fpr" title="Permalink to this heading">#</a></h4>
<p>La matrice di confusione riporta il numero di errori e predizioni corrette. Questi numeri vanno interpretati considerando la numerosit√† delle classi (un TP pari a <span class="math notranslate nohighlight">\(50\)</span> va interpretato in maniera diversa se sono presenti <span class="math notranslate nohighlight">\(70\)</span> o <span class="math notranslate nohighlight">\(1000\)</span> elementi in quella classe) e possono essere difficili da leggere quando le classi non sono perfettamente bilanciate. Per ovviare a questi problemi, √® possibile normalizzare la matrice per righe, ovvero dividere ogni elemento <span class="math notranslate nohighlight">\(C_{ij}\)</span> per <span class="math notranslate nohighlight">\(C_i=\sum_j{C_{ij}}\)</span>.</p>
<p>Si noti che la somma dei valori lungo una riga della matrice di confusione indica il numero di elementi appartenenti a quella classe:</p>
<br>
<center>
<table class='tab'>
    <tr></tr>
    <tr><td></td><th colspan=2>Predetti</th><th>Somma Riga</th></tr>
<tr></tr>
<tr>
    <td rowspan=3><b>Reali</b></td>
    <td>TN</td><td>FP</td><td>N = TN+FP</td>
</tr>
    <tr></tr>
<tr>
<td>FN</td><td>TP</td><td>P = TP+FN</td>
</tr>
</table>
</center>
<p>Dopo la normalizzazione otteniamo dunque:</p>
<br>
<center>
<table class='tab'>
    <tr></tr>
    <tr><td></td><th colspan=2>Predetti</th></tr>
<tr></tr>
<tr>
    <td rowspan=3><b>Reali</b></td>
    <td>TN/N</td><td>FP/N</td>
</tr>
    <tr></tr>
<tr>
<td>FN/P</td><td>TP/P</td>
</tr>
</table>
</center><p>I numeri nella matrice di confusione normalizzata hanno nomi e interpretazioni specifiche:</p>
<ul class="simple">
<li><p><strong>True Negative Rate</strong>: definito come <span class="math notranslate nohighlight">\(TNR=\frac{TN}{N}\)</span>, indica la frazione di elementi negativi che sono stati effettivamente classificati come negativi;</p></li>
<li><p><strong>True Positive Rate</strong>: definito come <span class="math notranslate nohighlight">\(TPR=\frac{TP}{P}\)</span>, indica la frazione di elementi positivi che sono stati effettivamente classificati come positivi;</p></li>
<li><p><strong>False Positive Rate</strong>: definito come <span class="math notranslate nohighlight">\(FPR=\frac{FP}{N}\)</span>, indica la frazione di elementi negativi che sono stati erroneamente classificati come positivi;</p></li>
<li><p><strong>False Negative Rate</strong>: definito come <span class="math notranslate nohighlight">\(FNR=\frac{FN}{P}\)</span>, indica la frazione di elementi positivi che sono stati erroneamente classificati come negativi.</p></li>
</ul>
<p>Possiamo infine vedere la matrice di confusione normalizzata come segue:</p>
<br>
<center>
<table class='tab'>
    <tr></tr>
    <tr><td></td><th colspan=2>Predetti</th></tr>
<tr></tr>
<tr>
    <td rowspan=3><b>Reali</b></td>
    <td>TNR</td><td>FPR</td>
</tr>
    <tr></tr>
<tr>
<td>FNR</td><td>TPR</td>
</tr>
</table>
</center>
<p>La matrice di confusione normalizzata viene calcolata come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="c1">#il reshape serve a trasformare il vettore in un vettore colonna</span>
<span class="n">cm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.98181818, 0.01818182],
       [0.046875  , 0.953125  ]])
</pre></div>
</div>
</div>
</div>
<p>Analogamente a quanto vista prima, possiamo ottenere TNR, FNR, FPR e TPR come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tnr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TNR:&quot;</span><span class="p">,</span><span class="n">tnr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FPR:&quot;</span><span class="p">,</span><span class="n">fpr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FNR:&quot;</span><span class="p">,</span><span class="n">fnr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TPR:&quot;</span><span class="p">,</span><span class="n">tpr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TNR: 0.9818181818181818
FPR: 0.01818181818181818
FNR: 0.046875
TPR: 0.953125
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 4</strong></p>
<p>Si confronti la matrice di confusione normalizzata con quella non normalizzata. Quale delle due offre un quadro pi√π chiaro?</p>
</div></blockquote>
<p>Si noti che, per valutare le performance di un classificatore, √® necessario analizzare tutti i e 4 i numeri appena visti. Esistono infatti dei casi in cui osservare solo alcuni indicatori fornisce un quadro parziale della situazione. Vediamo qualche esempio.</p>
</section>
<section id="precision-recall-e-f-measure">
<h4>Precision, Recall e F-Measure<a class="headerlink" href="#precision-recall-e-f-measure" title="Permalink to this heading">#</a></h4>
<p>Altre due importanti misure di performance sono <strong>precision</strong> e <strong>recall</strong>. Si tratta di due misure complementari, definibili come segue:</p>
<ul class="simple">
<li><p>La <strong>precision</strong> misura <strong>quanti dei valori predetti come positivi sono in effetti positivi</strong>. Si misura come segue: <span class="math notranslate nohighlight">\(Precision = \frac{TP}{TP+FP}\)</span>;</p></li>
<li><p>La <strong>recall</strong> misura <strong>quanti tra gli elementi effettivamente positivi, sono stati individuati dal classificatore</strong>. SI misura come segue: <span class="math notranslate nohighlight">\(recall=\frac{TP}{TP+FN}=TPR\)</span> (la recall √® il <span class="math notranslate nohighlight">\(TPR\)</span> sono la stessa cosa).</p></li>
</ul>
<p>Le due misure sono complementari. Consideriamo due classificatore sbilanciati:</p>
<ul class="simple">
<li><p>Un classificatore <span class="math notranslate nohighlight">\(C_1\)</span> che classifica un elemento come positivo solo quando ne √® assolutamente certo. Tutti i positivi predetti da questo classificatore saranno corretti, ma molti positivi saranno classificati come negativi;</p></li>
<li><p>Un classificatore <span class="math notranslate nohighlight">\(C_2\)</span> che classifica tutti gli elementi come positivi e non prevede mai che un elemento sia negativo.</p></li>
</ul>
<p>Il classificatore <span class="math notranslate nohighlight">\(C_1\)</span> avr√† precision alta in quanto ci sono pochissimi falsi positivi, ma recall bassa in quanto ci sono molti falsi negativi. Il classificatore <span class="math notranslate nohighlight">\(C_2\)</span> avr√† precision bassa per via della presenza di molti falsi positivi, mentre avr√† recall pari a uno per l‚Äôassenza di falsi negativi.</p>
<p>Vista questa complementariet√†, <strong>precision</strong> e <strong>recall</strong> vanno <strong>sempre guardate insieme</strong>. Possiamo calcolare precision e racall come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision: </span><span class="si">{:0.2f}</span><span class="s2">, Recall: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span><span class="n">recall</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Precision: 0.97, Recall: 0.95
</pre></div>
</div>
</div>
</div>
<p>Un modo per guardare a precision e recall insieme, consiste nel calcolare lo score <span class="math notranslate nohighlight">\(F_1\)</span>, che si definisce come la media armonica tra precision e recall:</p>
<div class="math notranslate nohighlight">
\[F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}\]</div>
<p>Lo score <span class="math notranslate nohighlight">\(F_1\)</span> pu√≤ essere calcolato come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1 score: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F1 score: 0.96
</pre></div>
</div>
</div>
</div>
</section>
<section id="accuracy">
<h4>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this heading">#</a></h4>
<p>Abbiamo visto che per valutare bene un classificatore, dobbiamo guardare alla matrice di confusione nella sua interezza. Una misura molto semplice che ci permette di ottenere un unico numero da guardare per valutare un classificatore √® l‚Äôaccuracy. L‚Äôaccuracy misura la frazione di campioni che sono stati correttamente classificati. Essa si calcola come segue:</p>
<div class="math notranslate nohighlight">
\[Accuracy = \frac{TP+TN}{TP+TN+FP+FN}\]</div>
<p>Sommando TP e TN otteniamo il numero di elementi classificati correttamente (si noti che ci√≤ corrisponde a sommare tutti gli elementi sulla diagonale principale della matrice di confusione non normalizzata). Sommando TP, TN, FP e FN otteniamo il numero totale degli elementi contenuti nel dataset.</p>
<p>Possiamo calcolare l‚Äôaccuracy sommando gli elementi sulla diagonale principale della matrice di confusione non normalizzata:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9712643678160919
</pre></div>
</div>
</div>
</div>
<p>Alternativamente, possiamo calcolare l‚Äôaccuracy utilizzando la funzione <code class="docutils literal notranslate"><span class="pre">accuracy_score</span></code> di  <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9712643678160919
</pre></div>
</div>
</div>
</div>
<p>Questo numero viene spesso interpretato come una percentuale: ‚Äúil <span class="math notranslate nohighlight">\(97\%\)</span> dei campioni sono stati correttamente classificati‚Äù.</p>
</section>
</section>
<section id="uso-delle-diverse-misure-di-performance">
<h3>1.3 Uso delle diverse misure di performance<a class="headerlink" href="#uso-delle-diverse-misure-di-performance" title="Permalink to this heading">#</a></h3>
<p>Abbiamo visto che esistono diverse misure di valutazione. In pratica, √® spesso conveniente osservare pi√π di una misura alla volta per non farci trarre in inganno. Vediamo qualche esempio con dati ‚Äúinventati‚Äù per illustrare qualcuno di questi casi.</p>
<section id="classificatore-sempre-positivo">
<h4>Classificatore sempre positivo<a class="headerlink" href="#classificatore-sempre-positivo" title="Permalink to this heading">#</a></h4>
<p>Consideriamo un classificatore che prevede sempre la classe positiva. Possiamo simulare le sue predizioni come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">positive_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">test_gt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo TNR, FPR, FNR e TPR</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">positive_preds</span><span class="p">)</span>
<span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">positive_preds</span><span class="p">)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">positive_preds</span><span class="p">)</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">positive_preds</span><span class="p">)</span>
<span class="n">tnr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TNR: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">tnr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FPR: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">fpr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FNR: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">fnr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TPR, Recall: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">tpr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">precision</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">f1</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">acc</span>)
<span class="n">cm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TNR: 0.00
FPR: 1.00
FNR: 0.00
TPR, Recall: 1.00
Precision: 0.37
F1: 0.54
Accuracy: 0.37
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 1.],
       [0., 1.]])
</pre></div>
</div>
</div>
</div>
<p>Come possiamo notare, il classificatore presenta una reacall (TPR) pari a <span class="math notranslate nohighlight">\(1\)</span>, e un FNR uguale a <span class="math notranslate nohighlight">\(0\)</span>. Ci√≤ indica che tutti gli elementi positivi sono stati in effetti classificati come positivi e che nessuno dei positivi √® stato classificato come negativo. Tuttavia, guardando anche a TNR e FNR, ci accorgiamo che il classificatore √® degenere e non √® capace di classificare i negativi. Precision, accuracy e F1, inoltre, indicano che il classificatore non √® buono. Un quadro pi√π chiaro √® offerto dalla matrice di confusione normalizzata:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 1.],
       [0., 1.]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="classificatore-sempre-negativo">
<h4>Classificatore sempre negativo<a class="headerlink" href="#classificatore-sempre-negativo" title="Permalink to this heading">#</a></h4>
<p>Analogamente, potremmo trovarci in presenza di un classificatore che prevede sempre la classe negativa:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">negative_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">test_preds</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">negative_preds</span><span class="p">)</span>
<span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">negative_preds</span><span class="p">)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">negative_preds</span><span class="p">)</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">negative_preds</span><span class="p">)</span>
<span class="n">tnr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TNR: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">tnr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FPR: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">fpr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FNR: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">fnr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TPR, Recall: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">tpr</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">precision</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">f1</span>)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="k">acc</span>)
<span class="n">cm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TNR: 1.00
FPR: 0.00
FNR: 1.00
TPR, Recall: 0.00
Precision: 0.00
F1: 0.00
Accuracy: 0.63
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/furnari/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)
/home/furnari/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 0.],
       [1., 0.]])
</pre></div>
</div>
</div>
</div>
<p>Come notiamo dai warning, precision e F1 non sono ben definite in questo caso.</p>
<section id="accuracy-e-sbilanciamento-delle-classi">
<h5>Accuracy e sbilanciamento delle classi<a class="headerlink" href="#accuracy-e-sbilanciamento-delle-classi" title="Permalink to this heading">#</a></h5>
<p><strong>Attenzione:</strong> bench√© l‚Äôaccuracy sia di facile interpretazione, anche essa ha i suoi limiti. Infatti, se il numero di elementi nelle classi √® molto sbilanciato, un classificatore degenere pu√≤ ottenere una accuracy alta. Vediamo un esempio concreto. Supponiamo che un classificatore binario sia caratterizzato dalla seguente matrice di confusione:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">72</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">cm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[72,  0],
       [10,  0]])
</pre></div>
</div>
</div>
</div>
<p>Dalla matrice di confusione evinciamo che il classificatore predice solo la classe negativa. Tuttavia <span class="math notranslate nohighlight">\(72\)</span> elementi appartengono alla classe positiva, mentre solo <span class="math notranslate nohighlight">\(10\)</span> appartengono alla classe negativa. Calcoliamo l‚Äôaccuracy relativa a questa matrice di confusione:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.88
</pre></div>
</div>
</div>
</div>
<p>Nonostante il classificatore sia degenere (ce ne accorgiamo dalla matrice di confusione), esso ottiene una buona accuracy. In questi casi, lo <span class="math notranslate nohighlight">\(F_1\)</span> score pu√≤ offrirci un quadro pi√π chiaro.</p>
</section>
</section>
</section>
<section id="nota-sulle-diverse-misure-di-valutazione-di-performance">
<h3>Nota sulle diverse misure di valutazione di performance<a class="headerlink" href="#nota-sulle-diverse-misure-di-valutazione-di-performance" title="Permalink to this heading">#</a></h3>
<p>Abbiamo visto che esistono diverse misure per valutare le performance di un algoritmo (e ne esistono molte altre non trattate in questo laboratorio!). Ogni misura cattura un aspetto diverso del classificatore e dunque, a seconda del problema considerato √® opportuno scegliere la misura pi√π corretta per la valutazione delle performance. Vediamo qualche esempio:</p>
<ul class="simple">
<li><p><strong>Classificazione di email di spam vs non spam</strong>: supponiamo che ‚Äúspam‚Äù rappresenti la classe positiva e che ‚Äúnon-spam‚Äù rappresenti la classe negativa. Da un lato, vorrei un sistema capace di individuare bene entrambe le categorie, dall‚Äôaltro voglio evitare di cestinare email legittime come ‚Äúspam‚Äù. In questo caso ha senso cercare di costruire un sistema che ottiene una <strong>recall</strong> accettabile (=email di spam effettivamente individuate) con valori di <strong>FPR</strong> bassissimi (=poche email cestinate come spam) e valori di <strong>precision</strong> alti (=ci√≤ che ho individuato come spam era effettivamente spam);</p></li>
<li><p><strong>Diagnosi precoce di tumori</strong>: si vuole adoperare un sistema di diagnosi precoce dei tumori automatica. Tutti i casi individuati come tumori verranno in seguito analizzati da un esperto che potr√† rigettare i casi di errore. I casi non classificati come tumori vengono analizzati nuovamente solo <span class="math notranslate nohighlight">\(6\)</span> mesi dopo. Se la presenza di un tumore √® indicata dalla classe positiva, vogliamo costruire un sistema che abbia <strong>reacall</strong> pari a <span class="math notranslate nohighlight">\(1\)</span> (tutti i tumori sono correttamente identificati) anche al costo di un <strong>FPR</strong> non bassissimo o di una <strong>precision</strong> non altissima (qualche negativo viene identificato come positivo - gli esami successivi potranno correggere questo risultato);</p></li>
<li><p><strong>Classificatore delle iris di Fisher</strong>: il dataset √® bilanciato (<span class="math notranslate nohighlight">\(150\)</span> esempi per classi). In questi casi l‚Äô<strong>accuracy</strong> pu√≤ essere una buona misura di valutazione.</p></li>
</ul>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 5</strong></p>
<p>Si vuole costruire un sistema automatico per l‚Äôanalisi di componenti elettronici prodotti da una fabbrica. Il sistema analizza delle misurazioni effettuate su alcuni elementi e determina se il componente pu√≤ essere passato alla vendita o se va cestinato. Il sistema deve permettere di mantenere uno standard qualitativo alto con pochi pezzi difettosi effettivamente venduti (tali pezzi potranno poi essere sostituiti dopo la notifica dell‚Äôacquirente). Che misure di valutazione conviene considerare durante la costruzione del sistema?</p>
</div></blockquote>
</section>
<section id="errore-di-training-e-errore-di-generalizzazione">
<h3>Errore di training e errore di generalizzazione<a class="headerlink" href="#errore-di-training-e-errore-di-generalizzazione" title="Permalink to this heading">#</a></h3>
<p>Finora abbiamo applicato le misure di valutazione sul test set. Ci√≤ ci permette di capire se il modello pu√≤ essere utilizzato su nuovi dati non visti in fase di training e dunque se il modello ‚Äúgeneralizza‚Äù a dati nuovi. L‚Äôerrore ottenuto sul test set √® dunque in genere detto anche ‚Äúerrore di generalizzazione‚Äù.</p>
<p>Un‚Äôaltra misura che ci permette di capire quanto il classificatore sia buono, soprattutto se confrontata all‚Äôerrore di generalizzazione, √® l‚Äôerrore di training, ovvero l‚Äôerrore calcolato sul training set. Vediamo come ottenerlo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Calcoliamo le predizioni sul test set</span>
<span class="n">train_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">biopsy_train</span><span class="o">.</span><span class="n">dropna</span><span class="p">())</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1">#Otteniamo le etichette di ground truth di test</span>
<span class="n">train_gt</span> <span class="o">=</span> <span class="n">biopsy_train</span><span class="o">.</span><span class="n">dropna</span><span class="p">()[</span><span class="s1">&#39;cl&#39;</span><span class="p">]</span>

<span class="c1">#calcoliamo accuracy e matrice di confusione normalizzata</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">train_gt</span><span class="p">,</span> <span class="n">train_preds</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">train_gt</span><span class="p">,</span> <span class="n">train_preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Accuracy di test: </span><span class="si">%0.2f</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrice di confusione normalizzata:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy di test: 0.97

Matrice di confusione normalizzata:

[[0.9760479  0.0239521 ]
 [0.03428571 0.96571429]]
</pre></div>
</div>
</div>
</div>
<p>Confrontiamo i valori ottenuti con quelli di test:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#calcoliamo accuracy e matrice di confusione normalizzata</span>
<span class="n">cm_test</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="n">cm_test</span> <span class="o">=</span> <span class="n">cm_test</span><span class="o">/</span><span class="n">cm_test</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_gt</span><span class="p">,</span> <span class="n">test_preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Accuracy di test: </span><span class="si">%0.2f</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">acc_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matrice di confusione normalizzata:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy di test: 0.97

Matrice di confusione normalizzata:

[[0.98181818 0.01818182]
 [0.046875   0.953125  ]]
</pre></div>
</div>
</div>
</div>
<p>L‚Äôaccuracy √® praticamente identica. La matrice di confusione sul training set appare leggermente pi√π bilanciata. Possiamo concludere che il modello generalizza bene.</p>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 6</strong></p>
<p>Si supponga che un classificatore abbia errore di training basso ed errore di generalizzazione alto. Come si chiama questo tipo di condizione?</p>
</div></blockquote>
</section>
<section id="classificazione-basata-su-soglia-e-curve-roc">
<h3>Classificazione Basata su Soglia e Curve ROC<a class="headerlink" href="#classificazione-basata-su-soglia-e-curve-roc" title="Permalink to this heading">#</a></h3>
<p>Consideriamo nuovamente il nostro dataset di altezze e pesi:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;http://iplab.dmi.unict.it/furnari/downloads/height_weight.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>BMI</th>
      <th>height</th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>M</td>
      <td>33.36</td>
      <td>187.96</td>
      <td>117.933920</td>
    </tr>
    <tr>
      <th>1</th>
      <td>M</td>
      <td>26.54</td>
      <td>177.80</td>
      <td>83.914520</td>
    </tr>
    <tr>
      <th>2</th>
      <td>F</td>
      <td>32.13</td>
      <td>154.94</td>
      <td>77.110640</td>
    </tr>
    <tr>
      <th>3</th>
      <td>M</td>
      <td>26.62</td>
      <td>172.72</td>
      <td>79.378600</td>
    </tr>
    <tr>
      <th>4</th>
      <td>F</td>
      <td>27.13</td>
      <td>167.64</td>
      <td>76.203456</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Abbiamo visto che le altezze di uomini e donne si distribuiscono diversamente. Rivediamo velocemente alcune statistiche descrittive dei due set di dati:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">][</span><span class="s1">&#39;height&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">(),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;F&#39;</span><span class="p">][</span><span class="s1">&#39;height&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    1946.000000
mean      177.631624
std         7.395885
min       160.020000
25%       172.720000
50%       177.800000
75%       182.880000
max       193.040000
Name: height, dtype: float64 

count    2285.000000
mean      163.303659
std         6.522939
min       149.860000
25%       157.480000
50%       162.560000
75%       167.640000
max       177.800000
Name: height, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Abbiamo visto in particolare che le donne sono in genere pi√π basse degli uomini. Ci chiediamo dunque cosa possiamo dire del sesso di un soggetto conoscendone solo l‚Äôaltezza. Sappiamo ad esempio che un soggetto alto 180 cm √® molto pi√π probabilmente un uomo che una donna. Analogamente un soggetto alto 150 cm sar√† pi√π probabilmente una donna. Cosa possiamo dire di un soggetto alto 170 cm? In generale ci aspettiamo che esista un <strong>valore soglia</strong> al di sopra del quale √® pi√π probabile trovare soggetti maschili che femminili. Resta da capire come trovare tale valore soglia. Supponiamo ad esempio di scegliere arbitrariamente 170 cm come valore soglia.</p>
<p>Se volessimo usare tale valore soglia per predire il sesso dei soggetti, dovremmo semplicemente classificare come uomini tutti i campioni con altezza superiore o uguale a 170 cm e viceversa come donne i campioni con altezza inferiore a 170 cm. Costruiamo un vettore <strong>male_pred</strong> che contenga True in corrispondenza dei soggetti classificati come uomini e False altrove:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">male_pred</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">height</span><span class="o">&gt;=</span><span class="mi">170</span>
<span class="n">male_pred</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0     True
1     True
2    False
3     True
4    False
Name: height, dtype: bool
</pre></div>
</div>
</div>
</div>
<p>Se vogliamo misurare la bont√† delle predizioni, abbiamo bisogno di confrontare il vettore ottenuto con il vettore di <strong>ground truth</strong>, ovvero il vettore dei sessi corretti. Possiamo ottenere tale vettore come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">male_gt</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="s1">&#39;M&#39;</span> <span class="c1">#&quot;gt&quot; sta per ground truth</span>
<span class="n">male_gt</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0     True
1     True
2    False
3     True
4    False
Name: sex, dtype: bool
</pre></div>
</div>
</div>
</div>
<p>Per valutare la bont√† delle predizioni, calcoliamo due numeri: il <strong>True Positive Rate</strong> (<strong>TPR</strong>) per misurare quanti positivi sono stati trovati e il <strong>False Positive Rate</strong> (<strong>FPR</strong>) per misurare quanti negativi sono stati classificati come positivi. Chiaramente, vorremmo un <strong>TPR</strong> alto e un <strong>FPR</strong> basso:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tpr</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False Positive Rate: 0.21
True Positive Rate: 0.87
</pre></div>
</div>
</div>
</div>
<p>Gli score ci dicono che <strong>l‚Äô86% dei soggetti maschili sono stati effettivamente classificati come maschi</strong> e che <strong>il 21% dei soggetti femminili sono stati erroneamente classificati come uomini</strong>. E‚Äô naturale chiedersi se esiste un‚Äôaltra sogli migliore di quella che abbiamo considerato. Proviamo a ripetere il processo per <span class="math notranslate nohighlight">\(165\)</span> cm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dataframe</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">threshold</span>

<span class="n">male_pred</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">165</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tpr</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False Positive Rate: 0.45
True Positive Rate: 0.97
</pre></div>
</div>
</div>
</div>
<p>Proviamo con una soglia pari a 175 cm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">male_pred</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">175</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">male_pred</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tpr</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False Positive Rate: 0.06
True Positive Rate: 0.67
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 7</strong></p>
<p>I risultati ottenuti con le due nuove soglie sono migliori o peggiori dei primi? Qual √® il risultato migliore?</p>
</div></blockquote>
<p>Abbiamo visto che cambiando il valore della soglia, i valori di <strong>TPR</strong> e <strong>FPR</strong> cambiano a loro volta. In particolare abbiamo notato che al crescere dell‚Äôuno diminuisce l‚Äôaltro e viceversa. Se vogliamo vedere cosa succede per diversi valori della soglia, possiamo visualizzare una curva <strong>ROC</strong>.</p>
<p>Una curva <strong>ROC</strong> effettua la classificazione con tutte le soglie possibili e calcola i valori di <strong>TPR</strong> e <strong>FPR</strong> per ciascuno dei classificatori. Le coppie di valori <strong>TPR</strong> e <strong>FPR</strong> vengono dunque plottati come punti su un piano. La funzione <code class="docutils literal notranslate"><span class="pre">roc_curve</span></code> di <code class="docutils literal notranslate"><span class="pre">scikit_learn</span></code> ci permette di ottenere le coppie di valori <strong>TPR</strong> e <strong>FPR</strong> che si ottengono sogliando un dato valore detto ‚Äúscore‚Äù:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>La funzione <code class="docutils literal notranslate"><span class="pre">roc_curve</span></code> restituisce i valori calcolati per <strong>FPR</strong>, <strong>TPR</strong> e le soglie utilizzate per calcolare tali valori. Plottiamo la curva ROC:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="n">tpr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ROC Curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/16d54d2796e6d1d2e663ea3e444ded9da766c677cc63075be2dc3f8198e4ebb3.png" src="../_images/16d54d2796e6d1d2e663ea3e444ded9da766c677cc63075be2dc3f8198e4ebb3.png" />
</div>
</div>
<p>La curva <strong>ROC</strong> mostra come il <strong>TPR</strong> decresce al crescere del <strong>FPR</strong>. In pratica, ogni valore della threshold definisce un classificatore che effettua un <strong>compromesso</strong> tra <strong>TPR</strong> e <strong>FPR</strong>. Per valori molto alti della soglia, riusciamo a massimizzare il <strong>TPR</strong> ottenendo un <strong>FPR</strong> alto (il che non √® desiderabile). Per valori molto bassi della soglia, riusciamo a minimizzare il <strong>FPR</strong> ma otteniamo un <strong>TPR</strong> molto basso (anche in questo caso non desiderabile).</p>
<p>In generale, la curva <strong>ROC</strong> ci permette di osservare le performance di un classificatore <strong>indipendentemente dal valore della soglia utilizzato</strong>. Idealmente, vorremmo che la nostra curva includesse il punto di coordinate (0,1), ovvero vorremmo che esistesse almeno una soglia per la quale otteniamo <strong>TPR</strong> pari a 1 e <strong>FPR</strong> pari a 0. Analogamente, vorremmo che la curva si trovasse sempre al di sopra dalla diagonale principale, sulla quale un incremento di <strong>TPR</strong> non corrisponderebbe a un decremento di <strong>FPR</strong>. Vediamo di plottare due prototipi della <strong>curva ideale</strong> e della <strong>curva pessima</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fpr_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">tpr_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="n">fpr_best</span><span class="p">),[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="n">tpr_best</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_best</span><span class="p">,</span><span class="n">fpr_best</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;FPR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;TPR&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Optimal Curve&#39;</span><span class="p">,</span><span class="s1">&#39;Worst Curve&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cb3c2fe7a6b3249b473c2ce99817cac7d98043af62abede3b2431ae493e25897.png" src="../_images/cb3c2fe7a6b3249b473c2ce99817cac7d98043af62abede3b2431ae493e25897.png" />
</div>
</div>
<p>Pi√π la nostra curva ‚Äúsomiglia‚Äù alla curva ottima, migliore √® il classificatore. Quando il classificatore prevede le classi in maniera totalmente casuale, otteniamo la curva pessima.</p>
<p>Abbiamo detto che le curve ROC sono buone per valutare la bont√† di un classificatore indipendentemente dalla soglia usata per classificare. In particolare, le curve ROC possono essere usate per confrontare pi√π classificatori. Supponiamo di costruire un classificatore analogo basato sulle misure dei pesi. Confrontiamo le due curve ROC:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_roc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="n">tpr</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="n">tpr</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ROC Curve&#39;</span><span class="p">)</span>
    
<span class="n">fpr_h</span><span class="p">,</span><span class="n">tpr_h</span><span class="p">,</span><span class="n">t_h</span><span class="o">=</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>
<span class="n">fpr_w</span><span class="p">,</span><span class="n">tpr_w</span><span class="p">,</span><span class="n">t_w</span><span class="o">=</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">sex</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plot_roc</span><span class="p">(</span><span class="n">fpr_h</span><span class="p">,</span><span class="n">tpr_h</span><span class="p">)</span>
<span class="n">plot_roc</span><span class="p">(</span><span class="n">fpr_w</span><span class="p">,</span><span class="n">tpr_w</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Height&#39;</span><span class="p">,</span><span class="s1">&#39;Weight&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/297dcbc1b074b122e10107a401170f0ebb5fb8dc4af6678e59d2280b40bc5ae7.png" src="../_images/297dcbc1b074b122e10107a401170f0ebb5fb8dc4af6678e59d2280b40bc5ae7.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 8</strong></p>
<p>Quale delle due variabili √® pi√π discriminativa nel nostro esempio? Esiste una soglia per la quale i pesi sono pi√π discriminativi delle altezze?</p>
</div></blockquote>
<p>Resta ancora da capire come scegliere una buona soglia per il classificatore. Abbiamo detto che la soglia va scelta sulla base di un qualche criterio legato all‚Äôapplicazione in cui utilizzeremo il classificatore. Supponiamo di voler trovare la soglia che massimizzi <strong>TPR</strong> e minimizzi <strong>FPR</strong> e supponiamo che i due score abbiano lo stesso peso nella nostra applicazione. Possiamo dunque scegliere la soglia che massimizza la somma <strong>TPR+(1-FPR)</strong>. Vediamo come questo valore cambia al cambiare delle soglie:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score</span> <span class="o">=</span> <span class="n">tpr_h</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">fpr_h</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_h</span><span class="p">,</span><span class="n">score</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Thresholds&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;TPR+(1-FPR)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e44d15fc1a7e1becba47a12a1ffb70e6fb8d3a088a1fbc9dd9f6551833faa150.png" src="../_images/e44d15fc1a7e1becba47a12a1ffb70e6fb8d3a088a1fbc9dd9f6551833faa150.png" />
</div>
</div>
<p>Possiamo osservare che esiste un massimo globale. Troviamo la soglia in corrispondenza della quale si ha tale massimo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimal_threshold</span> <span class="o">=</span> <span class="n">t_h</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">score</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;La soglia migliore √® </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">optimal_threshold</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>La soglia migliore √® 172.72
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo dunque TPR e FPR per il classificatore individuato da questa soglia:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">optimal_threshold</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tpr</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False Positive Rate: 0.12
True Positive Rate: 0.78
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 9</strong></p>
<p>Si ripeta il processo di selezione della soglia utilizzando i pesi invece delle altezze. Qual √® la soglia ottimale ottenuta?</p>
</div></blockquote>
</section>
</section>
<section id="classificazione-nearest-neighbor-k-nn">
<h2>Classificazione Nearest Neighbor (K-NN)<a class="headerlink" href="#classificazione-nearest-neighbor-k-nn" title="Permalink to this heading">#</a></h2>
<p>Un classificatore Nearest Neighbor (K-NN) permette di classificare i dati confrontandoli con un insieme pre-esistente di dati etichettati (il training set). Sia <span class="math notranslate nohighlight">\(\mathbf{X}=\{(\mathbf{x}_i, y_i)\}_{i=1}^N\)</span> un dataset etichettato, dove <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> rappresentano i campioni del dataset e <span class="math notranslate nohighlight">\(y_i\)</span> rappresentano le relative etichette e sia <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> un nuovo campione di test da classificare. L‚Äôalgoritmo K-NN cerca prima i <span class="math notranslate nohighlight">\(K\)</span> campioni del training set pi√π simili a <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> secondo una qualche misura di similarity <span class="math notranslate nohighlight">\(\delta\)</span>. La classificazione avviene dunque assegnando a <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> l‚Äôetichetta pi√π frequente fra le <span class="math notranslate nohighlight">\(K\)</span> etichette <span class="math notranslate nohighlight">\(y_i\)</span> dei campioni di training trovati. Per evitare situazioni di incertezza, di solito si sceglie un valore di <span class="math notranslate nohighlight">\(K\)</span> non multiplo del numero di classi. Nel caso in cui <span class="math notranslate nohighlight">\(K = 1\)</span>, l‚Äôalgoritmo assegna a <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> l‚Äôetichetta del campione pi√π simile nel training set.</p>
<p>Vediamo un esempio con il dataset degli iris di Fisher. Carichiamo il dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.datasets</span> <span class="kn">import</span> <span class="n">get_rdataset</span>
<span class="n">fisher</span> <span class="o">=</span> <span class="n">get_rdataset</span><span class="p">(</span><span class="s1">&#39;iris&#39;</span><span class="p">)</span>
<span class="n">fisher</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Consideriamo per il momento solo due features, ma consideriamo tutte e tre le classi:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fisher</span><span class="o">.</span><span class="n">data</span>
<span class="c1">#costruiamo un dataframe vuoto</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="c1">#inserieamo le prime due feature e chiamiamole X e Y</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Sepal.Length&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Sepal.Width&#39;</span><span class="p">]</span>
<span class="c1">#inseriamo una colonna &quot;C&quot; che contenga le classi</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">df</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X</th>
      <th>Y</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Suddividiamo adesso il dataset in training e test set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">#dato che il dataset √® molto piccolo, prendiamo il 90% dei dati per formare il training set</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">134</span><span class="p">)</span>
<span class="n">data_training</span><span class="p">,</span> <span class="n">data_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di campioni di training:&quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data_training</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numero di campioni di test:&quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numero di campioni di training: 112
       X    Y           C
60   5.0  2.0  versicolor
143  6.8  3.2   virginica
144  6.7  3.3   virginica
127  6.1  3.0   virginica
8    4.4  2.9      setosa

Numero di campioni di test: 38
       X    Y           C
65   6.7  3.1  versicolor
46   5.1  3.8      setosa
99   5.7  2.8  versicolor
101  5.8  2.7   virginica
47   4.6  3.2      setosa
</pre></div>
</div>
</div>
</div>
<p>Visualizziamo i campioni di training e test nello spazio 2D:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="k">def</span> <span class="nf">plot2d</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label_suffix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">):</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                 <span class="n">data</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">c</span><span class="o">+</span><span class="n">label_suffix</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plot2d</span><span class="p">(</span><span class="n">data_training</span><span class="p">,</span><span class="s1">&#39; training&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="c1">#azzero i colori in modo da avere glis tessi colori per training e test set</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">plot2d</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span><span class="s1">&#39; test&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bcafb514358c2ce8186eb7f93ab7a786f85674002ffec0953f8b740fdba1280e.png" src="../_images/bcafb514358c2ce8186eb7f93ab7a786f85674002ffec0953f8b740fdba1280e.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 10</strong></p>
<p>Come verrebbero classificati i dati di training da un classificatore 1-NN? La classificazione sarebbe sempre corretta? Un SVM lineare farebbe meglio o peggio di 1-NN in questo caso?</p>
</div></blockquote>
<p>Costruiamo adesso un classificatore 1-NN mediante scikit-learn:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span> <span class="k">as</span> <span class="n">KNN</span>

<span class="n">knn_1</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">knn_1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_training</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
           metric_params=None, n_jobs=1, n_neighbors=1, p=2,
           weights=&#39;uniform&#39;)
</pre></div>
</div>
</div>
</div>
<p>Di default, il classificatore verr√† costruito considerando la distanza Euclidea come misura di similarit√† tra i campioni. Calcoliamo adesso le accuracy di training e test:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy sul training set: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">knn_1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_training</span><span class="o">.</span><span class="n">C</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy sul test set: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span>  <span class="n">knn_1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_test</span><span class="o">.</span><span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy sul training set: 0.95
Accuracy sul test set: 0.74
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 11</strong></p>
<p>Perch√© l‚Äôaccuracy sul training set non √® pari a <span class="math notranslate nohighlight">\(1\)</span>? Si risponda guardando al grafico mostrato sopra.</p>
</div></blockquote>
<p>L‚Äôalgortimo KNN suddivide implicitamente lo spazio dei dati in delle regioni di decisione, dove il valore di decisione in un particolare punto dello spazio dipende dalla classe di appartenenza dei K vicini pi√π prossimi. Per visualizzare le regioni di decisione e dunque il decision boundary, possiamo effettuare la classificazione per una griglia uniforme di punti:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="c1">#creiamo una colormap per mostrare le regioni di decisione</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#AAAAFF&#39;</span><span class="p">,</span> <span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span> <span class="p">])</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>

<span class="k">def</span> <span class="nf">plot_knn_decision_boundary</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">knn</span><span class="p">):</span>
    <span class="n">plot2d</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">*</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.05</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">*</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.05</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    
    <span class="n">Z</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    
    <span class="c1">#costruiamo un dizionario per transformare le stringhe in indici</span>
    <span class="n">class_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">classes_</span><span class="p">)}</span>
    
    <span class="n">Z</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">class_dict</span><span class="p">[</span><span class="n">x</span><span class="p">],</span><span class="n">Z</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>
    <span class="c1">#plt.contour(X,Y,Z)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plot_knn_decision_boundary</span><span class="p">(</span><span class="n">data_training</span><span class="p">,</span><span class="n">knn_1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">plot2d</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span><span class="n">label_suffix</span><span class="o">=</span><span class="s1">&#39; test&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8a219b3c4904ed5a34b285e9ad0d1865e7fee197102a6312e7c49694ee6d89e2.png" src="../_images/8a219b3c4904ed5a34b285e9ad0d1865e7fee197102a6312e7c49694ee6d89e2.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 12</strong></p>
<p>Lo spazio √® stato suddiviso in maniera plausibile? Ci sono delle regioni in cui si potrebbe fare di meglio? A cosa sono dovuti gli errori di test?</p>
</div></blockquote>
<p>Proviamo adesso a costruire un classificatore <span class="math notranslate nohighlight">\(7-NN\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn_7</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">knn_7</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_training</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy sul training set: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">knn_7</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_training</span><span class="o">.</span><span class="n">C</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy sul test set: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span>  <span class="n">knn_7</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_test</span><span class="o">.</span><span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy sul training set: 0.84
Accuracy sul test set: 0.82
</pre></div>
</div>
</div>
</div>
<p>Visualizziamo il decision boundary di questo classificatore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plot_knn_decision_boundary</span><span class="p">(</span><span class="n">data_training</span><span class="p">,</span><span class="n">knn_7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">plot2d</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span><span class="n">label_suffix</span><span class="o">=</span><span class="s1">&#39; test&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/229655f8a7ed32d7ed9ac5f47c9ff661642607271358ea709ace835f9104a7e7.png" src="../_images/229655f8a7ed32d7ed9ac5f47c9ff661642607271358ea709ace835f9104a7e7.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 13</strong></p>
<p>In che modo √® cambiato il decision boundary?</p>
</div></blockquote>
<section id="trovare-il-valore-ottimale-di-k">
<h3>Trovare il valore ottimale di K<a class="headerlink" href="#trovare-il-valore-ottimale-di-k" title="Permalink to this heading">#</a></h3>
<p>Abbiamo visto come diverse scelte di valori per il parametro <span class="math notranslate nohighlight">\(K\)</span> permettano di modificare il decision boundary trovato implicitamente dall‚Äôalgoritmo <span class="math notranslate nohighlight">\(K-NN\)</span>. In particolare abbiamo visto come, a seconda di come i dati si distribuiscono nello spazio, un valore pi√π alto di <span class="math notranslate nohighlight">\(K\)</span> permetta di ridurre l‚Äôinfluenza di alcuni outliers, mentre un valore pi√π basso di <span class="math notranslate nohighlight">\(K\)</span> permette di delineare un decision boundary pi√π accurato. Va notato che il valore ottimale di <span class="math notranslate nohighlight">\(K\)</span> dipende dai dati sui quali stiamo lavorando. Proviamo ad esempio a vedere come varia l‚Äôaccuracy di training per diverse scelte di <span class="math notranslate nohighlight">\(K\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">k_values</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data_training</span><span class="p">))</span> 
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_values</span><span class="p">:</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_training</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
    <span class="n">test_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_test</span><span class="o">.</span><span class="n">C</span><span class="p">))</span>

<span class="n">best_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_accuracies</span><span class="p">)</span>
<span class="n">best_k</span> <span class="o">=</span> <span class="n">k_values</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>
<span class="n">best_accuracy</span> <span class="o">=</span> <span class="n">test_accuracies</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Migliore accuracy di test: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">best_accuracy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Migliore k: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">best_k</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_values</span><span class="p">,</span><span class="n">test_accuracies</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Migliore accuracy di test: 0.87
Migliore k: 9.00
</pre></div>
</div>
<img alt="../_images/f8c4ad46d71cf1424f0f41fa71b0f12ab66bf6ff630efecaf5827bdfeaf24139.png" src="../_images/f8c4ad46d71cf1424f0f41fa71b0f12ab66bf6ff630efecaf5827bdfeaf24139.png" />
</div>
</div>
<p>Abbiamo scoperto qual √® il <span class="math notranslate nohighlight">\(K\)</span> ottimale per il nostro dataset e qual √® la migliore accuracy raggiungibile. Tuttavia, dobbiamo sempre ricordare che i dati di test non sono disponibili in fase di training, per cui <strong>non dovremmo mai utilizzarli per scegliere i valori dei parametri</strong>. Dunque possiamo scegliere il migliore <span class="math notranslate nohighlight">\(k\)</span> solo utilizzando il training set. Ci√≤ si pu√≤ fare utilizzando l‚Äôoggetto GridSearchCV come visto nel caso di SVM:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="c1">#definiamo la griglia di parametri.</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">)}]</span>
<span class="c1">#definiamo l&#39;oggetto Grid Search</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KNN</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="c1">#avviamo la ricerca.</span>
<span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_training</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
<span class="n">best_knn</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Migliore valore di K trovato utilizzando solo il training set: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">best_knn</span><span class="o">.</span><span class="n">n_neighbors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Migliore valore di K trovato utilizzando solo il training set: 18
</pre></div>
</div>
</div>
</div>
<p>Il miglior valore di <span class="math notranslate nohighlight">\(K\)</span> trovato in questa maniera pu√≤ differire da quello trovato utilizzando il test set. Vediamo che accuracy di test otteniamo utilizzando il miglior modello trovato:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy su test set con miglior K trovato sul training set: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> \
        <span class="n">best_knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">data_test</span><span class="o">.</span><span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy su test set con miglior K trovato sul training set: 0.82
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 13</strong></p>
<p>Perch√© dobbiamo utilizzare il training set per scegliere il migliore valore di <span class="math notranslate nohighlight">\(K\)</span> e non possiamo semplicemente utilizzare il test set?</p>
</div></blockquote>
</section>
</section>
<section id="discriminante-lineare-di-fisher">
<h2>Discriminante Lineare di Fisher<a class="headerlink" href="#discriminante-lineare-di-fisher" title="Permalink to this heading">#</a></h2>
<p>Siano <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^N\)</span> un insieme di osservazioni <span class="math notranslate nohighlight">\(D\)</span>-dimensionali suddivise in due classi <span class="math notranslate nohighlight">\(C_1\)</span> e <span class="math notranslate nohighlight">\(C_2\)</span> in modo che <span class="math notranslate nohighlight">\(x_i \in C_1 \oplus x_i \in C_2\)</span>. Vogliamo trovare una trasformazione lineare dei dati in uno spazio monodimensionale:</p>
<div class="math notranslate nohighlight">
\[y(\mathbf{x}) = \mathbf{w}^T \mathbf{x}\]</div>
<p>dove <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> √® una osservazione <span class="math notranslate nohighlight">\(D\)</span>-dimensionale, <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> √® un vettore <span class="math notranslate nohighlight">\(D\)</span>-dimensionale detto <strong>vettore dei pesi</strong> e <span class="math notranslate nohighlight">\(y(\mathbf{x})\)</span> √® la proiezione su una dimensione di <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (dunque, <span class="math notranslate nohighlight">\(y(\mathbf{x})\)</span> √® uno scalare). Se la proiezione trovata massimizza la separabilit√† dei dati, non ci resta che trovare una soglia <span class="math notranslate nohighlight">\(y_0\)</span> e classificare:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} \ \text{ appartenente a }  C_1 \text{ se } y(\mathbf{x})\geq y_0\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{x} \ \text{appartenente a } C_2 \text{ se } y(\mathbf{x})&lt;y_0\]</div>
<p>Vediamo prima come trovare i parametri <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> che massimizzino la separabilit√† dei dati proiettati. Consideriamo le medie dei dati appartenenti alle due classi:</p>
<div class="math notranslate nohighlight">
\[\mathbf{m}_1 = \frac{1}{N_1} \sum_{x_i \in C_1} x_i, \ \ \ \ \ \ \ \ \mathbf{m}_2 = \frac{1}{N_2} \sum_{x_i \in C_2} x_i,\]</div>
<p>dove <span class="math notranslate nohighlight">\(N_1=|\{x_i\ |\ x_i \in C_1\}|\)</span> √® il numero di elementi appartenenti alla classe <span class="math notranslate nohighlight">\(C_1\)</span> e <span class="math notranslate nohighlight">\(N_2=|\{x_i\ |\ x_i \in C_2\}|\)</span> √® il numero di elementi appartenenti alla classe <span class="math notranslate nohighlight">\(C_2\)</span>. La misura pi√π semplice di separabilit√† tra le classi sarebbe la distanza tra le medie delle classi calcolate sui dati proiettati:</p>
<div class="math notranslate nohighlight">
\[m_1 = \mathbf{w}^T\mathbf{m}_1, \ m_2 = \mathbf{w}^T\mathbf{m}_2\]</div>
<div class="math notranslate nohighlight">
\[m_2 - m_1 = \mathbf{w}^T(\mathbf{m}_2-\mathbf{m}_1)\]</div>
<p>Per massimizzare la quantit√† <span class="math notranslate nohighlight">\(m_2-m_1\)</span>, inseriamo il vincolo che <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> sia un vettore unitario (<span class="math notranslate nohighlight">\(\mathbf{w}^T\mathbf{w}=1\)</span>). Introducendo un moltiplicatore di Lagrange, dobbiamo massimizzare:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^T (\mathbf{m}_2-\mathbf{m}_1) + \lambda (1-\mathbf{w}^T\mathbf{w})\]</div>
<p>Ponendo la derivata rispetto a <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> uguale a zero, vediamo che una soluzione √® data da:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^* = \frac{\mathbf{m}_2-\mathbf{m}_1}{||\mathbf{m}_2-\mathbf{m}_1||}\]</div>
<p>Questa soluzione tuttavia non ci garantisce nulla sulla varianza dei dati all‚Äôinterno di una singola classe, che va minimizzata per assicurare una maggiore separabilit√†. Consideriamo il seguente esempio:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">12</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">g1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">g2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Classe 1&#39;</span><span class="p">,</span><span class="s1">&#39;Classe 2&#39;</span><span class="p">,</span><span class="s1">&#39;Media Classe 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Media Classe 2&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">g3</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">g4</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g3</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g4</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">g3</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">g4</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Classe 1&#39;</span><span class="p">,</span><span class="s1">&#39;Classe 2&#39;</span><span class="p">,</span><span class="s1">&#39;Media Classe 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Media Classe 2&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/88e3f99a57851309e86f10601de77c6a9cf06ca2e317f2f9e9b7d364df401a5b.png" src="../_images/88e3f99a57851309e86f10601de77c6a9cf06ca2e317f2f9e9b7d364df401a5b.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 14</strong></p>
<p>In entrambi i casi la differenza tra le medie √® la stessa. Tuttavia i dati non sono separabili alla stessa maniera. Quale set di dati √® ‚Äúpi√π separabile‚Äù tra i due? Perch√©?</p>
</div></blockquote>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 15</strong></p>
<p>Si faccia un esempio analogo (con disegno) a quello appena visto nel caso 2D.</p>
</div></blockquote>
<p>Oltre a massimizzare la distanza tra le medie, vogliamo duqnue anche minimizzare le varianze dei dati appartenenti alle singole classi date da:</p>
<div class="math notranslate nohighlight">
\[s_k^2 = \sum_{\mathbf{x}_i \in C_k} y(\mathbf{x}_i - m_k)^2,\ k=1,2\]</div>
<p>Per <strong>massimizzare la distanza tra le medie trasformate</strong> e <strong>minimizzare le varianze dei dati trasformati</strong>, viene definito il criterio di Fisher come segue:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}) = \frac{(m_2-m_1)^2}{s_1^2+s_2^2}\]</div>
<p>Massimizzando <span class="math notranslate nohighlight">\(J(\mathbf{w})\)</span> rispetto a <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, raggiungiamo entrambi gli obiettivi:</p>
<ul class="simple">
<li><p>Massimizziamo <span class="math notranslate nohighlight">\((m_2-m_1)\)</span> che si trova al numeratore;</p></li>
<li><p>Minimizziamo <span class="math notranslate nohighlight">\(s_1^2+s_2^2\)</span> che si trova al denominatore.</p></li>
</ul>
<p>Il criterio di Fisher pu√≤ essere riscritto in forma esplicita come dipendente da <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> come segue:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B\mathbf{w}}{\mathbf{w}^T \mathbf{S}_W\mathbf{w}}\]</div>
<p>Dove <span class="math notranslate nohighlight">\(\mathbf{S}_B\)</span> √® la <strong>between-class covariance matrix</strong> (matrice di covarianza intra-classe) ed √® data da:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S}_B = (\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^T\]</div>
<p>e <span class="math notranslate nohighlight">\(\mathbf{S}_W\)</span> √® la <strong>within-class covariance matrix</strong> totale (matrice di covarianza inter-classe) ed √® data da:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S}_W = \sum_{\mathbf{x}_i \in C_1}(\mathbf{x}_i-\mathbf{m}_1)(\mathbf{x}_i-\mathbf{m}_1)^T + \sum_{\mathbf{x}_i \in C_2}(\mathbf{x}_i-\mathbf{m}_2)(\mathbf{x}_i-\mathbf{m}_2)^T\]</div>
<p>La quantit√† <span class="math notranslate nohighlight">\(J(\mathbf{w})\)</span> risulta essere massimizzata da:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}\propto\mathbf{S}^{-1}_{W}(\mathbf{m_2}-\mathbf{m_1})\]</div>
<p>Per cui, una possibile soluzione √®:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^* = \frac{\mathbf{S}^{-1}_{W}(\mathbf{m_2}-\mathbf{m_1})}{||\mathbf{S}^{-1}_{W}(\mathbf{m_2}-\mathbf{m_1})||}\]</div>
<p>Notiamo che se la matrice di covarianza inter-classe √® diagonale, la soluzione trovata √® equivalente a quella trovata massimizzando solo <span class="math notranslate nohighlight">\(\mathbf{m}_2-\mathbf{m}_1\)</span>.</p>
<p>Una volta trovata la proiezione ottimale <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> possiamo proiettare i dati mediante <span class="math notranslate nohighlight">\(y(\mathbf{x})=\mathbf{w}^T\mathbf{x}\)</span>. Dobbiamo dunque trovare la soglia <span class="math notranslate nohighlight">\(y_0\)</span> per classificare i dati utilizzando il discriminante <span class="math notranslate nohighlight">\(y(x)\geq y_0\)</span>. La soglia ottimale <span class="math notranslate nohighlight">\(y_0\)</span> pu√≤ essere trovata modellando i dati delle due classi mediante due Gaussiane e trovando il punto in cui esse si intersecano.</p>
<section id="esempio-breast-cancer-dataset">
<h3>Esempio: Breast Cancer Dataset<a class="headerlink" href="#esempio-breast-cancer-dataset" title="Permalink to this heading">#</a></h3>
<p>Vediamo un esempio utilizzando come dataset il ‚ÄúBreast Cancer Wisconsin (Diagnostic) Database‚Äù. Possiamo caricarlo tramile <strong>scikit-learn</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="n">bcancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Il dataset contiene 30 misurazioni derivate da campioni di masse estratte da seni. Ognuno di questo campione √® classificato come ‚Äúmaligno‚Äù (classe 0) o ‚Äúbenigno‚Äù (classe 1). Le misurazioni sono contenute in <code class="docutils literal notranslate"><span class="pre">bcancer.data</span></code>, mentre le classi sono contenute in <code class="docutils literal notranslate"><span class="pre">bcancer.target</span></code>. Stampare <code class="docutils literal notranslate"><span class="pre">bcancer.DESCR</span></code> per visualizzare una descrizione del dataset. Consideriamo come feature solo le prime due features (raggio medio e tessitura media):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">bcancer</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])]</span><span class="o">.</span><span class="n">T</span> <span class="c1"># features</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">bcancer</span><span class="o">.</span><span class="n">target</span> <span class="c1">#classi</span>
</pre></div>
</div>
</div>
</div>
<p>Visualizziamo i dati in un plot 2D:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Maligno&#39;</span><span class="p">,</span><span class="s1">&#39;Benigno&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f572ba11450a58e534568a592a71f8d296fb85d3624db584723d643f3c8cada6.png" src="../_images/f572ba11450a58e534568a592a71f8d296fb85d3624db584723d643f3c8cada6.png" />
</div>
</div>
<p>Calcoliamo le medie relative alle due classi e visualizziamole nel plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="n">C</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">m2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="n">C</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">m1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">m1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Xk&#39;</span><span class="p">,</span><span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">m2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">m2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;Xr&#39;</span><span class="p">,</span><span class="n">markersize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Maligno&#39;</span><span class="p">,</span><span class="s1">&#39;Benigno&#39;</span><span class="p">,</span><span class="s1">&#39;m1&#39;</span><span class="p">,</span><span class="s1">&#39;m2&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/069eff943ec8246123e900b4405b8b5b6f1562a7c66251ecfb4d34056a51bcd2.png" src="../_images/069eff943ec8246123e900b4405b8b5b6f1562a7c66251ecfb4d34056a51bcd2.png" />
</div>
</div>
<p>Calcoliamo ora la matrice <span class="math notranslate nohighlight">\(\mathbf{S}_W\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">C</span><span class="o">==</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">C</span><span class="o">==</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>La soluzione secondo il criterio di Fisher √®:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">inv</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">SW</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">m2</span><span class="o">-</span><span class="n">m1</span><span class="p">))</span>
<span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.96270156 -0.27056552]
</pre></div>
</div>
</div>
</div>
<p>Proiettiamo i dati utilizzando il vettore <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> appena trovato:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">#adesso i dati sono unidimensionali</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(569,)
</pre></div>
</div>
</div>
</div>
<p>Per visualizzare come i dati si dispongono nel nuovo spazio, visualizziamo i due relativi istogrammi:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">C</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">C</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Maligno&#39;</span><span class="p">,</span><span class="s1">&#39;Benigno&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c06b88087fbc3eb3930197348f937447382e0ec1e237071c5ef65d0005b942c1.png" src="../_images/c06b88087fbc3eb3930197348f937447382e0ec1e237071c5ef65d0005b942c1.png" />
</div>
</div>
<p>Ora che i dati sono unidimensionali, possiamo provare a classificarli con una soglia. Utilizziamo il criterio MAP per trovare quella ottimale. Facciamo fit di due Gaussiane:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="n">g0</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">C</span><span class="o">==</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">C</span><span class="o">==</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g0</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Maligno&#39;</span><span class="p">,</span><span class="s1">&#39;Benigno&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1e36d7110676b86f3d741770bc7e2dfe9e206e322e2797729830c7854f1e1aed.png" src="../_images/1e36d7110676b86f3d741770bc7e2dfe9e206e322e2797729830c7854f1e1aed.png" />
</div>
</div>
<p>Come abbiamo gi√† visto, la soglia ottimale √® data dal punto in cui si intersecano le due Gaussiane. Utilizziamo il metodo visto in precedenza per farlo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">risolvi</span><span class="p">(</span><span class="n">mu_1</span><span class="p">,</span><span class="n">mu_2</span><span class="p">,</span><span class="n">sigma_1</span><span class="p">,</span><span class="n">sigma_2</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">mu_2</span><span class="o">/</span><span class="p">(</span><span class="n">sigma_2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu_1</span><span class="o">/</span><span class="p">(</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">mu_1</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu_2</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma_2</span><span class="o">/</span><span class="n">sigma_1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">roots</span><span class="p">([</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">])</span>

<span class="n">solutions</span> <span class="o">=</span> <span class="n">risolvi</span><span class="p">(</span><span class="n">g0</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">g1</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">g0</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">g1</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">solutions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-19.36113911  -7.01507854]
</pre></div>
</div>
</div>
</div>
<p>Scartiamo la seconda soluzione (in quel punto le Gaussiane sono quasi entrambe nulle) e teniamo la prima come soglia ottimale <span class="math notranslate nohighlight">\(y_0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y0</span><span class="o">=</span><span class="n">solutions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Possiamo dunque classificare i dati mediante la regola <span class="math notranslate nohighlight">\(y(\mathbf{x})\geq y_0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">Y</span><span class="o">&gt;=</span><span class="n">y0</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo l‚Äôaccuracy del classificatore appena costruito:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="n">predictions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8910369068541301
</pre></div>
</div>
</div>
</div>
</section>
<section id="interpretazione-geometrica-dei-coefficienti">
<h3>Interpretazione geometrica dei coefficienti<a class="headerlink" href="#interpretazione-geometrica-dei-coefficienti" title="Permalink to this heading">#</a></h3>
<p>Mettendo insieme la proiezione LDA e la classificazione mediante soglia <span class="math notranslate nohighlight">\(y(\mathbf{x})\)</span>, abbiamo costruito il <strong>discriminante lineare</strong>:</p>
<div class="math notranslate nohighlight">
\[z(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + w_0\]</div>
<p>dove <span class="math notranslate nohighlight">\(w_0=-y_0\)</span>. Utilizzando il discriminante, possiamo classificare i dati come segue:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}\ \text{ appartenente a } C_1  \text{ se } z(\mathbf{x})\geq 0\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{x}\ \text{ appartenente a } C_2  \text{ se } z(\mathbf{x})&lt; 0\]</div>
<p>Questo schema di classificazione √® del tutto equivalente a quello visto in precedenza, ma questo modo di vedere le cose ci permette di fare alcune considerazioni geometriche.</p>
<p>Il classificatore lineare separa i dati nel loro spazio di appartenenza <span class="math notranslate nohighlight">\(D\)</span>-dimensionale mediante un iperpiano <span class="math notranslate nohighlight">\(D-1\)</span> dimensionale definito dalla formula <span class="math notranslate nohighlight">\(z(\mathbf{x})=0\)</span>. Questo iperpiano di separazione √® in genere detto ‚Äúdecision boundary‚Äù. Ad esempio, se i dati sono bidimensionali, il discriminante pu√≤ essere espresso come <span class="math notranslate nohighlight">\(z((x,y)) = w_1 x + w_2 y + w_0\)</span> e l‚Äôiperpiano che separa i dati √® la retta di equazione <span class="math notranslate nohighlight">\(w_1 x + w_2 y + w_0=0\)</span> o, in forma esplicita <span class="math notranslate nohighlight">\(y=-\frac{w_1}{w_2}x-\frac{w_0}{w_2}\)</span> Proviamo a graficarla nel caso del nostro esempio:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span> <span class="n">x2</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="n">y1</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x1</span><span class="o">+</span><span class="n">y0</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y2</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="o">+</span><span class="n">y0</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">],[</span><span class="n">y1</span><span class="p">,</span><span class="n">y2</span><span class="p">],</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Maligno&#39;</span><span class="p">,</span><span class="s1">&#39;Benigno&#39;</span><span class="p">,</span><span class="s1">&#39;Decision Boundary&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1690e955413f307a389f832d014bd2df45c269c33d1a327bd15a415160c45f76.png" src="../_images/1690e955413f307a389f832d014bd2df45c269c33d1a327bd15a415160c45f76.png" />
</div>
</div>
<p>In generale, siano <span class="math notranslate nohighlight">\(\mathbf{x}_A\)</span> e <span class="math notranslate nohighlight">\(\mathbf{x}_B\)</span> due punti distinti che si trovano sul decision boundary. Si ha:</p>
<div class="math notranslate nohighlight">
\[z(\mathbf{x}_A)=z(\mathbf{x}_B)=0 \Rightarrow \mathbf{w}^T(\mathbf{x}_A-\mathbf{x}_B)=0\]</div>
<p>da cui evinciamo che <strong>il vettore w √® ortogonale all‚Äôiperpiano che separa i dati (il decision bounday)</strong>. Inoltre, la distanza tra l‚Äôiperpiano e l‚Äôorigine √® data da:</p>
<div class="math notranslate nohighlight">
\[\frac{|\mathbf{w}^T \mathbf{0}+w_0|}{||\mathbf{w}||} = \frac{|w_0|}{||\mathbf{w}||}\]</div>
<p>Non dobbiamo inoltre dimenticare che, dato che proiettiamo i dati mediante l‚Äôespressione <span class="math notranslate nohighlight">\(z(\mathbf{x})=\mathbf{w}^Tx\)</span>, <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> rappresenta la direzione lungo la quale i dati saranno proiettati.</p>
<p>Possiamo dunque interpretare i parametri trovati come segue:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}\)</span> indica la direzione lungo la quale i dati verranno proiettati ed √® <strong>ortogonale</strong> al decision bounday;</p></li>
<li><p><span class="math notranslate nohighlight">\(w_0\)</span> indica la distanza tra l‚Äôorigine e il decision boundary secondo la formula <span class="math notranslate nohighlight">\(\frac{|w_0|}{||\mathbf{w}||}\)</span></p></li>
</ul>
<p>Vediamo di graficare il vettore <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="o">==</span><span class="n">c</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">bcancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="n">x2</span> <span class="o">=</span> <span class="mi">20</span><span class="p">;</span>
<span class="n">y1</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x1</span><span class="o">+</span><span class="n">y0</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y2</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="o">+</span><span class="n">y0</span><span class="o">/</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">],[</span><span class="n">y1</span><span class="p">,</span><span class="n">y2</span><span class="p">],</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Maligno&#39;</span><span class="p">,</span><span class="s1">&#39;Benigno&#39;</span><span class="p">,</span><span class="s1">&#39;Decision Boundary&#39;</span><span class="p">,</span><span class="s1">&#39;Direzione di Proiezione&#39;</span><span class="p">],</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c25794b67c69c2c88d51979f39bc30e5d80bd87032408cfe139174de96fcfa2b.png" src="../_images/c25794b67c69c2c88d51979f39bc30e5d80bd87032408cfe139174de96fcfa2b.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 16</strong></p>
<p>Calcolare il valore <span class="math notranslate nohighlight">\(\frac{|w_0|}{||\mathbf{w}||}\)</span> e segnare sul grafico sopra a quale segmento corrisponde questa lunghezza. Il valore ottenuto √® verosimile?</p>
</div></blockquote>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 17</strong></p>
<p>Si considerino altre possibili direzioni di proiezioni dei dati. Esse permettono una maggiore separabilit√† dei dati?</p>
</div></blockquote>
</section>
</section>
<section id="linear-discriminant-analysis">
<h2>Linear Discriminant Analysis<a class="headerlink" href="#linear-discriminant-analysis" title="Permalink to this heading">#</a></h2>
<p>Scikit-Learn contiene una implementazione ottimizzata della LDA. Vediamo come utilizzarla:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">bcancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
              solver=&#39;svd&#39;, store_covariance=False, tol=0.0001)
</pre></div>
</div>
</div>
</div>
<p>Una volta allenato il modello, possiamo effettuare direttamente le predizioni come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Possiamo anche ottenere l‚Äôaccuracy del modello come segue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">bcancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8857644991212654
</pre></div>
</div>
</div>
</div>
<p>Vediamo come accedere ad alcuni dei parametri interni:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficienti della LDA (w)&quot;</span><span class="p">,</span><span class="n">lda</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Valore soglia (w_0)&quot;</span><span class="p">,</span><span class="n">lda</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Medie dei valori in ingresso</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">lda</span><span class="o">.</span><span class="n">means_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficienti della LDA (w) [[-0.90292832 -0.2221124 ]]
Valore soglia (w_0) [18.27761577]
Medie dei valori in ingresso
 [[17.46283019 21.60490566]
 [12.14652381 17.9147619 ]]
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 18</strong></p>
<p>Si confrontino i risultati ottenuti mediante l‚Äôimplementazione di scikit-learn con quelli ottenuti prima in maniera manuale. I due classificatori sono simili o equivalenti?</p>
</div></blockquote>
<p>Visualizziamo il decision boundary per un semplice problema di classificazione con LDA:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>

<span class="c1"># Generate synthetic data with two predictors and three classes</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">mean_class0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">mean_class1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="n">mean_class2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>

<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">class0_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean_class0</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
<span class="n">class1_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean_class1</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
<span class="n">class2_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean_class2</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">class0_samples</span><span class="p">,</span> <span class="n">class1_samples</span><span class="p">,</span> <span class="n">class2_samples</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_samples</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_samples</span><span class="p">),</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)))</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Apply Linear Discriminant Analysis</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">store_covariance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Generate a meshgrid for visualization</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span><span class="p">])</span>
<span class="n">m2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">x_max</span><span class="p">,</span> <span class="n">y_max</span><span class="p">])</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>

<span class="c1"># Predict the class for each point in the meshgrid</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot the data, decision regions, and Gaussian fits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Plot the samples for each class</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">class0_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class0_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">class1_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class1_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">class2_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class2_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1"># Plot the decision regions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>

<span class="c1"># Fit Gaussian distributions to each class</span>
<span class="n">class0_gaussian</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">covariance_</span><span class="p">)</span>
<span class="n">class1_gaussian</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">covariance_</span><span class="p">)</span>
<span class="n">class2_gaussian</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">covariance_</span><span class="p">)</span>

<span class="c1"># Plot the Gaussian fits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">class0_gaussian</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">class1_gaussian</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">class2_gaussian</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear Discriminant Analysis with Decision Regions and Gaussian Fits (3 Classes)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9173247dfc4993f717e17ee1b0f59e9e35d9d6a7687cbf8af1f3e2d6f20bca09.png" src="../_images/9173247dfc4993f717e17ee1b0f59e9e35d9d6a7687cbf8af1f3e2d6f20bca09.png" />
</div>
</div>
<section id="riduzione-della-dimensionalita-con-lda">
<h3>Riduzione della Dimensionalit√† con LDA<a class="headerlink" href="#riduzione-della-dimensionalita-con-lda" title="Permalink to this heading">#</a></h3>
<p>La LDA pu√≤ essere utlizzata anche per riduzioene della dimensionalit√†. In questo caso, i dati verranno proiettati in uno spazio <span class="math notranslate nohighlight">\(K-1\)</span> dimensionale, dove <span class="math notranslate nohighlight">\(K\)</span> √® il numero totale di classi. La proiezione viene sempre effettuata in maniera tale da garantire la separabilit√† della classi. In generale dunque, l‚Äôanalisi LDA trover√† una matrice di pesi <span class="math notranslate nohighlight">\(W\)</span> di dimensione <span class="math notranslate nohighlight">\(D \times K-1\)</span> che permetta di trasformare i dati in una nuova matrice <span class="math notranslate nohighlight">\(Y\)</span> di dimensione <span class="math notranslate nohighlight">\(K-1 \times N\)</span>:</p>
<div class="math notranslate nohighlight">
\[Y=W^T X\]</div>
<p>I nuovi dati possono essere utilizzati per allenare successivamente un classificatore. Consideriamo ad esempio il dataset degli iris di Fisher:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.datasets</span> <span class="kn">import</span> <span class="n">get_rdataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">get_rdataset</span><span class="p">(</span><span class="s1">&#39;iris&#39;</span><span class="p">)</span>
<span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Il dataset contiene quattro features e tre classi. Utilizzando la tecnica LDA possiamo proiettare i dati in uno spazio bidimensionale nel quale le classi risultino massimamente separate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Species&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Species</span><span class="p">)</span>
<span class="n">iris_lda</span><span class="o">=</span><span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Species&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris_lda</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(150, 2)
</pre></div>
</div>
</div>
</div>
<p>Abbiamo cos√¨ proiettato i dati su due dimensioni. Proviamo a plottare i dati classe per classe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="c1">#otteniamo le tre classi uniche dal dataset</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
    <span class="n">class_data</span> <span class="o">=</span> <span class="n">iris_lda</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">s</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">class_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">class_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f20d02aba9864cd0b62ed6a8f4f60d0b033db88ee129ef0725c05459e8aeefd0.png" src="../_images/f20d02aba9864cd0b62ed6a8f4f60d0b033db88ee129ef0725c05459e8aeefd0.png" />
</div>
</div>
<p>Le classi sono adesso linearmente separabili. Possiamo provare ad allenare un classificatore Naive Bayes Gaussiano a partire da queste features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_lda</span><span class="p">,</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Species</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">iris_lda</span><span class="p">,</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Species</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.98
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="classificazione-maximum-a-posteriori-map">
<h2>Classificazione Maximum a Posteriori (MAP)<a class="headerlink" href="#classificazione-maximum-a-posteriori-map" title="Permalink to this heading">#</a></h2>
<p>Abbiamo visto come costruire un classificatore che, mediante la selezione di una opportuna soglia, ci permetta di predire il sesso di un soggetto in base alla sua altezza. Vediamo adesso come implementare un classificatore Maximum a Posteriori per raggiungere lo stesso scopo. Consideriamo le variabili aleatorie <span class="math notranslate nohighlight">\(S\)</span> rappresentante il sesso dei soggetti presenti nel dataset (pu√≤ assumere i valori M o F) e <span class="math notranslate nohighlight">\(H\)</span> rappresentante l‚Äôaltezza dei soggetti. Il nostro obiettivo √® scegliere il valore di <span class="math notranslate nohighlight">\(S\)</span> pi√π probabile sapendo che <span class="math notranslate nohighlight">\(H\)</span> assume un dato valore <span class="math notranslate nohighlight">\(h\)</span> (es. <span class="math notranslate nohighlight">\(h=175\ cm\)</span>). Secondo il principio MAP, sceglieremo la classe <span class="math notranslate nohighlight">\(s\)</span> che massimizza l‚Äôespressione:</p>
<div class="math notranslate nohighlight">
\[P(S=s\ |\ H=h)\]</div>
<p>Ci√≤ si scrive formalmente come segue:</p>
<div class="math notranslate nohighlight">
\[s^* = \arg\max_s P(S=s\ |H=h)\]</div>
<p>dove <span class="math notranslate nohighlight">\(s^*\)</span> √® la classe predetta. Dato che <span class="math notranslate nohighlight">\(S\)</span> pu√≤ assumere solo due valori, <span class="math notranslate nohighlight">\(M\)</span> o <span class="math notranslate nohighlight">\(F\)</span>, non resta che calcolare:</p>
<div class="math notranslate nohighlight">
\[P(S=M\ |\ H=h)\]</div>
<div class="math notranslate nohighlight">
\[P(S=F\ |\ H=h)\]</div>
<p>e scegliere la classe per la quale la probabilit√† a posteriori √® maggiore. Ricordiamo che secondo la regola di Bayes:</p>
<div class="math notranslate nohighlight">
\[P(S\ |\ H=h) = \frac{P(H=h \ | \ S)\ P(S)}{P(H=h)}\]</div>
<p>Per entrambe le probabilit√† <span class="math notranslate nohighlight">\(P(S=M\ |\ H=h)\)</span> e <span class="math notranslate nohighlight">\(P(S=F\ |\ H=h)\)</span>, il termine <span class="math notranslate nohighlight">\(P(H=h)\)</span> √® identico. Dato che il nostro scopo √® quello di confrontare le due quantit√† possiamo ometterlo. Dobbiamo dunque confrontare le seguenti quantit√†:</p>
<div class="math notranslate nohighlight">
\[P(H=h\ |\ S=M) P(S=M)\]</div>
<div class="math notranslate nohighlight">
\[P(H=h\ |\ S=F) P(S=F)\]</div>
<p>In pratica, spesso si assumono probabilit√† a priori identiche (<span class="math notranslate nohighlight">\(P(S=M)=P(S=F)=\frac{1}{2}\)</span>). In tal caso il nostro classificatore pu√≤ essere espresso cos√¨:</p>
<div class="math notranslate nohighlight">
\[\begin{split}s^* = \begin{cases}
M &amp;se&amp; P(H=h\ |\ S=M) \geq P(H=h\ |\ S=F) \\
F &amp;altrimenti &amp;
\end{cases}\end{split}\]</div>
<p>Dobbiamo dunque trovare un modo per <strong>rappresentare la verosimiglianza</strong> <span class="math notranslate nohighlight">\(P(H\ | \ S)\)</span> in modo da poterne calcolare i valori per ogni valore di <span class="math notranslate nohighlight">\(H\)</span> e di <span class="math notranslate nohighlight">\(S\)</span>. Osservando che <span class="math notranslate nohighlight">\(S\)</span> pu√≤ assumere solo due valori, possiamo rappresentare due funzioni di <span class="math notranslate nohighlight">\(H\)</span> distinte:</p>
<div class="math notranslate nohighlight">
\[P(H\ |\ S=M)\]</div>
<div class="math notranslate nohighlight">
\[P(H\ |\ S=F)\]</div>
<p>Secondo i principi della probabilit√† Bayesiana, queste due probabilit√† possono essere modellate sulla base delle nostre <strong>credenze</strong> (<strong>assunzioni</strong>) sui dati. In particolare, ci siamo gi√† ‚Äúconvinti‚Äù in passato che <span class="math notranslate nohighlight">\(H\)</span> si distribuisce <strong>approssimativamente</strong> secondo una Gaussiana. Possiamo dunque modellare le due probabilit√† viste con due Gaussiane:</p>
<div class="math notranslate nohighlight">
\[P(H=h\ |\ S=M) = G(h; \mu_M,\sigma_M^2)\]</div>
<div class="math notranslate nohighlight">
\[P(H=h\ |\ S=F) = G(h; \mu_F,\sigma_F^2)\]</div>
<p>Dove <span class="math notranslate nohighlight">\(\mu_M\)</span> e <span class="math notranslate nohighlight">\(\mu_F\)</span> sono le medie stimate dai due campioni di altezze maschili e femminili mentre <span class="math notranslate nohighlight">\(\sigma_M^2\)</span> e <span class="math notranslate nohighlight">\(\sigma_F^2\)</span> sono le due varianze stimate dai campioni.</p>
<p>Una volta definite le due Gaussiane, il nostro classificatore diventa:</p>
<div class="math notranslate nohighlight">
\[\begin{split}s^* = \begin{cases}
M &amp;se&amp; G(h; \mu_M,\sigma_M^2) \geq G(h; \mu_F,\sigma_F^2) \\
F &amp;altrimenti &amp;
\end{cases}\end{split}\]</div>
<p>Vediamo ora di tradurre tutto in termini computazionali e di implementare il nostro classificatore <strong>MAP</strong> con assunzione di Gaussianit√† delle altezze. Utilizzeremo nuovamente il dataset di pesi e altezze:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;http://iplab.dmi.unict.it/furnari/downloads/height_weight.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>BMI</th>
      <th>height</th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>M</td>
      <td>33.36</td>
      <td>187.96</td>
      <td>117.933920</td>
    </tr>
    <tr>
      <th>1</th>
      <td>M</td>
      <td>26.54</td>
      <td>177.80</td>
      <td>83.914520</td>
    </tr>
    <tr>
      <th>2</th>
      <td>F</td>
      <td>32.13</td>
      <td>154.94</td>
      <td>77.110640</td>
    </tr>
    <tr>
      <th>3</th>
      <td>M</td>
      <td>26.62</td>
      <td>172.72</td>
      <td>79.378600</td>
    </tr>
    <tr>
      <th>4</th>
      <td>F</td>
      <td>27.13</td>
      <td>167.64</td>
      <td>76.203456</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Suddividiamo l‚Äôinsieme in training e test set utilizzando il <span class="math notranslate nohighlight">\(25\%\)</span> dei dati come test set e il restante <span class="math notranslate nohighlight">\(75\%\)</span> come training set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">data_train</span><span class="p">,</span> <span class="n">data_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Effettuiamo dunque il fitting di due Gaussiane sui dati delle altezze maschili e femminili:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1">#la notazione sotto √® una notazione compatta</span>
<span class="c1">#per definire una distribuzione normale e fittarla sui dati</span>

<span class="c1">#costruiamo due distribuzioni normali separate per i due set di dati</span>
<span class="n">g_m</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">][</span><span class="s1">&#39;height&#39;</span><span class="p">]))</span>
<span class="n">g_f</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;F&#39;</span><span class="p">][</span><span class="s1">&#39;height&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>Plottiamo dunque le due Gaussiane considerando un range di valori <span class="math notranslate nohighlight">\(x\)</span> comune:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">g_f</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">g_m</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.999</span><span class="p">),</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g_m</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g_f</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;M&#39;</span><span class="p">,</span><span class="s1">&#39;F&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b289b88515678bf2a448be2126565c8ffa0ef101965269b2cf639d1da9929e98.png" src="../_images/b289b88515678bf2a448be2126565c8ffa0ef101965269b2cf639d1da9929e98.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 19</strong></p>
<p>Cosa ci dice il plot? Sapendo che un individuo √® alto 160 cm, √® pi√π probabile che sia una donna o un uomo? Cosa possiamo dire di un individuo alto 180 cm? E di un individuo alto 170 cm? Esiste un punto soglia al di l√† del quale conviene classificare le altezze come ‚Äúuomini‚Äù?</p>
</div></blockquote>
<p>Calcoliamo dunque le probabilit√† <span class="math notranslate nohighlight">\(P(H\ |\ S)\)</span> per tutti i valori del test set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_m</span> <span class="o">=</span> <span class="n">g_m</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span>
<span class="n">prob_f</span> <span class="o">=</span> <span class="n">g_f</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>A questo punto costruiamo un vettore <code class="docutils literal notranslate"><span class="pre">pred_male</span></code> che contenga <code class="docutils literal notranslate"><span class="pre">True</span></code> se pensiamo che il soggetto sia un uomo, e dunque se <code class="docutils literal notranslate"><span class="pre">prob_m</span></code> √® maggiore di <code class="docutils literal notranslate"><span class="pre">prob_f</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_male</span> <span class="o">=</span> <span class="n">prob_m</span><span class="o">&gt;=</span><span class="n">prob_f</span>
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo FPR, TPR e accuracy del classificatore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="n">male_gt</span> <span class="o">=</span> <span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;M&#39;</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">pred_male</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">pred_male</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False Positive Rate: 0.12
True Positive Rate: 0.81
Accuracy: 0.85
</pre></div>
</div>
</div>
</div>
<p>Abbiamo visto come l‚Äôapproccio MAP ci permetta di definire un classificatore senza la necessit√† di definire una soglia. In pratica, il criterio MAP identifica un punto (una soglia) nello spazio dei valori di <span class="math notranslate nohighlight">\(H\)</span> <strong>oltre il quale</strong> √® pi√π conveniente predire ‚ÄúM‚Äù piuttosto che ‚ÄúF‚Äù.</p>
<p>Tale punto √® quello in cui le due Gaussiane si intersecano e si trova (a occhio) intorno a 170cm. Per tutti i valori superiori a tale valore soglia, la Gaussiana relativa a ‚ÄúM‚Äù dar√† sempre valori maggiori di quelli della Gaussiana relativa a ‚ÄúF‚Äù e viceversa, al di sotto del valore soglia, la Gaussiana relativa a ‚ÄúF‚Äù dar√† valori superiori a quelli della Gaussiana relativa a ‚ÄúM‚Äù. Se volessimo trovare il valore soglia analiticamente, dovremmo risolvere l‚Äôequazione:</p>
<div class="math notranslate nohighlight">
\[G_M(x)=G_F(x)\]</div>
<p>nella variabile <span class="math notranslate nohighlight">\(x\)</span>. Tale equazione si riconduce alla forma:</p>
<p><span class="math notranslate nohighlight">\(ax^2 + bx + c=0\)</span></p>
<p>con:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a = \frac{1}{2\sigma_1^2} - \frac{1}{2\sigma_2^2}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(b = \frac{\mu_2}{\sigma_2^2} - \frac{\mu_1}{\sigma_1^2}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(c = \frac{m_1^2}{2 \sigma_1^2} - \frac{m_2^2}{2 \sigma_2^2} - \log\frac{\sigma_2}{\sigma_1}\)</span>.</p></li>
</ul>
<p>Possiamo risolvere l‚Äôequazione mediante il metodo <strong>roots</strong> di <strong>numpy</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Una funzione per risolvere l&#39;equazione vista sopra</span>
<span class="k">def</span> <span class="nf">risolvi</span><span class="p">(</span><span class="n">mu_1</span><span class="p">,</span><span class="n">mu_2</span><span class="p">,</span><span class="n">sigma_1</span><span class="p">,</span><span class="n">sigma_2</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">mu_2</span><span class="o">/</span><span class="p">(</span><span class="n">sigma_2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu_1</span><span class="o">/</span><span class="p">(</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">mu_1</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu_2</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma_2</span><span class="o">/</span><span class="n">sigma_1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">roots</span><span class="p">([</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">])</span>

<span class="n">solutions</span> <span class="o">=</span> <span class="n">risolvi</span><span class="p">(</span><span class="n">g_m</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">g_f</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">g_m</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">g_f</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">solutions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[170.35818328  43.85968878]
</pre></div>
</div>
</div>
</div>
<p>L‚Äôequazione ha due soluzioni. La prima √® il punto in cui le Gaussiane si intersecano al centro del grafico, mentre la seconda √® relativa a un punto in cui le Gaussine si interseacano a sinistra (non visibile nel grafico). Plottiamo la posizione in cui cade la prima delle due soluzioni:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">g_f</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">g_m</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.999</span><span class="p">),</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g_m</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">g_f</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;M&#39;</span><span class="p">,</span><span class="s1">&#39;F&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">solutions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">solutions</span><span class="p">[</span><span class="mi">0</span><span class="p">]],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.066</span><span class="p">],</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6969e522102db99655381c8e2a30daa133f8b1c9f22c16053bce67c92d846973.png" src="../_images/6969e522102db99655381c8e2a30daa133f8b1c9f22c16053bce67c92d846973.png" />
</div>
</div>
<p>In pratica il calssificatore <strong>MAP</strong> √® equivalente (o quasi, vista la presenza di due soluzioni) a un classificatore basato su soglia per il quale la soglia ottimale √® stata determinata automaticamente. Va notato che il calssificatore MAP trovato √® molto simile a un classificatore basato su soglia anche dal momento che poche altezze saranno inferiori a <span class="math notranslate nohighlight">\(50\ cm\)</span> (il secondo punto di intersezione tra le Gaussiane).</p>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 20</strong></p>
<p>Confrontare la soglia con quella trovata mediante il metodo di ottimizzazione visto nella Sezione 1. Le due soglie sono simili? A che cosa √® dovuta la differenza tra le due soglie?</p>
</div></blockquote>
<section id="classificatore-basato-su-distanza-di-mahalanobis">
<h3>Classificatore Basato su Distanza di Mahalanobis<a class="headerlink" href="#classificatore-basato-su-distanza-di-mahalanobis" title="Permalink to this heading">#</a></h3>
<p>Un altro metodo di classificazione consiste nel misurare la distanza dell‚Äôelemento <span class="math notranslate nohighlight">\(h\)</span> da classificare rispetto alle due popolazioni alle quali esso pu√≤ appartenere. Tale misura di distanza viene ottenuta mediante la <strong>distanza di Mahalanobis</strong>. La distanza di <strong>Mahalanobis</strong> tra un vettore <span class="math notranslate nohighlight">\(\mathbf x = (x_1, \ldots, x_n)^T\)</span> multivariato e un gruppo di valori di valor medio <span class="math notranslate nohighlight">\(\mathbf \mu = (\mu_1, \ldots, \mu_n)^T\)</span> e matrice di covarianza <span class="math notranslate nohighlight">\(\Sigma\)</span> si definisce come:</p>
<div class="math notranslate nohighlight">
\[D_M(\mathbf{x}) = \sqrt{(\mathbf{x}-\mathbf{\mu})^T \Sigma^{-1}(\mathbf{x}-\mathbf{\mu})}\]</div>
<p>Considerando la matrice di covarianza, la distanza di <strong>Mahalanobis</strong> tiene conto della correlazione che sussiste tra le variabili del gruppo di valori considerato.</p>
<p>Nel caso di un campione <strong>monovariato</strong> <span class="math notranslate nohighlight">\(x\)</span> e di un gruppo di valori di media <span class="math notranslate nohighlight">\(\mu\)</span> e varianza <span class="math notranslate nohighlight">\(\sigma^2\)</span>, la distanza di Mahalanobis resta definita come segue:</p>
<div class="math notranslate nohighlight">
\[D_M(x) = \sqrt{(\frac{x-\mu}{\sigma})^2}\]</div>
<p>Una interpretazione della distanza di Mahalanobis nel caso monovariato √® che essa misura lo scostamento di un valore dalla media di un campione in termini di deviazioni standard <span class="math notranslate nohighlight">\(\sigma\)</span>. Ci√≤ permette di pesare le distanze da campioni <strong>pi√π dispersi</strong> in maniera minore rispetto alle distanze da campioni <strong>meno dispersi</strong>.</p>
<p>Vediamo un esempio. Consideriamo due popolazioni normali di media e varianza <span class="math notranslate nohighlight">\(\mu_1=-5, \mu_2=3, \sigma_1=3, \sigma_2=1\)</span>. Consideriamo inoltre il punto <span class="math notranslate nohighlight">\(x=0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu_1</span><span class="o">=-</span><span class="mi">5</span>
<span class="n">mu_2</span><span class="o">=</span><span class="mi">3</span>
<span class="n">sigma_1</span><span class="o">=</span><span class="mi">3</span>
<span class="n">sigma_2</span><span class="o">=</span><span class="mi">1</span>

<span class="n">g1</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">mu_1</span><span class="p">,</span><span class="n">sigma_1</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">mu_2</span><span class="p">,</span><span class="n">sigma_2</span><span class="p">)</span>

<span class="n">x</span><span class="o">=</span><span class="mi">0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">dom</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dom</span><span class="p">,</span><span class="n">g1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">dom</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dom</span><span class="p">,</span><span class="n">g2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">dom</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Popolazione 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Popolazione 2&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9a77d5103ad544fffa17b473045a453605c6b8ec23bbe4682a7d7ac02aeb7c50.png" src="../_images/9a77d5103ad544fffa17b473045a453605c6b8ec23bbe4682a7d7ac02aeb7c50.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 21</strong></p>
<p>A quale delle due popolazioni appartiene il punto? Qual √® un buon criterio per determinarlo?</p>
</div></blockquote>
<p>Ci chiediamo dunque a quale popolazione appartenga il punto <span class="math notranslate nohighlight">\(x1\)</span>. Iniziamo calcolando la distanza euclidea tra il punto e le medie delle due popolazioni:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dist_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu_1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">dist_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu_2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distanza dalla media della prima popolazione:&quot;</span><span class="p">,</span><span class="n">dist_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distanza dalla media della seconda popolazione:&quot;</span><span class="p">,</span><span class="n">dist_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Distanza dalla media della prima popolazione: 25
Distanza dalla media della seconda popolazione: 9
</pre></div>
</div>
</div>
</div>
<p>Il punto √® pi√π vicino alla media della seconda popolazione, il che potrebbe portarci a concludere che esso appartiene ad essa. Tuttavia, se calcoliamo le probabilit√†:</p>
<div class="math notranslate nohighlight">
\[P(x\ |\ Popolazione = 1) = G(x;\mu_1,\sigma_1)\]</div>
<div class="math notranslate nohighlight">
\[P(x\ |\ Popolazione = 2) = G(x;\mu_2,\sigma_2)\]</div>
<p>scopriamo che:</p>
<div class="math notranslate nohighlight">
\[P(x\ |\ Popolazione=1) &gt; P(x\ |\ Popolazione = 2)\]</div>
<p>Vediamolo in termini computazionali:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probabilit√† P(x | Popolazione = 1)=&quot;</span><span class="p">,</span><span class="n">g1</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probabilit√† P(x | Popolazione = 2)=&quot;</span><span class="p">,</span><span class="n">g2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Probabilit√† P(x | Popolazione = 1)= 0.03315904626424956
Probabilit√† P(x | Popolazione = 2)= 0.0044318484119380075
</pre></div>
</div>
</div>
</div>
<p>Dunque, per il criterio MAP, la nostra conclusione (ovvero che il punto appartiene alla seconda popolazione) √® errata! Ci√≤ nasce dall‚Äôaver ignorato il fatto che le due popolazioni hanno varianze diverse. Se invece delle distanze Euclidee calcoliamo le distanze di Mahalanobis e confrontiamo i loro valori assoluti, otteniamo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mx_p1</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu_1</span><span class="p">)</span><span class="o">/</span><span class="n">sigma_1</span>
<span class="n">mx_p2</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu_2</span><span class="p">)</span><span class="o">/</span><span class="n">sigma_2</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">mx_p1</span><span class="p">),</span><span class="nb">abs</span><span class="p">(</span><span class="n">mx_p2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.6666666666666667 3.0
</pre></div>
</div>
</div>
</div>
<p>Il punto √® pi√π vicino alla prima popolazione e dunque va classificato come tale!</p>
<p>Torniamo al caso del classificatore basato sulle altezze. Considerate le due popolazioni Gaussiane, possiamo calcolare le distanze di Mahalanobis di un dato valore <span class="math notranslate nohighlight">\(h\)</span> rispetto alle due popolazioni. Il valore <span class="math notranslate nohighlight">\(h\)</span> verr√† classificato come appartenente alla popolazione dalla quale si √® misurata una distanza (in valore assoluto) minore. Vediamolo in termini computazionali:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#calcoliamo le distanze per i dati di test</span>
<span class="c1">#utilizzando media e varianza calcoalte sul training set</span>
<span class="n">dist_male</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">height</span><span class="o">-</span><span class="n">g_m</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="n">g_m</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">dist_female</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">height</span><span class="o">-</span><span class="n">g_f</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="n">g_f</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Adesso classifichiamo come uomini tutti i casi in cui la distanza dalla popolazione degli uomini √® in valore assoluto minore della distanza dalla popolazione delle donne:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_male</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">dist_male</span><span class="p">)</span><span class="o">&lt;</span><span class="n">npw</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">dist_female</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo l‚Äôaccuracy del nuovo classificatore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">pred_male</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span> <span class="n">pred_male</span><span class="p">)</span>
<span class="n">cm</span><span class="o">=</span><span class="n">cm</span><span class="o">/</span><span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tpr</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positive Rate: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tpr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">{:0.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False Positive Rate: 0.20
True Positive Rate: 0.88
Accuracy: 0.83
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 22</strong></p>
<p>Si confronti la matrice di confusione del classificatore basato sulla distanza di Mahalanobis con quella relativa al classificatore MAP precedentemente allenato. I due metodi sono equivalenti?</p>
</div></blockquote>
</section>
</section>
<section id="naive-bayes">
<h2>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this heading">#</a></h2>
<p>Abbiamo visto come il principio MAP ci permette di costruire un semplice classificatore basato sull‚Äôosservazione dei valori di una variabile casuale. Spesso per√≤ pu√≤ capitare che la classe di appartenenza di un determinato elemento possa dipendere dai valori di pi√π variabili casuali. Consideriamo ad esempio le variabili aletaorie:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span> indica il sesso e pu√≤ assumere come valori ‚ÄúM‚Äù o ‚ÄúF‚Äù;</p></li>
<li><p><span class="math notranslate nohighlight">\(H\)</span> indica l‚Äôaltezza del soggetto;</p></li>
<li><p><span class="math notranslate nohighlight">\(W\)</span> indica il peso del soggetto;</p></li>
</ul>
<p>Se volessimo classificare i soggetti sulla base dei valori delle due variabili <span class="math notranslate nohighlight">\(H\)</span> e <span class="math notranslate nohighlight">\(B\)</span>, per il principio MAP dovremmo trovare il valore di <span class="math notranslate nohighlight">\(S\)</span> che massimizza la probabilit√† a posteriori:</p>
<div class="math notranslate nohighlight">
\[P(S\ |\ H,W) = \frac{P(H,W\ |\ S)\ P(S)}{P(H,W)}\]</div>
<p>Dato che vogliamo massimizzare la probabilit√† a posteriori, possiamo tralasciare il denominatore e scrivere:</p>
<div class="math notranslate nohighlight">
\[s^* = \arg\max_s P(S = s\ |\ H,W) = \arg\max_s P(H,W\ |\ S = s) P(S = s)\]</div>
<p>L‚Äôespressione da massimizzare corrisponde alla distribuzione congiunta di <span class="math notranslate nohighlight">\(H\)</span> e <span class="math notranslate nohighlight">\(W\)</span> dato <span class="math notranslate nohighlight">\(S\)</span>. Coinvolgendo pi√π variabili aleatorie, questa probabilit√† √® difficile da modellare.</p>
<p>Cerchiamo dunque di ‚Äúmanopolare‚Äù l‚Äôespressione vista sopra per semplificare il problema. Possiamo sviluppare l‚Äôespressione come segue:</p>
<div class="math notranslate nohighlight">
\[P(H,W\ |\ S) P(S) =  P(H,W,S) = P(H\ |\ W,S)\ P(W,S) = P(H\ |\ W,S)\ P(W\ |\ S)\ P(S)\]</div>
<p>Un classificatore Naive Bayes fa a questo punto l‚Äôassunzione (naive per l‚Äôappunto) che le variabili <span class="math notranslate nohighlight">\(H\)</span> e <span class="math notranslate nohighlight">\(W\)</span> siano <strong>condizionalmente indipendenti l‚Äôuna rispetto all‚Äôaltra data la classe S</strong>, ovvero che:</p>
<div class="math notranslate nohighlight">
\[H \perp\!\!\!\perp W\ |\ S\]</div>
<p>Va notato che l‚Äôassunzione √® <strong>davvero Naive</strong> in quanto assume che <strong>una volta nota la classe di appartenenza, il valore di B non ci dice nulla sul valore di H e, viceversa, il valore di H non ci dice nulla sul valore B</strong>. Ci√≤ implicherebbe che, considerato l‚Äôinsieme delle donne (<span class="math notranslate nohighlight">\(S\)</span> noto), allora a un valore di <span class="math notranslate nohighlight">\(H\)</span> ad esempio pari a <span class="math notranslate nohighlight">\(150 cm\)</span> pu√≤ corrispondere un qualsiasi valore di <span class="math notranslate nohighlight">\(W\)</span> . In realt√† sappiamo che questa condizione √® spesso falsa (le variabili che rappresentano diversi aspetti di un fenomeno tendono a influenzarsi). Nonostante tale assunzione, i classificatori Naive Bayes restano uno strumento molto potente in grado di gestire con facilit√† un grosso numero di variabili indipendenti.</p>
<p>L‚Äôindipendenza condizionale tra <span class="math notranslate nohighlight">\(H\)</span> e <span class="math notranslate nohighlight">\(W\)</span> implica che:</p>
<div class="math notranslate nohighlight">
\[P(H\ |\ W,S) = P(H\ |\ S)\]</div>
<p>da cui otteniamo:</p>
<div class="math notranslate nohighlight">
\[P(H,W\ |\ S) P(S) = P(H\ |\ W,S)\ P(W\ |\ S)\ P(S) = P(H\ |\ S)\ P(W\ |\ S)\ P(S)\]</div>
<p>Assumendo nuovamente la distribuzione a priori sui sessi uniforme (<span class="math notranslate nohighlight">\(P(s)=0.5\)</span>), la nostra ottimizzazione diventa:</p>
<div class="math notranslate nohighlight">
\[s^* = \arg\max_s P(H\ |\ S = s)\ P(W\ |\ S = s)\]</div>
<p>Possiamo modellare le distribuzioni <span class="math notranslate nohighlight">\(P(H\ |\ S=s)\)</span> e <span class="math notranslate nohighlight">\(P(W\ |\ S=s)\)</span> con quattro Gaussiane (due per ogni variabile, una per sesso):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g_m_h</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">][</span><span class="s1">&#39;height&#39;</span><span class="p">]))</span>
<span class="n">g_f_h</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;F&#39;</span><span class="p">][</span><span class="s1">&#39;height&#39;</span><span class="p">]))</span>
<span class="n">g_m_w</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">][</span><span class="s1">&#39;weight&#39;</span><span class="p">]))</span>
<span class="n">g_f_w</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;F&#39;</span><span class="p">][</span><span class="s1">&#39;weight&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">dom</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">130</span><span class="p">,</span><span class="mi">220</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dom</span><span class="p">,</span><span class="n">g_m_h</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">dom</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dom</span><span class="p">,</span><span class="n">g_f_h</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">dom</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;M&#39;</span><span class="p">,</span><span class="s1">&#39;F&#39;</span><span class="p">])</span>

<span class="n">dom2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">160</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dom2</span><span class="p">,</span><span class="n">g_m_w</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">dom2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dom2</span><span class="p">,</span><span class="n">g_f_w</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">dom2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Weight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;M&#39;</span><span class="p">,</span><span class="s1">&#39;F&#39;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/226403d7fbca0803e6a049d4a3ea1549c52e12c0017397e6fceee8f7b30bdbc3.png" src="../_images/226403d7fbca0803e6a049d4a3ea1549c52e12c0017397e6fceee8f7b30bdbc3.png" />
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 23</strong></p>
<p>Quale delle due variabili sembra offrirci una minore incertezza? Come potremmo quantificare tale incertezza?</p>
</div></blockquote>
<p>A questo punto, per ogni elemento del test set, dobbiamo calcolare le probabilit√† <span class="math notranslate nohighlight">\(P(H\ |\ S = s)\ P(W\ |\ S = M)\)</span> e <span class="math notranslate nohighlight">\(P(H\ |\ S = s)\ P(W\ |\ S = F)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pm</span> <span class="o">=</span> <span class="n">g_m_h</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span><span class="o">*</span><span class="n">g_m_w</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span>
<span class="n">pf</span> <span class="o">=</span> <span class="n">g_f_h</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">])</span><span class="o">*</span><span class="n">g_f_w</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Classifichiamo ogni elemento controllando per quali valori si ha: <span class="math notranslate nohighlight">\(P(H\ |\ S = s)\ P(W\ |\ S = M) &gt;= P(H\ |\ S = s)\ P(W\ |\ S = F)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">male_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">&gt;=</span><span class="n">pf</span>
<span class="n">male_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([False,  True, False, ...,  True, False, False])
</pre></div>
</div>
</div>
</div>
<p>Calcoliamo l‚Äôaccuracy del classificatore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">male_gt</span><span class="p">,</span><span class="n">male_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8506616257088847
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>üôã‚Äç‚ôÇÔ∏è Domanda 24</strong></p>
<p>Si confrontino accuracy e matrice di confusione del classificatore Naive Bayes appena costruito con quelle relative al classificatore MAP costruito solo sulle altezze.</p>
</div></blockquote>
<p><strong>Scikit-Learn</strong> mette a disposizione una implementazione del classificatore Naive Bayes basato su assunzione Gaussiana. Vediamo come utilizzarla:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="c1">#passiamo priors=[0.5, 0.5] per assumere probabilit√† a priori uniformi</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">(</span><span class="n">priors</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>
<span class="c1">#effettuiamo il fit del modello mediante il metodo fit</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;weight&#39;</span><span class="p">]],</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GaussianNB(priors=[0.5, 0.5])
</pre></div>
</div>
</div>
</div>
<p>Possiamo classificare gli elementi del test set mediante il metodo <code class="docutils literal notranslate"><span class="pre">predict</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_test</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;weight&#39;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;F&#39;, &#39;M&#39;, &#39;F&#39;, ..., &#39;M&#39;, &#39;F&#39;, &#39;F&#39;], dtype=&#39;&lt;U1&#39;)
</pre></div>
</div>
</div>
</div>
<p>Possiamo calcolare l‚Äôaccuracy del classificatore direttamente mediante il metodo <code class="docutils literal notranslate"><span class="pre">score</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;weight&#39;</span><span class="p">]],</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8506616257088847
</pre></div>
</div>
</div>
</div>
<p>Proviamo adesso ad allenare il classificatore Naive Bayes utilizzando anche la variabile BMI:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">(</span><span class="n">priors</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span><span class="s1">&#39;BMI&#39;</span><span class="p">]],</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">,</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span><span class="s1">&#39;BMI&#39;</span><span class="p">]],</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8572778827977315
</pre></div>
</div>
</div>
</div>
<section id="implementazione-map-di-scikit-learn">
<h3>Implementazione MAP di Scikit-Learn<a class="headerlink" href="#implementazione-map-di-scikit-learn" title="Permalink to this heading">#</a></h3>
<p>Si noti che l‚Äôoggetto <code class="docutils literal notranslate"><span class="pre">Naive</span> <span class="pre">Bayes</span></code> permette anche di implementare un classificatore di tipo MAP specificando una unica variabile. Vediamo ad esempio come implementare un classificatore MAP basato sulla variabile <code class="docutils literal notranslate"><span class="pre">height</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="c1">#dobbiamo inserire values.reshape(-1,1) per trasformare il vettore monodimensionale</span>
<span class="c1">#in una matrice Nx1 (N osservazion, una variabile)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">data_train</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">data_test</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8478260869565217
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="esercizi">
<h2>Esercizi<a class="headerlink" href="#esercizi" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 1</p>
<p>Si suddivida il dataset Titanic in training e test set. Il test set deve contenere il <span class="math notranslate nohighlight">\(15\%\)</span> degli elementi. Si costruisca un regressore logistico per predire i valori della variabile <code class="docutils literal notranslate"><span class="pre">Survived</span></code> a partire dal training set. Si utilizzi il metodo della backward elimination per eliminare le variabili che non contribuiscono alla regressione in maniera significativa. Si testi il modello ottenuto sul test set considerando le seguenti misure di valutazione:</p>
<ul class="simple">
<li><p>Matrice di confusione;</p></li>
<li><p>Matrice di confusione normalizzata;</p></li>
<li><p>True Positive Rate;</p></li>
<li><p>False Positive Rate;</p></li>
<li><p>Numero di hit;</p></li>
<li><p>Accuracy.</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 2</p>
<p>Si testi il modello ottenuto nell‚Äôesercizio precedente sul training set utilizzando le stesse misure di valutazione. Si confrontino le performance di training con quelle di test. Il metodo generalizza?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 3</p>
<p>Si valutino tutti i modelli intermedi ottenuti mediante il metodo della backward elimination applicato all‚Äôesercizio 1 sul test set utilizzando l‚Äôaccuracy. Si plottino le accuracy rispetto ai numeri di parametri contenuti nei modelli. Cosa si apprende dal grafico?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 4</p>
<p>Si considerino due classificatori basati su soglia che predicono i risultati di <code class="docutils literal notranslate"><span class="pre">Survived</span></code> a partire dalle due variabili numeriche <code class="docutils literal notranslate"><span class="pre">Age</span></code> e <code class="docutils literal notranslate"><span class="pre">Fare</span></code>. Si plottino le relative curve ROC. Quale delle due variabili √® pi√π discriminativa? Si calcolino le soglie ottimali e si valutino i classificatori individuati mediante <strong>precision</strong>, <strong>recall</strong>, <strong><span class="math notranslate nohighlight">\(F_1\)</span> score</strong>, <strong>Accuracy</strong> e <strong>matrice di confusione</strong>. Quale dei due classificatori √® migliore?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 5</p>
<p>Si consideri il dataset di classificazione di vetri (glass). Lo si suddivida in training (<span class="math notranslate nohighlight">\(70%\)</span>) e test (<span class="math notranslate nohighlight">\(30%\)</span>) e si confrontino le accuracy e matrici di confusione ottenute con i seguenti classificatore:</p>
<ul class="simple">
<li><p>1-NN;</p></li>
<li><p>5-NN;</p></li>
<li><p>7-NN;</p></li>
<li><p>9-NN;</p></li>
<li><p>11-NN;</p></li>
<li><p>Regressore logistico;</p></li>
<li><p>Classificatore Naive Bayes.</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 6</p>
<p>Con riferimento all‚Äôesercizio precedente, si trovi il valore ottimale di K mediante cross validation e si alleni il classificatore K-NN con il valore trovato per K. Si applichino le tecniche di condensing e multi-editing e si confrontino le performance del K-NN sui dati originali e su quelli modificati.</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 7</p>
<p>Si carichi il dataset MNIST e si costruisca un training set selezionino 500 campioni casuali e un test set selezionando altri 100 campioni casuali. Si trovi il miglior parametro di <span class="math notranslate nohighlight">\(K\)</span> fra i valori <span class="math notranslate nohighlight">\(1,5,10,20,30,50\)</span> utilizzando la grid search sul training set. Che accuracy di test otteniamo con il miglior <span class="math notranslate nohighlight">\(K\)</span> trovato?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 8</p>
<p>Si consideri il dataset Breast Cancer. Si proiettino su una unica dimensione i dati mediante PCA e LDA. Si confrontino con due istogrammi le caratteristiche dei dati trasformati. In quali dei due spazi i dati sono pi√π linearmente separabili? Si calcolino due regressori logistici a partire dai due insiemi di dati. Quale trasformazione permette di ottenere il regressore migliore?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 9</p>
<p>Si consideri il dataset Breast Cancer. Si suddivida il dataset in training e test set. Si allenino sul training set i seguenti classificatori:</p>
<ul class="simple">
<li><p>Naive Bayes Gaussiano allenato sui dati di partenza;</p></li>
<li><p>Naive Bayes Gaussiano allenato sui dati trasformati mediante PCA;</p></li>
<li><p>Naive Bayes Gaussiano allenato sui dati trasformati mediante LDA.</p></li>
</ul>
<p>Si confrontino le performance di training e test dei classificatori ottenuti mediante matrici di confusione e accuracy. Quale classificatore ottiene una performance migliore?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 10</p>
<p>Si consideri il dataset delle Iris di Fisher. Si proiettino i dati su due dimensioni utilizzando PCA e LDA. Si visualizzino i dati ottenuti e si confrontino i due insiemi di dati trasformati. Quale set di dati √® pi√π facilmente separabile linearmente? Si allenino due classificatori Naive Bayes Gaussiani sui due insiemi di dati. Quali dei due classificatori ottiene performance migliori?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 10</p>
<p>Si consideri il dataset Titanic e lo si suddivida in training e test set. Si costruisca un calssificatore basato su soglia per determinare i valori di <code class="docutils literal notranslate"><span class="pre">Survived</span></code> a partire dai valori di <code class="docutils literal notranslate"><span class="pre">Age</span></code>. Si visualizzi la curva ROC del classificatore e si determini una soglia di classificazione usando un metodo a scelta. Si allenino un classificatore MAP e un regressore logistico per risolvere lo stesso problema. Si confrontino i risultati ottenuti dai tre classificatori sul test set mediante matrici di confusione, accuracy, precision e recall. Quale classificatore ottiene i risultati migliori?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 11</p>
<p>Con riferimento all‚Äôesercizio precedente, si alleni un classificatore Naive Bayes per predire i valori di <code class="docutils literal notranslate"><span class="pre">Survived</span></code> includendo oltre ad <code class="docutils literal notranslate"><span class="pre">Age</span></code> altre variabili. Si confrontino i risultati ottenuti sul test set con quelli ottenuti mediante il classificatore MAP basato su una unica variabile. Quale classificatore ottiene i risultati migliori?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 12</p>
<p>Si consideri il dataset Titanic e lo si suddivida in training e test set. Si alleni un classificatore di tipo Naive Bayes per inferire i valori della variabile <code class="docutils literal notranslate"><span class="pre">PClass</span></code> a partire da quelli delle altre variabili (si scartino le variabili poco utilizzabili quali ‚ÄúTicket‚Äù). Si confrontino le performance su training e test set. Il classificatore generalizza?</p>
</div></blockquote>
<blockquote>
<div><p>üßë‚Äçüíª Esercizio 13</p>
<p>Si consideri il dataset DIGITS e lo si suddivida in training e test set. Si calcoli la PCA sul training set e si utilizzi il modello ottenuto per trasformare i dati di training e test mantenendo solo le prime <span class="math notranslate nohighlight">\(10\)</span> componenti. Si alleni sul training set un classificatore Naive Bayes per classificare gli elementi del dataset e si valuti il classificatore sul test set. Si provi a diminuire e aumentare il numero di componenti principali considerate e si ripeta il processo. Si confrontino le performance sul test set dei classificatori allenati.</p>
</div></blockquote>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./laboratories"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misure-di-valutazione">Misure di Valutazione</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-e-testing-del-classificatore">Training e testing del classificatore</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#misure-di-valutazione-degli-errori">Misure di valutazione degli errori</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hit-true-positive-e-miss-false-negative">Hit (True Positive) e Miss (False Negative)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#errori-di-tipo-i-o-falsi-allarmi-false-positive">Errori di tipo I o falsi allarmi (False Positive)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#correct-rejection-true-negative">Correct Rejection (True Negative)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#riepilogo-tp-tn-fp-fn">Riepilogo: TP, TN, FP, FN</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrice-di-confusione">Matrice di Confusione</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizzazione-della-matrice-di-confusione-tnr-tpr-fnr-fpr">Normalizzazione della matrice di confusione - TNR, TPR, FNR, FPR</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-recall-e-f-measure">Precision, Recall e F-Measure</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy">Accuracy</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uso-delle-diverse-misure-di-performance">1.3 Uso delle diverse misure di performance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classificatore-sempre-positivo">Classificatore sempre positivo</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classificatore-sempre-negativo">Classificatore sempre negativo</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-e-sbilanciamento-delle-classi">Accuracy e sbilanciamento delle classi</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nota-sulle-diverse-misure-di-valutazione-di-performance">Nota sulle diverse misure di valutazione di performance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#errore-di-training-e-errore-di-generalizzazione">Errore di training e errore di generalizzazione</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classificazione-basata-su-soglia-e-curve-roc">Classificazione Basata su Soglia e Curve ROC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classificazione-nearest-neighbor-k-nn">Classificazione Nearest Neighbor (K-NN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trovare-il-valore-ottimale-di-k">Trovare il valore ottimale di K</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminante-lineare-di-fisher">Discriminante Lineare di Fisher</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#esempio-breast-cancer-dataset">Esempio: Breast Cancer Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretazione-geometrica-dei-coefficienti">Interpretazione geometrica dei coefficienti</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-discriminant-analysis">Linear Discriminant Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#riduzione-della-dimensionalita-con-lda">Riduzione della Dimensionalit√† con LDA</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classificazione-maximum-a-posteriori-map">Classificazione Maximum a Posteriori (MAP)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classificatore-basato-su-distanza-di-mahalanobis">Classificatore Basato su Distanza di Mahalanobis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementazione-map-di-scikit-learn">Implementazione MAP di Scikit-Learn</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#esercizi">Esercizi</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>