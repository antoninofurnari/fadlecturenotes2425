

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>5. Probability Distributions &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/08_common_distributions';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. Basic Elements of Information Theory" href="09_information_theory.html" />
    <link rel="prev" title="4. Probability for Data Manipulation" href="07_probability.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">1. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">2. Introduzione a Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_main_data_analysis_concepts.html">3. Introduction to Data Analysis and Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_probability.html">4. Probability for Data Manipulation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5. Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_information_theory.html">6. Basic Elements of Information Theory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">7. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">8. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">9. Introduzione a Pandas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">10. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_misure_di_tendenza_centrale_dispersione_e_forma.html">11. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_associazione_variabili.html">12. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">13. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">14. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/08_associazione_variabili.html">15. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_statistical_inference.html">16. Statistical Inference</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7 &amp; 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_linear_regression.html">18. Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">19. Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_causal_analysis.html">20. Causal Data Analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/08_common_distributions.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/08_common_distributions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/08_common_distributions.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probability Distributions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmf-discrete-variables">5.1. Probability Mass Functions (PMF) - Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-fair-coin">5.1.1. Example: Probability Mass Function for a Fair Coin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-biased-coin">5.1.2. Example: Probability Mass Function for a Biased Coin</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-probability-mass-function">5.1.2.1. Exercise: Probability Mass Function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdf-continuous-variables">5.2. Probability Density Functions (PDF) - Continuous Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-uniform-pdf">5.2.1. Example: Uniform PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-functions-cdf">5.2.2. Cumulative Distribution Functions (CDF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cdf-of-continuous-random-variables">5.2.3. CDF of Continuous Random Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">5.2.4. Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cdf-of-discrete-random-variables">5.2.5. CDF of Discrete Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pdf-and-cdf-of-a-fair-die">5.2.5.1. Example - PDF and CDF of a fair die</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">5.3. Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">5.4. Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">5.5. Covariance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization">5.6. Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-distributions">5.7. Common Probability Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-uniform-distribution">5.7.1. Discrete Uniform Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">5.7.1.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">5.7.2. Bernoulli Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5.7.2.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">5.7.3. Binomial Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">5.7.3.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">5.7.4. Categorical Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-distribution">5.7.5. Multinomial Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution">5.7.6. Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">5.7.6.1. Interpretation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">5.7.6.2. Multivariate Gaussian</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-sigma">5.7.6.3. Effect of <span class="math notranslate nohighlight">\(\Sigma\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-gaussian-distribution">5.7.6.4. Estimation of the Parameters of a Gaussian Distribution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization-and-normal-distribution">5.7.7. Standardization and Normal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#central-limit-theorem">5.7.8. Central Limit Theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-distributions-optional">5.8. Other Distributions (Optional)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-distribution">5.8.1. Poisson Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">5.8.1.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#laplacian-distribution">5.8.2. Laplacian Distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">5.9. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probability-distributions">
<h1><span class="section-number">5. </span>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Permalink to this heading">#</a></h1>
<p>We have seen how it is possible to assign a probability value to a given
outcome of a random variable.</p>
<p>In practice, it is often useful to assign probability values to <strong>all
the values</strong> that the random variable can assume.</p>
<p>To do so, we can define a <strong>function</strong>, which we will call <strong>probability
distribution</strong> which assigns a probability value to each of the possible
values of a random variable.</p>
<p>In the case of discrete variables, we will talk about “<strong>probability
mass functions</strong>”, whereas in the case of continuous variable, we will
refer to “<strong>probability density functions</strong>”.</p>
<p>A probability distribution characterizes the random variable and defines
which outcomes it is more likely to observe.</p>
<p>Once we find that a given random variable <span class="math notranslate nohighlight">\(X\)</span> is characterized by a
probability distirbution <span class="math notranslate nohighlight">\(P(X)\)</span>, we can say that <strong>“X follows P”</strong> and
write:</p>
<div class="math notranslate nohighlight">
\[X \sim P\]</div>
<section id="probability-mass-functions-pmf-discrete-variables">
<h2><span class="section-number">5.1. </span>Probability Mass Functions (PMF) - Discrete Variables<a class="headerlink" href="#probability-mass-functions-pmf-discrete-variables" title="Permalink to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is discrete, <span class="math notranslate nohighlight">\(P(X)\)</span> is called a “probability mass function”
(PMF). <span class="math notranslate nohighlight">\(P\)</span> maps the values of <span class="math notranslate nohighlight">\(X\)</span> to real numbers indicating whether a
given value is more or less likely.</p>
<p>A PMF on a random variable <span class="math notranslate nohighlight">\(X\)</span> is a function</p>
<div class="math notranslate nohighlight">
\[P:\Omega \rightarrow \lbrack 0,1\rbrack\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Omega\)</span> is the sample space <span class="math notranslate nohighlight">\(X\)</span>, which satisfies the following property:</p>
<div class="math notranslate nohighlight">
\[\sum_{\mathbf{x}\mathbf{\in}\mathbf{\Omega}}^{}{\mathbf{P}\mathbf{(}\mathbf{x}\mathbf{)}}\mathbf{=}\mathbf{1}\]</div>
<p>This condition implies that the probability distribution is
normalized. Also, this means that at least one of the events should
happen.</p>
<p><strong>Example:</strong> Let <span class="math notranslate nohighlight">\(X\)</span> be the random variable indicating the outcome of a
coin toss.</p>
<ul class="simple">
<li><p>The space of all possible functions (the domain of <span class="math notranslate nohighlight">\(P(X)\)</span>) is
<span class="math notranslate nohighlight">\(\{ head,\ tail\}\)</span>.</p></li>
<li><p>The probabilities <span class="math notranslate nohighlight">\(P(head)\)</span> and <span class="math notranslate nohighlight">\(P(tail)\)</span> must be larger than or
equal to zero and smaller than or equal to 1.</p></li>
<li><p>Also, <span class="math notranslate nohighlight">\(P(head) + P(tail) = 1\ \)</span>. This is obvious, as one of the two
outcomes will always happen. Indeed, if we had <span class="math notranslate nohighlight">\(P(tail) = 0.3\)</span>, this
would mean that, 30 times out of 100 times we toss a coin, the
outcome will be tail. What will happen in all other cases? The
outcome will be head, hence, <span class="math notranslate nohighlight">\(P(head)\)</span>, so <span class="math notranslate nohighlight">\(P(head) + P(tail) = 1\)</span>.</p></li>
<li><p>In the case of a fair coin, we can characterize <span class="math notranslate nohighlight">\(P(X)\)</span> as a
“discrete uniform distribution”, i.e., a distribution which maps any
value <span class="math notranslate nohighlight">\(x \in X\)</span> to a constant, such that the properties of the
probability mass functions are satisfied.</p></li>
<li><p>If we have <span class="math notranslate nohighlight">\(N\)</span> possible outcomes, the discrete uniform probability
will be <span class="math notranslate nohighlight">\(P(X = x) = \frac{1}{N}\)</span> , which means that all outcomes
have the same probability.</p></li>
<li><p>This definition satisfies the constraints. Indeed,
<span class="math notranslate nohighlight">\(\frac{1}{N} \geq 0,\ \forall N\)</span> and
<span class="math notranslate nohighlight">\(\sum_{i}^{}{P\left( X = x_{i} \right)} = 1\)</span>.</p></li>
</ul>
<section id="example-probability-mass-function-for-a-fair-coin">
<h3><span class="section-number">5.1.1. </span>Example: Probability Mass Function for a Fair Coin<a class="headerlink" href="#example-probability-mass-function-for-a-fair-coin" title="Permalink to this heading">#</a></h3>
<p>A probability mass function can be plotted as a 2D diagram where the
values of the function (<span class="math notranslate nohighlight">\(P(x)\)</span>) is plotted against the values of the
independent variable <span class="math notranslate nohighlight">\(x\)</span>. This is the diagram associated to the PMF of
the previous example, where <span class="math notranslate nohighlight">\(P(head) = P(tail) = 0.5\)</span>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/888a70ac931c8fe0436e04741501456ef024eb997604c71fdf4d99dbff9985b2.png" src="../_images/888a70ac931c8fe0436e04741501456ef024eb997604c71fdf4d99dbff9985b2.png" />
</div>
</div>
</section>
<section id="example-probability-mass-function-for-a-biased-coin">
<h3><span class="section-number">5.1.2. </span>Example: Probability Mass Function for a Biased Coin<a class="headerlink" href="#example-probability-mass-function-for-a-biased-coin" title="Permalink to this heading">#</a></h3>
<p>Now suppose we tossed our coin for 10000 times and discovered that 6000
times the outcome was “head”, whereas 4000 times it was “tail”. We
deduce the coin is not fair.</p>
<p>Using a <strong>frequentist</strong> approach, we can manually assign values to our
PMF using the general formula:</p>
<div class="math notranslate nohighlight">
\[P(x) = \frac{\# trials\ in\ which\ X = x}{\#\ trials}\]</div>
<p>That is, in our case:</p>
<div class="math notranslate nohighlight">
\[P(head) = \frac{6000}{10000} = 0.6;P(tail) = \frac{4000}{10000} = 0.4\]</div>
<p>We shall note that the probability we just defined satisfies all
properties of probabilities, i.e.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \leq P(x) \leq 1\ \forall x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{x}^{}{P(x) = 1}.\)</span></p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c738a5bad6a12903b8ef1cac7fbee67d920bab1e09697f9d72cb615a10fe4226.png" src="../_images/c738a5bad6a12903b8ef1cac7fbee67d920bab1e09697f9d72cb615a10fe4226.png" />
</div>
</div>
<section id="exercise-probability-mass-function">
<h4><span class="section-number">5.1.2.1. </span>Exercise: Probability Mass Function<a class="headerlink" href="#exercise-probability-mass-function" title="Permalink to this heading">#</a></h4>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable representing the outcome of rolling a fair
dice with <span class="math notranslate nohighlight">\(6\)</span> faces:</p>
<ul class="simple">
<li><p>What is the space of possible values of <span class="math notranslate nohighlight">\(X\)</span>?</p></li>
<li><p>What is its cardinality?</p></li>
<li><p>What is the associated probability mass function <span class="math notranslate nohighlight">\(P(X)\)</span>?</p></li>
<li><p>Suppose the dice is not fair and <span class="math notranslate nohighlight">\(P(X = 1) = 0.2\)</span>, whereas all other
outcomes are equally probable. What is the probability mass function
of <span class="math notranslate nohighlight">\(P(X)\)</span>?</p></li>
<li><p>Draw the PMF obtained for the dice.</p></li>
</ul>
</section>
</section>
</section>
<section id="probability-density-functions-pdf-continuous-variables">
<h2><span class="section-number">5.2. </span>Probability Density Functions (PDF) - Continuous Variables<a class="headerlink" href="#probability-density-functions-pdf-continuous-variables" title="Permalink to this heading">#</a></h2>
<p>Probability distributions are called “probability density functions”
when the random variable is continuous.</p>
<p>A probability density function over a variable <span class="math notranslate nohighlight">\(X\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight">
\[f:\Omega \rightarrow \lbrack 0,1\rbrack\]</div>
<p>and must satisfy the following property:</p>
<div class="math notranslate nohighlight">
\[\int f(x)dx = 1\]</div>
<p>This condition is equivalent to <span class="math notranslate nohighlight">\(\sum P(x) = 1\)</span> in the
case of a discrete variable. The sum turns into an integral in the
case of continuous variables.</p>
<p>Note that, in the case of continuous variables, we have:</p>
<div class="math notranslate nohighlight">
\[ P(a \leq x \leq b) = \int_{a}^b f(x) dx\]</div>
<p><strong>NOTE</strong>: In general, we say that the density function at a given value <span class="math notranslate nohighlight">\(x\)</span> is zero: <span class="math notranslate nohighlight">\(f(x)=0\)</span>. While this may seem counter-intuitive, we should consider the density function as the limit fo the probability as we narrow a neighborhood around <span class="math notranslate nohighlight">\(x\)</span>. If the neighborhood has size <span class="math notranslate nohighlight">\(0\)</span>, then the density will be zero. In practice, if we take a neighborhood which is non-zero, then we get an integral between two values and a final probability not equal to zero.</p>
<p>After all, from an intuitive point of view, the probability of having a value exactly equal to <span class="math notranslate nohighlight">\(x\)</span> is indeed zero, in the case of a continuous variable! So we should be more interested in the probability in a given range of values.</p>
<section id="example-uniform-pdf">
<h3><span class="section-number">5.2.1. </span>Example: Uniform PDF<a class="headerlink" href="#example-uniform-pdf" title="Permalink to this heading">#</a></h3>
<p>Let us consider a random number generator which outputs numbers comprised between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable assuming the values generated by the
random number generator.</p>
<p>The PDF of <span class="math notranslate nohighlight">\(X\)</span> will be a uniform distribution such that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x) = 0\ \forall x &lt; a\ \ or\ x &gt; b\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(x) = \frac{1}{b - a}\ \forall a \leq x \leq b\)</span>;</p></li>
</ul>
<p>We can see that this PDF satisfies all constraints:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x) \geq 0\ \forall x.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\int P(x)dx = 1\)</span> (prove that this is true as an exercise).</p></li>
</ul>
<p>The diagram below shows an illustration of a uniform PDF with bounds a and b,
i.e., <span class="math notranslate nohighlight">\(U(a,b)\)</span>. Of course, continuous distributions can be (and
generally are) much more complicated than that.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/f511b722e64c3d354c9622cf02e03ef28ca948bdaf4f84407b5ce27775a9f8ea.png" src="../_images/f511b722e64c3d354c9622cf02e03ef28ca948bdaf4f84407b5ce27775a9f8ea.png" />
</div>
</div>
</section>
<section id="cumulative-distribution-functions-cdf">
<h3><span class="section-number">5.2.2. </span>Cumulative Distribution Functions (CDF)<a class="headerlink" href="#cumulative-distribution-functions-cdf" title="Permalink to this heading">#</a></h3>
<p>Similar to the Empirical Cumulative Distribution Functions, we can define Cumulative Distribution Functions for random variables, starting from the density or mass functions. A cumulative distribution function is generally defined as:</p>
<div class="math notranslate nohighlight">
\[ F(x) = f(X \leq x) \]</div>
</section>
<section id="cdf-of-continuous-random-variables">
<h3><span class="section-number">5.2.3. </span>CDF of Continuous Random Variables<a class="headerlink" href="#cdf-of-continuous-random-variables" title="Permalink to this heading">#</a></h3>
<p>In the case of continuous random variables, the definition leads to:</p>
<div class="math notranslate nohighlight">
\[F(x) = \int_{-\infty}^x P(x)dx\]</div>
<p>The CDF is useful in different ways. For instance, it’s easy to see that:</p>
<div class="math notranslate nohighlight">
\[P(a \leq X \leq b) = \int_a^b f(x)dx = \int_{-\infty}^b f(x)dx - \int_{-\infty}^a f(x)dx = F(b) - F(a)\]</div>
</section>
<section id="example">
<h3><span class="section-number">5.2.4. </span>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p>The CDF of the uniform distribution will be given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F(x) = 0\)</span> for <span class="math notranslate nohighlight">\(x &lt; a\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F(x) = \frac{x - a}{b - a}\)</span> for <span class="math notranslate nohighlight">\(a \leq x \leq b\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F(x) = 1\)</span> for <span class="math notranslate nohighlight">\(x &gt; b\)</span></p></li>
</ul>
<p>The plot below shows a diagram:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/00a4f47c84852bb09012db4118569e52f61380509db4e20454d4686e00d27f1e.png" src="../_images/00a4f47c84852bb09012db4118569e52f61380509db4e20454d4686e00d27f1e.png" />
</div>
</div>
</section>
<section id="cdf-of-discrete-random-variables">
<h3><span class="section-number">5.2.5. </span>CDF of Discrete Random Variables<a class="headerlink" href="#cdf-of-discrete-random-variables" title="Permalink to this heading">#</a></h3>
<p>In the case of discrete random variables, the definition of CDF leads to:</p>
<div class="math notranslate nohighlight">
\[F(x) = \sum_{y\leq x} P(y)\]</div>
<section id="example-pdf-and-cdf-of-a-fair-die">
<h4><span class="section-number">5.2.5.1. </span>Example - PDF and CDF of a fair die<a class="headerlink" href="#example-pdf-and-cdf-of-a-fair-die" title="Permalink to this heading">#</a></h4>
<p>In the case of a fair die, the PMF will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x) = \begin{cases} 
\frac{1}{6} &amp; \text{if } x \in \{1, 2, 3, 4, 5, 6\} \\
0 &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>The CDF will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
F(x) = \begin{cases} 
0 &amp; \text{if } x &lt; 1 \\
\frac{1}{6} &amp; \text{if } 1 \leq x &lt; 2 \\
\frac{1}{3} &amp; \text{if } 2 \leq x &lt; 3 \\
\frac{1}{2} &amp; \text{if } 3 \leq x &lt; 4 \\
\frac{2}{3} &amp; \text{if } 4 \leq x &lt; 5 \\
1 &amp; \text{if } x \geq 5
\end{cases}
\end{split}\]</div>
<p>The diagram below shows an example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/b96fc33c7fc05bf59bd380dab928b8f83fdd5ee97fe60e06e6065fa77f644e77.png" src="../_images/b96fc33c7fc05bf59bd380dab928b8f83fdd5ee97fe60e06e6065fa77f644e77.png" />
</div>
</div>
</section>
</section>
</section>
<section id="expectation">
<h2><span class="section-number">5.3. </span>Expectation<a class="headerlink" href="#expectation" title="Permalink to this heading">#</a></h2>
<p>When it is known that a random variable follows a probability
distribution, <strong>it is possible to characterize that variable</strong> (and
hence the related probability distribution) <strong>with some statistics</strong>.</p>
<p>The most straightforward of them is the expectation. <strong>The concept of
expectation is very related to the concept of mean.</strong> When we compute
the mean of a given set of numbers, we usually sum all the numbers
together and then divide by the total.</p>
<p>Since a probability distribution will tell us which values will be more
frequent than others, we can compute this mean with a weighted average,
where the weights are given by the probability distribution.</p>
<p>Specifically, we can define the expectation of a random variable X as
follows:</p>
<div class="math notranslate nohighlight">
\[E_{X\sim P}\lbrack X\rbrack = \sum_{x \in \chi}^{}{xP(x)}\]</div>
<p>In the case of continuous variables, the expectation takes the form of
an integral:</p>
<div class="math notranslate nohighlight">
\[E_{X \sim P}\lbrack X\rbrack = \int xf(x)dx\]</div>
<p>This is very related to the concept of mean value (or expected value) of
a random variable.</p>
</section>
<section id="variance">
<h2><span class="section-number">5.4. </span>Variance<a class="headerlink" href="#variance" title="Permalink to this heading">#</a></h2>
<p>The variance gives a measure of how much variability there is in a
variable <span class="math notranslate nohighlight">\(X\)</span> around its mean <span class="math notranslate nohighlight">\(E\lbrack X\rbrack\)</span>.</p>
<p>The variance is defined as follows:</p>
<div class="math notranslate nohighlight">
\[var\lbrack X\rbrack = E\lbrack\left( X - E\lbrack X\rbrack \right)^{2}\rbrack\]</div>
</section>
<section id="covariance">
<h2><span class="section-number">5.5. </span>Covariance<a class="headerlink" href="#covariance" title="Permalink to this heading">#</a></h2>
<p>The covariance gives a measure of how two variables are linearly related
to each other. It allows to <strong>measure to what extent the increase of one
of the variables corresponds to an increase of the value of the other
one</strong>.</p>
<p>Given two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, the covariance is defined as
follows:</p>
<div class="math notranslate nohighlight">
\[Cov(X,Y) = E\lbrack\left( X - E\lbrack X\rbrack \right)\left( Y - E\lbrack Y\rbrack \right)\rbrack\]</div>
<p>We can distinguish the following terms:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack X\rbrack\)</span> and <span class="math notranslate nohighlight">\(E\lbrack Y\rbrack\)</span> are the expectations of
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((X - E\lbrack X\rbrack)\)</span> and <span class="math notranslate nohighlight">\((Y - E\lbrack Y\rbrack)\)</span> are the
differences between the samples and the expected values.</p></li>
<li><p><span class="math notranslate nohighlight">\(\left( X - E\lbrack X\rbrack \right)\left( Y - E\lbrack Y\rbrack \right)\)</span>
computes the product between the differences.</p></li>
</ul>
<p>We have:</p>
<ul class="simple">
<li><p>If the signs of the terms agree, the product is positive.</p></li>
<li><p>If the signs of the terms disagree, the product is negative.</p></li>
</ul>
<p>In practice, if when <span class="math notranslate nohighlight">\(X\)</span> is larger than the mean, then <span class="math notranslate nohighlight">\(Y\)</span>
is larger than the mean and vice versa, when <span class="math notranslate nohighlight">\(X\)</span> is lower
than the mean then <span class="math notranslate nohighlight">\(Y\)</span> is lower than the mean, then the two
variables are <em>correlated,</em> and the covariance is high.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a multi-dimensional variable
<span class="math notranslate nohighlight">\(X = \lbrack X_{1},X_{2},\ldots,X_{n}\rbrack\)</span>, we can compute all the
possible covariances between variable pairs:
<span class="math notranslate nohighlight">\(Cov\lbrack X_{i},X_{j}\rbrack\)</span>. This allows to create a matrix, which
is generally referred to as <strong>the covariance matrix</strong>. The general term
of the covariance matrix <span class="math notranslate nohighlight">\(Cov(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[Cov(X)_{i,j} = \Sigma_{ij} = Cov(X_{i},X_{j})\]</div>
</section>
<section id="standardization">
<h2><span class="section-number">5.6. </span>Standardization<a class="headerlink" href="#standardization" title="Permalink to this heading">#</a></h2>
<p>Standardization transforms a random variable <span class="math notranslate nohighlight">\(X\)</span> into a variable <span class="math notranslate nohighlight">\(Z\)</span> so that it has:</p>
<ul class="simple">
<li><p>Expectation equal to zero: <span class="math notranslate nohighlight">\(E(Z) = 0\)</span>.</p></li>
<li><p>Variance equal to one: <span class="math notranslate nohighlight">\(Var(Z) = 1\)</span>.</p></li>
</ul>
<p>The standardized variable will be:</p>
<div class="math notranslate nohighlight">
\[Z = \frac{X - \mu_X}{\sigma_X} = \frac{X-E[X]}{\sqrt{Var[X]}}\]</div>
</section>
<section id="common-probability-distributions">
<h2><span class="section-number">5.7. </span>Common Probability Distributions<a class="headerlink" href="#common-probability-distributions" title="Permalink to this heading">#</a></h2>
<p>There are several common probability distributions which can be used to
describe random events. <strong>These distributions have an analytical
formulation which depends generally on one or more parameters.</strong></p>
<p>When we have <strong>enough evidence that a given random variable is well
described by one of these distributions</strong>, we can simply “fit” the
distribution to the data (i.e., choose the correct parameters for the
distribution) and use the analytical formulation to deal with the random
variable.</p>
<p>It is hence useful to know the <strong>most common probability distributions</strong>
so that we can recognize the cases in which they can be used.</p>
<section id="discrete-uniform-distribution">
<h3><span class="section-number">5.7.1. </span>Discrete Uniform Distribution<a class="headerlink" href="#discrete-uniform-distribution" title="Permalink to this heading">#</a></h3>
<p>The discrete uniform distribution is controlled by a parameter <span class="math notranslate nohighlight">\(k \in \mathbb{N}\)</span> and assumes that all outcomes have the same probability of occurring:</p>
<div class="math notranslate nohighlight">
\[P(X=a_i) = \frac{1}{k}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Omega = \{a_1,\ldots,a_k\}\)</span>.</p>
<p>It can be shown that:</p>
<div class="math notranslate nohighlight">
\[E[X] = \frac{k+1}{2}\]</div>
<div class="math notranslate nohighlight">
\[Var[X] = \frac{1}{12}(k^2-1)\]</div>
<section id="id1">
<h4><span class="section-number">5.7.1.1. </span>Example<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<p>The outcomes of rolling a fair die follow a uniform distribution with <span class="math notranslate nohighlight">\(k=6\)</span>, as shown in the diagram below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/689d5dc078fb3b115fe23860123251ca54e9328aa880529a13032e78754d263a.png" src="../_images/689d5dc078fb3b115fe23860123251ca54e9328aa880529a13032e78754d263a.png" />
</div>
</div>
</section>
</section>
<section id="bernoulli-distribution">
<h3><span class="section-number">5.7.2. </span>Bernoulli Distribution<a class="headerlink" href="#bernoulli-distribution" title="Permalink to this heading">#</a></h3>
<p>The Bernoulli distribution is a distribution over a single binary random
variable, i.e., the variable <span class="math notranslate nohighlight">\(X\)</span> can take only two values:
<span class="math notranslate nohighlight">\(\left\{ 0,1 \right\}\)</span>.</p>
<p>The distribution is controlled by a single parameter
<span class="math notranslate nohighlight">\(\phi \in \lbrack 0,1\rbrack\)</span>, which gives the probability of the
variable to be equal to 1.</p>
<p>The analytical formulation of the Bernoulli distribution is very simple:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X = 1) = \phi\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P(X = 0) = 1 - \phi\)</span></p></li>
</ul>
<p>The expected value and variance of the associated random variable are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack x\rbrack = \phi\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(Var(x) = \phi(1 - \phi)\)</span>.</p></li>
</ul>
<section id="id2">
<h4><span class="section-number">5.7.2.1. </span>Example<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h4>
<p>A skewed coin lands on “head” <span class="math notranslate nohighlight">\(60\%\)</span> of the
times. If we define <span class="math notranslate nohighlight">\(X = 1\)</span> when the outcome is head and <span class="math notranslate nohighlight">\(X = 0\)</span> when
the outcome is tail, then the variable follows a Bernoulli distribution
with <span class="math notranslate nohighlight">\(\phi = 0.6\)</span>.</p>
</section>
</section>
<section id="binomial-distribution">
<h3><span class="section-number">5.7.3. </span>Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this heading">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/b4ba486ffca81fed577fcc5d0d825a6576c5dcd4433a1eed434d02c99df6b8aa.png" src="../_images/b4ba486ffca81fed577fcc5d0d825a6576c5dcd4433a1eed434d02c99df6b8aa.png" />
</div>
</div>
<p>The binomial distribution is a discrete probability distribution (PMF)
over natural numbers with parameters <span class="math notranslate nohighlight">\(\mathbf{n}\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(\mathbf{p}\)</span></p>
<p>It models <strong>the probability of obtaining</strong> <span class="math notranslate nohighlight">\(\mathbf{k}\)</span> <strong>successes in a
sequence of</strong> <span class="math notranslate nohighlight">\(\mathbf{n}\)</span> <strong>independent experiments which follow a
Bernoulli distribution with parameter</strong>
<span class="math notranslate nohighlight">\(\mathbf{p}\mathbf{\ (}\mathbf{\phi}\mathbf{=}\mathbf{p}\mathbf{)}\)</span><strong>;</strong></p>
<p>The probability mass function of the distribution is given by:</p>
<div class="math notranslate nohighlight">
\[P(k) = \binom{n}{k}p^{k}(1 - p)^{n - k}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the number of successes</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the number of independent trials</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the probability of a success in a single trial</p></li>
</ul>
<p>The expected value is <span class="math notranslate nohighlight">\(E\lbrack k\rbrack = np\)</span>;</p>
<p>The variance is <span class="math notranslate nohighlight">\(Var\lbrack k\rbrack = np(1 - p)\)</span>;</p>
<section id="id3">
<h4><span class="section-number">5.7.3.1. </span>Example<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<p>What is the probability of tossing a coin three times and obtaining
three heads? We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k = 3\)</span>: number of successes (three times head)</p></li>
<li><p><span class="math notranslate nohighlight">\(n = 3\)</span>: number of trials</p></li>
<li><p><span class="math notranslate nohighlight">\(p = 0.5\)</span>: the probability of getting a head when tossing a coin</p></li>
</ul>
<p>The required probability will be given by:</p>
<div class="math notranslate nohighlight">
\[P(3) = \binom{3}{3}{0.5}^{3}(1 - 0.5)^{3 - 3} = {0.5}^{3} = 0.125\]</div>
<p><strong>Exercise</strong></p>
<p>What is the probability of tossing an unfair coin
(<span class="math notranslate nohighlight">\(P\left( 'head^{'} \right) = 0.6\)</span>) 7 times and obtaining <span class="math notranslate nohighlight">\(2\)</span> tails?</p>
</section>
</section>
<section id="categorical-distribution">
<h3><span class="section-number">5.7.4. </span>Categorical Distribution<a class="headerlink" href="#categorical-distribution" title="Permalink to this heading">#</a></h3>
<p>The <strong>multinoulli</strong> or <strong>categorical</strong> distribution is a distribution of
a <em>single discrete variable with</em> <span class="math notranslate nohighlight">\(k\)</span> <em>different states</em>, where <span class="math notranslate nohighlight">\(k\)</span> is
finite.</p>
<ul class="simple">
<li><p>The distribution is parametrized by a vector
<span class="math notranslate nohighlight">\(\mathbf{p} \in \lbrack 0,1\rbrack^k\)</span>, where <span class="math notranslate nohighlight">\(p_{i}\)</span> gives the
probability of the <span class="math notranslate nohighlight">\(i^{th}\)</span> state.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{p}\)</span> must be such that <span class="math notranslate nohighlight">\(\sum_{i = 1}^kp_{i} = 1\)</span> to
obtain a valid probability distribution.</p></li>
<li><p>The analytical form of the distribution is given by:
<span class="math notranslate nohighlight">\(p(x = i) = p_{i}\)</span>;</p></li>
</ul>
<p>This distribution is the <strong>generalization of the Bernoulli distribution
to the case of multiple states</strong>.</p>
<p><strong>Example:</strong></p>
<p>Rolling a fair die. In this case, <span class="math notranslate nohighlight">\(k = 6\)</span> and
<span class="math notranslate nohighlight">\(p_{i} = \frac{1}{k}\ ,\ i = 0,\ldots,k \ \)</span>.</p>
</section>
<section id="multinomial-distribution">
<h3><span class="section-number">5.7.5. </span>Multinomial Distribution<a class="headerlink" href="#multinomial-distribution" title="Permalink to this heading">#</a></h3>
<p>The multinomial distribution <strong>generalizes the binomial distribution to
the case in which the experiments are not binary</strong>, but they can have
multiple outcomes (e.g., <em>a dice vs a coin</em>).</p>
<p>In particular, the multinomial distribution models the probability of
obtaining exactly <span class="math notranslate nohighlight">\((n_{1},\ldots,n_{k})\)</span> occurrences (with
<span class="math notranslate nohighlight">\(n = \sum_{i}^{}n_{i}\)</span>) for each of the <span class="math notranslate nohighlight">\(k\)</span> possible outcomes in a
sequence of <span class="math notranslate nohighlight">\(n\)</span> independent experiments which follow a Categorial
distribution with probabilities <span class="math notranslate nohighlight">\(p_{1},\ldots,p_{k}\)</span>.</p>
<p>The parameters of the distribution are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span>: the number of trials</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span>: the number of possible outcomes</p></li>
<li><p><span class="math notranslate nohighlight">\(p_{1},\ldots,p_{k}\)</span> the probabilities of obtaining a given class in
each trial (with <span class="math notranslate nohighlight">\(\sum_{i = 1}^{k}p_{i} = 1\)</span>)</p></li>
</ul>
<p>The PMF of the distribution is:</p>
<div class="math notranslate nohighlight">
\[P\left( n_{1},\ldots,n_{k} \right) = \frac{n!}{n_{1}!\ldots n_{k}!}p_{1}^{n_{1}} \cdot \ldots \cdot p_{k}^{n_{k}}\]</div>
<p>The mean is: <span class="math notranslate nohighlight">\(E\left\lbrack n_{i} \right\rbrack = np_{i}\)</span>.</p>
<p>The variance is:
<span class="math notranslate nohighlight">\(Var\left\lbrack n_{i} \right\rbrack = np_{i}(1 - p_{i})\)</span>.</p>
<p>The covariance between two of the input variables is:
<span class="math notranslate nohighlight">\(Cov\left( n_{i},n_{j} \right) = - np_{i}p_{j}\ (i \neq j)\)</span>.</p>
<p><strong>Example</strong></p>
<p>Given a fair die with 6 possible outcomes, what is the probability of
getting 3 times 1, 2 times 2, 4 time 3, 5 times 4, 0 times 5, and 1 time
6, rolling the dice for 15 times?</p>
<p>We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n = 15\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k = 6\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p_{1} = p_{2} = \ldots p_{6} = \frac{1}{6}\)</span></p></li>
</ul>
<p>The required probability is given by:</p>
<div class="math notranslate nohighlight">
\[P(3,2,4,5,0,1) = \frac{15!}{3!2!4!5!0!1!} \cdot \frac{1}{6^{3}} \cdot \frac{1}{6^{2}} \cdot \frac{1}{6^{4}} \cdot \frac{1}{6^{5}} \cdot \frac{1}{6^{0}} \cdot \frac{1}{6^{1}} = 8.04 \cdot 10^{- 5}\]</div>
</section>
<section id="gaussian-distribution">
<h3><span class="section-number">5.7.6. </span>Gaussian Distribution<a class="headerlink" href="#gaussian-distribution" title="Permalink to this heading">#</a></h3>
<p>The Bernoulli and Categorical distributions are PMF, i.e., distributions
over discrete random variables.</p>
<p>A common PDF when dealing with real values is the <strong>Gaussian
distribution,</strong> also known as <strong>Normal Distribution</strong>.</p>
<p>The distribution is characterized by two parameters:</p>
<ul class="simple">
<li><p>The mean <span class="math notranslate nohighlight">\(\mu\mathfrak{\in R}\)</span></p></li>
<li><p>The standard deviation <span class="math notranslate nohighlight">\(\sigma \in (0, + \infty)\)</span></p></li>
</ul>
<p>In practice, the distribution is often <em>seen in terms of</em> <span class="math notranslate nohighlight">\(\mu\)</span> <em>and</em>
<span class="math notranslate nohighlight">\(\sigma^{2}\)</span> rather than <span class="math notranslate nohighlight">\(\sigma\)</span>, where <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is called <strong>the
variance</strong>.</p>
<p>The analytical formulation of the Normal distribution is as follows:</p>
<div class="math notranslate nohighlight">
\[N\left( x;\mu,\sigma^{2} \right) = \sqrt{\frac{1}{2\pi\sigma^{2}}}e^{- \frac{1}{2\sigma^{2}}(x - \mu)^{2}}\]</div>
<p>The term under the square root is a normalization term which ensures
that the distribution integrates to 1.</p>
<p>The expectation and variance of a variable following the Normal
distribution are as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E\lbrack x\rbrack = \mu\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Var\lbrack x\rbrack = \sigma^{2}\)</span></p></li>
</ul>
<p>The Gaussian distribution is very used when we do not have much prior
knowledge on the real distribution we wish to model. This in mainly due
to the <strong>central limit theorem</strong>, which states that the sum of many
independent random variables with the same distribution is approximately
normally distributed.</p>
<section id="interpretation">
<h4><span class="section-number">5.7.6.1. </span>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this heading">#</a></h4>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/ff3fc200b1dac4e599d0357ac8790cdeb0b07329a61bda14eceb69124445e94e.png" src="../_images/ff3fc200b1dac4e599d0357ac8790cdeb0b07329a61bda14eceb69124445e94e.png" />
</div>
</div>
<p>If we plot the PDF of a Normal distribution, we can find that it is easy
to interpret the meaning of its parameters:</p>
<ul class="simple">
<li><p>The resulting curve has a maximum (highest probability) when
<span class="math notranslate nohighlight">\(x = \mu\)</span></p></li>
<li><p>The curve is symmetric, with the inflection points at
<span class="math notranslate nohighlight">\(x = \mu \pm \sigma\)</span></p></li>
<li><p>The example shows a normal distribution for <span class="math notranslate nohighlight">\(\mu = 0\)</span> and
<span class="math notranslate nohighlight">\(\sigma = 1\)</span></p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/f881bd32ce8ea398c660d4f1b689083eceb9caa2865df155d8674291b720b160.png" src="../_images/f881bd32ce8ea398c660d4f1b689083eceb9caa2865df155d8674291b720b160.png" />
</div>
</div>
<p>Another notable property of the Normal distribution is that:</p>
<ul class="simple">
<li><p>About <span class="math notranslate nohighlight">\(68\%\)</span> of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - \sigma,\sigma\rbrack\)</span>;</p></li>
<li><p>About <span class="math notranslate nohighlight">\(95\%\)</span> of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - 2\sigma,2\sigma\rbrack\)</span>;</p></li>
<li><p>Almost 100% of the density is comprised in the interval
<span class="math notranslate nohighlight">\(\lbrack - 3\sigma,3\sigma\rbrack\)</span>.</p></li>
</ul>
</section>
<section id="multivariate-gaussian">
<h4><span class="section-number">5.7.6.2. </span>Multivariate Gaussian<a class="headerlink" href="#multivariate-gaussian" title="Permalink to this heading">#</a></h4>
<p>The formulation of the Gaussian distribution generalizes to the
multivariate case, i.e., the case in which <span class="math notranslate nohighlight">\(X\)</span> is n-dimensional.</p>
<p>In that case, the distribution is parametrized by a <strong>n-dimensional
vector</strong> <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> and a
<span class="math notranslate nohighlight">\(\mathbf{n}\mathbf{\times}\mathbf{n}\mathbf{\ }\)</span><strong>positive definite
symmetric matrix</strong> <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>. The formulation of the
multi-variate Gaussian is:</p>
<div class="math notranslate nohighlight">
\[N\left( \mathbf{x;\mu,}\mathbf{\Sigma} \right) = \sqrt{\frac{1}{(2\pi)^{n}\det(\Sigma)}}e^{( - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\Sigma^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right))}\]</div>
<p>In the 2D case, <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> is a 2D
point representing the center of the Gaussian (the position of the
mode), whereas the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> influences the “shape” of the
Gaussian.</p>
<p>Examples of bivariate Gaussian distributions are shown below.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1f251b1c4d04abd9ad75d11b2b34c3263168f41d5ce8fa485598f7bf30f7bc9e.png" src="../_images/1f251b1c4d04abd9ad75d11b2b34c3263168f41d5ce8fa485598f7bf30f7bc9e.png" />
<img alt="../_images/3616cdd0ac2346f211bdc4356b38ff015a138159409832c1a5774354c77fa7a1.png" src="../_images/3616cdd0ac2346f211bdc4356b38ff015a138159409832c1a5774354c77fa7a1.png" />
</div>
</div>
<p>The two plots above are common representations for bivariate continuous distributions:</p>
<ul class="simple">
<li><p>The plot on the top shows a 3D representation of the PDF in which the X and Y axes are the values of the variables, while the third axis reports the probability density.</p></li>
<li><p>Since it’s often hard to draw 3D graphs, we often use a contour plot to represent the 3D curves. In the 3D plot, curves of the same color represent points which have the same density in the 3D plot.</p></li>
</ul>
</section>
<section id="effect-of-sigma">
<h4><span class="section-number">5.7.6.3. </span>Effect of <span class="math notranslate nohighlight">\(\Sigma\)</span><a class="headerlink" href="#effect-of-sigma" title="Permalink to this heading">#</a></h4>
<p>Similar to how variance affects the dispersion of a 1D Gaussian, the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> affects the dispersion in both axes. As a result, changing the values of the matrix will affect the shape of the distribution. Let’s consider the general covariance matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = \begin{bmatrix}
  \sigma_x^2 &amp; \sigma_{xy} \\
  \sigma_{yx} &amp; \sigma_y^2
\end{bmatrix}
\end{split}\]</div>
<p>If the matrix is diagonal (<span class="math notranslate nohighlight">\(\sigma_{xy}=\sigma_{yx}=0\)</span>), then we have an isotropic Gaussian, meaning that it is symmetric along the two axes. Adding values different from zeros in the secondary diagonal will change the shape. Some examples are shown below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c405ad8432a9efe42acc83cf7d6fbe84f621d3ae27880a0196c8cbe0320eea8b.png" src="../_images/c405ad8432a9efe42acc83cf7d6fbe84f621d3ae27880a0196c8cbe0320eea8b.png" />
</div>
</div>
</section>
<section id="estimation-of-the-parameters-of-a-gaussian-distribution">
<h4><span class="section-number">5.7.6.4. </span>Estimation of the Parameters of a Gaussian Distribution<a class="headerlink" href="#estimation-of-the-parameters-of-a-gaussian-distribution" title="Permalink to this heading">#</a></h4>
<p>We have noted that in many cases we can assume a random variable follows
a Gaussian distribution. However, it is not yet clear how to choose the
parameters of the Gaussian distribution.</p>
<p>Given some data (remember, data is values assumed by random variables!),
we can obtain the parameters of the Gaussian distribution related to the
data with a <strong>maximum likelihood</strong> estimation.</p>
<p>This consists in computing the mean and variance parameters using the
following formula (in the univariate case):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu = \frac{1}{n}\sum_{j}^{}x_{j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^{2} = \frac{1}{n}\sum_{j}^{}\left( x_{j} - \mu \right)^{2}\)</span></p></li>
</ul>
<p>Where <span class="math notranslate nohighlight">\(x_{j}\)</span> represent the different data points.</p>
<p>In the multi-variate case, the computation of the multi-dimensional
<span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> vector is similar:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{\mu} = \frac{1}{n}\sum_{j}^{}\mathbf{x}_{j}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is instead computed as the covariance matrix related to
<span class="math notranslate nohighlight">\(X\)</span>: <span class="math notranslate nohighlight">\(\Sigma = Cov(\mathbf{X})\)</span>, i.e.,
<span class="math notranslate nohighlight">\(\Sigma_{ij} = Cov\left( \mathbf{X}_{i},\mathbf{X}_{j} \right)\)</span></p></li>
</ul>
<p>The diagram below shows an example in which we fit a Gaussian to a set of data and compare it with a 2D KDE of the data.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/407ed8c3dbc3b9d75b2781cb4de5256c8657b136c8c7f5196ef720d3bbc78fe1.png" src="../_images/407ed8c3dbc3b9d75b2781cb4de5256c8657b136c8c7f5196ef720d3bbc78fe1.png" />
</div>
</div>
</section>
</section>
<section id="standardization-and-normal-distribution">
<h3><span class="section-number">5.7.7. </span>Standardization and Normal Distribution<a class="headerlink" href="#standardization-and-normal-distribution" title="Permalink to this heading">#</a></h3>
<p>We have seen that standardization gives rise to a new variable:</p>
<div class="math notranslate nohighlight">
\[Z = \frac{X - \mu_X}{\sigma_X} = \frac{X-E[X]}{\sqrt{Var[X]}}\]</div>
<p>The plot below shows a data distribution before and after standardization.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/aba8db2cd10ec3fa2bd151841d5b89c89e4239402a10e698b6839d3c3978c4cd.png" src="../_images/aba8db2cd10ec3fa2bd151841d5b89c89e4239402a10e698b6839d3c3978c4cd.png" />
</div>
</div>
</section>
<section id="central-limit-theorem">
<h3><span class="section-number">5.7.8. </span>Central Limit Theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this heading">#</a></h3>
<p>The Central Limit Theorem is a statistical principle that states that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span> approaches a normal distribution as <span class="math notranslate nohighlight">\(n \to \infty\)</span>, regardless of the shape of the original population’s distribution.</p>
<p>While we will not see this theorem formally, it is a fundamental result which in some sense “justifies” the pervasive use of the Gaussian distribution in data analysis.</p>
<p>In the plot below, we plot the distributions of the average outcome of rolling a given number of dice. We can see each die as described by a different and independent random variable, hence the distribution will get close to Gaussian as we increase the number of dice.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d3b0dcfd55343902762bdbfdf6de40e0d4d72373e1bf9592ba8247dffffb8d78.png" src="../_images/d3b0dcfd55343902762bdbfdf6de40e0d4d72373e1bf9592ba8247dffffb8d78.png" />
</div>
</div>
</section>
</section>
<section id="other-distributions-optional">
<h2><span class="section-number">5.8. </span>Other Distributions (Optional)<a class="headerlink" href="#other-distributions-optional" title="Permalink to this heading">#</a></h2>
<section id="poisson-distribution">
<h3><span class="section-number">5.8.1. </span>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Permalink to this heading">#</a></h3>
<p>The Poisson distribution expresses the <strong>probability of a given number of events occurring in a fixed interval of time or space if they occur independently with a known rate</strong>. It is useful to model situations in which the number of events is very large and the probability of a given event happening is relatively small. The Poisson distribution is controlled by a parameter <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> (the rate at which each event occurs) and defined as:</p>
<div class="math notranslate nohighlight">
\[P(X=x) = \frac{\lambda^x}{x!}\exp(-\lambda)\]</div>
<p>with <span class="math notranslate nohighlight">\(x=0,1,2,3,\ldots\)</span> representing the number of events happening at the same time/space. The plot below shows the Poisson distribution for different choices of the <span class="math notranslate nohighlight">\(\lambda\)</span> parameter:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/26b32f3ff10869b84d807ff3b8402495ab558285601402006112a858df2434d1.png" src="../_images/26b32f3ff10869b84d807ff3b8402495ab558285601402006112a858df2434d1.png" />
</div>
</div>
<section id="id4">
<h4><span class="section-number">5.8.1.1. </span>Example<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<p>During World War II Germans bombed the south of London many times. People started to believe that some areas were more targeted by the bombing and hence started moving from one part of a city to another in an attempt to avoid those areas.</p>
<p>After the end of the war, R. D. Charles showed that the bombings were actually random and independent. To do so, he divided the interested area in <span class="math notranslate nohighlight">\(576\)</span> squares of <span class="math notranslate nohighlight">\(0.24 Km^2\)</span>. The number of total bombs was <span class="math notranslate nohighlight">\(537\)</span>, hence less than one per square (a rare event). Assuming that the bombing was uniform and casual, the probability of a square being bombed would be:</p>
<div class="math notranslate nohighlight">
\[\lambda = \frac{537}{576}\]</div>
<p>We can hence compute the expected probability that <span class="math notranslate nohighlight">\(x\)</span> squares were bombed as <span class="math notranslate nohighlight">\(P(X=x)\)</span>, where <span class="math notranslate nohighlight">\(P\)</span> is a Poisson function with <span class="math notranslate nohighlight">\(\lambda=\frac{537}{576}\)</span>. The expected number of areas being bombed <span class="math notranslate nohighlight">\(x\)</span> times will be: <span class="math notranslate nohighlight">\(P(X=x)\times576\)</span></p>
<p>Comparing the expected numbers with the measured ones, we obtain the following table:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Expected</th>
      <th>Real</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>226.742723</td>
      <td>229</td>
    </tr>
    <tr>
      <th>1</th>
      <td>211.390351</td>
      <td>211</td>
    </tr>
    <tr>
      <th>2</th>
      <td>98.538731</td>
      <td>93</td>
    </tr>
    <tr>
      <th>3</th>
      <td>30.622279</td>
      <td>35</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.137224</td>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Since the numbers are very close, we can imagine that the bombing was really random.</p>
</section>
</section>
<section id="laplacian-distribution">
<h3><span class="section-number">5.8.2. </span>Laplacian Distribution<a class="headerlink" href="#laplacian-distribution" title="Permalink to this heading">#</a></h3>
<p>The Gaussian distribution assumes that the probability of an observation deviating from the mean <strong>decreases exponentially as the square of the deviation</strong>. For some types of data, this assumption is not accurate: in some cases, deviating from the mean is much more likely than prescribed by the Gaussian model. An alternative mathematical model, introduced by Laplace, posits that the probability of an observation deviating from the mean decreases exponentially with the absolute value of the deviation:</p>
<div class="math notranslate nohighlight">
\[L(x, M, b) = \frac{1}{2b} e^{-\frac{|x - M|}{b}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(M\)</span> represents the c<strong>entral/mean value of the distribution</strong>, and b is a scaling parameter known as <strong>diversity</strong>. It should be noted that due to the absolute value involved, this function is not differentiable at the mean value.</p>
<p>The <strong>best fit</strong> of this function to the data occurs when <strong>M is chosen as the median of the data</strong>, and <strong>b is chosen as the mean of the absolute differences between the data points and the median</strong>:</p>
<div class="math notranslate nohighlight">
\[M=median(\{x_i\}_{i=1}^n)\]</div>
<div class="math notranslate nohighlight">
\[b=\frac{\sum_{i=1}^n|x_i-M|}{n}\]</div>
<p>In this model, values far from the central value occur more frequently than they would in a Gaussian model. This phenomenon is referred to as <strong>fat tails</strong> in contrast to the Gaussian model, which is described as having <strong>thin tails</strong>.</p>
<p>Expectation and variance of <span class="math notranslate nohighlight">\(X \sim L\)</span> are:</p>
<div class="math notranslate nohighlight">
\[E[X] = \lambda\]</div>
<div class="math notranslate nohighlight">
\[Var(X) = \lambda\]</div>
<p>The following plot shows some examples of Laplacian distributions:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/364732194241ad5e066822a4c7dc4a56aebae6f81d906fac0afbead5a714d0c2.png" src="../_images/364732194241ad5e066822a4c7dc4a56aebae6f81d906fac0afbead5a714d0c2.png" />
</div>
</div>
</section>
</section>
<section id="references">
<h2><span class="section-number">5.9. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Parts of chapter 1 of [1];</p></li>
<li><p>Most of chapter 3 of [2];</p></li>
<li><p>Parts of chapter 8 of [3].</p></li>
</ul>
<p>[1] Bishop, Christopher M. <em>Pattern recognition and machine learning</em>.
springer, 2006.
<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<p>[2] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep
learning</em>. MIT press, 2016. <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p>[3] Heumann, Christian, and Michael Schomaker Shalabh. Introduction to statistics and data analysis. Springer International Publishing Switzerland, 2016.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="07_probability.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Probability for Data Manipulation</p>
      </div>
    </a>
    <a class="right-next"
       href="09_information_theory.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Basic Elements of Information Theory</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmf-discrete-variables">5.1. Probability Mass Functions (PMF) - Discrete Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-fair-coin">5.1.1. Example: Probability Mass Function for a Fair Coin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-probability-mass-function-for-a-biased-coin">5.1.2. Example: Probability Mass Function for a Biased Coin</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-probability-mass-function">5.1.2.1. Exercise: Probability Mass Function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdf-continuous-variables">5.2. Probability Density Functions (PDF) - Continuous Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-uniform-pdf">5.2.1. Example: Uniform PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumulative-distribution-functions-cdf">5.2.2. Cumulative Distribution Functions (CDF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cdf-of-continuous-random-variables">5.2.3. CDF of Continuous Random Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">5.2.4. Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cdf-of-discrete-random-variables">5.2.5. CDF of Discrete Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pdf-and-cdf-of-a-fair-die">5.2.5.1. Example - PDF and CDF of a fair die</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation">5.3. Expectation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">5.4. Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">5.5. Covariance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization">5.6. Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-distributions">5.7. Common Probability Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-uniform-distribution">5.7.1. Discrete Uniform Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">5.7.1.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bernoulli-distribution">5.7.2. Bernoulli Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5.7.2.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-distribution">5.7.3. Binomial Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">5.7.3.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-distribution">5.7.4. Categorical Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-distribution">5.7.5. Multinomial Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution">5.7.6. Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">5.7.6.1. Interpretation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian">5.7.6.2. Multivariate Gaussian</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-sigma">5.7.6.3. Effect of <span class="math notranslate nohighlight">\(\Sigma\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-gaussian-distribution">5.7.6.4. Estimation of the Parameters of a Gaussian Distribution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization-and-normal-distribution">5.7.7. Standardization and Normal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#central-limit-theorem">5.7.8. Central Limit Theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-distributions-optional">5.8. Other Distributions (Optional)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-distribution">5.8.1. Poisson Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">5.8.1.1. Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#laplacian-distribution">5.8.2. Laplacian Distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">5.9. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>