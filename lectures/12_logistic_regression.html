

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Logistic Regression &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/12_logistic_regression';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">1. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">2. Introduzione a Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">3. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">4. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">5. Introduzione a Pandas</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/12_logistic_regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/12_logistic_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/12_logistic_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Logistic Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-data">Example Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limits-of-linear-regression">Limits of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-binary-values-to-probabilities">From Binary Values to Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-function">The Logistic Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model">The Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation-of-the-coefficients-of-a-linear-regressor">Statistical Interpretation of the Coefficients of a Linear Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-interpretation-of-the-coefficients-of-a-logistic-regressor">Geometrical Interpretation of the Coefficients of a Logistic Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-logistic-regressor">Estimation of the Parameters of a Logistic Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-logistic-regression">Example of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression">Multinomial Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-iris-dataset">The Iris Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-limitations-of-a-linear-regression-in-the-presence-of-more-than-two-classes">Additional Limitations of a Linear Regression in the Presence of More than Two Classes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-multinomial-logistic-regression-model">The Multinomial Logistic Regression Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-parameters-of-the-multinomial-logistic-regressor-model">Estimating the Parameters of the Multinomial Logistic Regressor Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-parameters-of-a-multinomial-logistic-regressor">Interpretation of the Parameters of a Multinomial Logistic Regressor</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-softmax-regressor">The Softmax Regressor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="logistic-regression">
<h1>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h1>
<p>Linear regression allows to model relationships between <strong>continuos independent and dependent variables</strong> and <strong>between qualitative independent variables and continuous variables</strong>. However, it does not allow to model relationships between continuous or qualitative independent variables and <strong>qualitative dependent variables</strong>.</p>
<section id="example-data">
<h2>Example Data<a class="headerlink" href="#example-data" title="Permalink to this heading">#</a></h2>
<p>Establishing such relationships is useful in different contexts. For instance, let us consider the <a class="reference external" href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic">Breast Cancer Wisconsin</a> dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>radius1</th>
      <th>texture1</th>
      <th>perimeter1</th>
      <th>area1</th>
      <th>smoothness1</th>
      <th>compactness1</th>
      <th>concavity1</th>
      <th>concave_points1</th>
      <th>symmetry1</th>
      <th>fractal_dimension1</th>
      <th>...</th>
      <th>texture3</th>
      <th>perimeter3</th>
      <th>area3</th>
      <th>smoothness3</th>
      <th>compactness3</th>
      <th>concavity3</th>
      <th>concave_points3</th>
      <th>symmetry3</th>
      <th>fractal_dimension3</th>
      <th>Diagnosis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.16220</td>
      <td>0.66560</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
      <td>M</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.12380</td>
      <td>0.18660</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.14440</td>
      <td>0.42450</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
      <td>M</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>...</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.20980</td>
      <td>0.86630</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>...</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.13740</td>
      <td>0.20500</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
      <td>M</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>0.05623</td>
      <td>...</td>
      <td>26.40</td>
      <td>166.10</td>
      <td>2027.0</td>
      <td>0.14100</td>
      <td>0.21130</td>
      <td>0.4107</td>
      <td>0.2216</td>
      <td>0.2060</td>
      <td>0.07115</td>
      <td>M</td>
    </tr>
    <tr>
      <th>565</th>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>0.05533</td>
      <td>...</td>
      <td>38.25</td>
      <td>155.00</td>
      <td>1731.0</td>
      <td>0.11660</td>
      <td>0.19220</td>
      <td>0.3215</td>
      <td>0.1628</td>
      <td>0.2572</td>
      <td>0.06637</td>
      <td>M</td>
    </tr>
    <tr>
      <th>566</th>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>0.05648</td>
      <td>...</td>
      <td>34.12</td>
      <td>126.70</td>
      <td>1124.0</td>
      <td>0.11390</td>
      <td>0.30940</td>
      <td>0.3403</td>
      <td>0.1418</td>
      <td>0.2218</td>
      <td>0.07820</td>
      <td>M</td>
    </tr>
    <tr>
      <th>567</th>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>0.07016</td>
      <td>...</td>
      <td>39.42</td>
      <td>184.60</td>
      <td>1821.0</td>
      <td>0.16500</td>
      <td>0.86810</td>
      <td>0.9387</td>
      <td>0.2650</td>
      <td>0.4087</td>
      <td>0.12400</td>
      <td>M</td>
    </tr>
    <tr>
      <th>568</th>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>0.05884</td>
      <td>...</td>
      <td>30.37</td>
      <td>59.16</td>
      <td>268.6</td>
      <td>0.08996</td>
      <td>0.06444</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.2871</td>
      <td>0.07039</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 31 columns</p>
</div></div></div>
</div>
<p>The dataset contains several measurements of given quantities measured from digitized image of a fine needle aspirate (FNA) of a breast mass, together with a categorical variable <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> with two levels: <code class="docutils literal notranslate"><span class="pre">M</span></code> (malignant) and <code class="docutils literal notranslate"><span class="pre">B</span></code> (benign).</p>
<p>In this case, it would be good to be able to study whether a relationship exists between some of the considered independent variables and the dependent variable.</p>
<p>We will consider the <code class="docutils literal notranslate"><span class="pre">radius1</span></code> variable for the moment. Let us plot this variable with respect to <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/01c33ebded8bb30ea7bb8492a0c9edac9908961157669eb728f33788ce57ff28.png" src="../_images/01c33ebded8bb30ea7bb8492a0c9edac9908961157669eb728f33788ce57ff28.png" />
</div>
</div>
<p>From the plot above, we can note that there is some form of relationship between the two variables. Indeed:</p>
<ul class="simple">
<li><p>For low values of <code class="docutils literal notranslate"><span class="pre">radius1</span></code>, we tend to have more benign cases;</p></li>
<li><p>For large values of <code class="docutils literal notranslate"><span class="pre">radius1</span></code>, we tend to have more malignant cases.</p></li>
</ul>
</section>
<section id="limits-of-linear-regression">
<h2>Limits of Linear Regression<a class="headerlink" href="#limits-of-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Of course, we would like to quantify this relationship in a more formal way.
<strong>As in the case of a linear regressor, we want to define a model which can predict the independent variable <span class="math notranslate nohighlight">\(y\)</span> from the dependent variables <span class="math notranslate nohighlight">\(x_i\)</span>. If such model gives good predictions, than we can trust its interpretation as a means of studying the relationship between the variables.</strong></p>
<p>We can think of converting <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">=&gt;</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">=&gt;</span> <span class="pre">0</span></code>, and then compute a linear regressor:</p>
<div class="math notranslate nohighlight">
\[Diagnosis = \beta_0 + \beta_1 radius1\]</div>
<p>This would be the result:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/ee318307dbc7f52c1a2c2a7f5c8daee9f19f1d06e6b2ed625dfcccff403e0df5.png" src="../_images/ee318307dbc7f52c1a2c2a7f5c8daee9f19f1d06e6b2ed625dfcccff403e0df5.png" />
</div>
</div>
<p>We can immediately see that this function does not model the relationship between the two variables very well. While we obtain a statistically relevant regressor with <span class="math notranslate nohighlight">\(R^2=0.533\)</span> and statistically relevant coefficients, the residual plot will look like this:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/cb22e8950a18ed9d77288bf5aaf8f32f3fa2d4ebee49d85905d6c1576a8ec27e.png" src="../_images/cb22e8950a18ed9d77288bf5aaf8f32f3fa2d4ebee49d85905d6c1576a8ec27e.png" />
</div>
</div>
<p>The correlation between the residuals and the independent variable is a strong indication that the true relationship between the two variables is not correcly modeled. After all, from a purely predictive point of view, we are using a linear regressor which takes the form:</p>
<div class="math notranslate nohighlight">
\[f:\mathbb{R} \to \mathbb{R}\]</div>
<p>while the values <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> variable belong to the set <span class="math notranslate nohighlight">\(\{0,1\}\)</span> and we would need instead a function with the following form:</p>
<div class="math notranslate nohighlight">
\[f:\mathbb{R} \to \{0,1\}\]</div>
<p>However, the linear regressor cannot directly predict <strong>discrete values</strong>.</p>
<p><strong>In practice, while with a linear regressor we wanted to predict continuous values, now we want to assign observations <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to discrete bins (in this case only two possible ones). As we will better study later in the course, this problem is known as classification.</strong></p>
</section>
<section id="from-binary-values-to-probabilities">
<h2>From Binary Values to Probabilities<a class="headerlink" href="#from-binary-values-to-probabilities" title="Permalink to this heading">#</a></h2>
<p>If we want to model some form of continuous value, we could think to transition from <span class="math notranslate nohighlight">\(\{0,1\}\)</span> to <span class="math notranslate nohighlight">\([0,1]\)</span> using probabilities, which is a way to turn discretized values to “soft” values indicating our belief in the fact that <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> will take either a <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span> value. We could hence think to model the following probability, rather than modeling <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code> directly:</p>
<div class="math notranslate nohighlight">
\[P(Diagnosis=1| radius1)\]</div>
<p>However, even in this case, a model of the form:</p>
<div class="math notranslate nohighlight">
\[P(Diagnosis=1|radius1) = \beta_0 + \beta_1 radius1\]</div>
<p>Would not be appropriate. Indeed, while <span class="math notranslate nohighlight">\(P(Diagnosis=1| radius1)\)</span> needs to be in the <span class="math notranslate nohighlight">\([0,1]\)</span> range, the linear combination <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 radius1\)</span> will naturally output values <strong>smaller than <span class="math notranslate nohighlight">\(0\)</span></strong> and <strong>larger than <span class="math notranslate nohighlight">\(1\)</span></strong>. How should we interpret such values?</p>
<p>Intuitively, we would expect to <span class="math notranslate nohighlight">\(P(Diagnosis=1| radius1)\)</span> to assume values in the <span class="math notranslate nohighlight">\([0,1]\)</span> range for intermediate values (say <code class="docutils literal notranslate"><span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(\in [10,20]\)</span>), while for extremely low values of (say <code class="docutils literal notranslate"><span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(&lt;10\)</span>) the probability <strong>should saturate to <span class="math notranslate nohighlight">\(0\)</span></strong> and for extremely large values (say <code class="docutils literal notranslate"><span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(&gt;20\)</span>) the probability should saturate to 1.</p>
<p>when <code class="docutils literal notranslate"><span class="pre">radius1</span></code> takes large values (say larger than <span class="math notranslate nohighlight">\(20\)</span>), we expect <strong>probability to saturate to <span class="math notranslate nohighlight">\(1\)</span></strong>.</p>
<p>In practice, we would expect a result similar to the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/65b1ad849523ee9c99475d6bbe79b679a33735a93fea247a052e153198cc37bc.png" src="../_images/65b1ad849523ee9c99475d6bbe79b679a33735a93fea247a052e153198cc37bc.png" />
</div>
</div>
<p>As can be noted, the function above is not linear, and hence it cannot be fit with a linear regressor. However, we have seen that a linear regressor can be tweaked to also represent nonlinear functions.</p>
</section>
<section id="the-logistic-function">
<h2>The Logistic Function<a class="headerlink" href="#the-logistic-function" title="Permalink to this heading">#</a></h2>
<p>Similarly to polynomial regression, we need to find a <strong>transformation of the formulation of the linear regressor to transform its output into a nonlinear function of the independent variables</strong>. Of course, we do not want <em>any</em> transformation, but one that has the previously highlighted properties. While different functions have similar characteristics, in practice the <strong>logistic function has some nice properties that, as we will se in a moment, allow to easily interpret the resulting model in a probabilistic way</strong>. The logistic function is defined as:</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{1+e^{-x}}\]</div>
<p>and has the following shape:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/cc727668f29fecd8a6d4ca465dbb298a07d5ce320a48c8aa16ca077b3e7b70c4.png" src="../_images/cc727668f29fecd8a6d4ca465dbb298a07d5ce320a48c8aa16ca077b3e7b70c4.png" />
</div>
</div>
<p>As we can see, the function has the properties we need:</p>
<ul class="simple">
<li><p>Its values are comprised between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>;</p></li>
<li><p>It saturates to <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> for extreme values of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
</section>
<section id="the-logistic-regression-model">
<h2>The Logistic Regression Model<a class="headerlink" href="#the-logistic-regression-model" title="Permalink to this heading">#</a></h2>
<p>In practice, we define our model, <strong>the logistic regressor model</strong> as follows (<strong>simple logistic regression</strong>):</p>
<div class="math notranslate nohighlight">
\[P(Diagnosis=1|X) = f(\beta_0 + \beta_1 X) = \frac{1}{1+e^{-(\beta_0 + \beta_1 X)}}\]</div>
<p>Or, more in general (<strong>multiple logistic regression</strong>):</p>
<div class="math notranslate nohighlight">
\[P(y=1|\mathbf{x}) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n)}}\]</div>
<p>It is easy to see that:</p>
<div class="math notranslate nohighlight">
\[p=\frac{1}{1+e^{-x}} \Rightarrow p+pe^{-x} = 1 \Rightarrow pe^{-x} = 1-p \Rightarrow e^{-x} = \frac{1-p}{p} \Rightarrow e^{x} = \frac{p}{1-p}\]</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[e^{\beta_0+\beta_1x_1 + \ldots + \beta_nx_n} = \frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\]</div>
<p>We note that the term on the right is the odd of <span class="math notranslate nohighlight">\(P(y=1|\mathbf{x})\)</span>. We recall that the odd of <span class="math notranslate nohighlight">\(P(y=1|\mathbf{x})\)</span> is the number of times we believe the example will be positive (observed <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) over the number of times we believe the example will be negative. For instance, if we believe that the example will be positive <span class="math notranslate nohighlight">\(3\)</span> times out of <span class="math notranslate nohighlight">\(10\)</span>, then the odd will be <span class="math notranslate nohighlight">\(\frac{3}{7}\)</span>.</p>
<p>By taking the logarithm of both terms, we obtain:</p>
<div class="math notranslate nohighlight">
\[\log \left(\frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\right) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<p>The expression:</p>
<div class="math notranslate nohighlight">
\[\log \left(\frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\right)\]</div>
<p>Is the logarithm of the odd (log odd), and it is called <strong>logit</strong>, hence the <strong>logistic regression</strong> is sometimes called <strong>logit regression</strong>.</p>
<p>The expression above shows how a logistic regressor can be seen as <strong>a linear regressor (the expression on the right side of the equation) on the logit (the log odd)</strong>. This paves the way to useful interpretations of the model, as shown in the next section.</p>
</section>
<section id="statistical-interpretation-of-the-coefficients-of-a-linear-regressor">
<h2>Statistical Interpretation of the Coefficients of a Linear Regressor<a class="headerlink" href="#statistical-interpretation-of-the-coefficients-of-a-linear-regressor" title="Permalink to this heading">#</a></h2>
<p>Let’s now see how to interpret the coefficients of a logistic regressor. Remember that the regression model (in the case of simple logistic regression) is as follows:</p>
<div class="math notranslate nohighlight">
\[
\log(\frac{p}{1-p})=\beta_0 + \beta_1 x
\]</div>
<p>Applying what we know about linear regressors, we can write:</p>
<div class="math notranslate nohighlight">
\[x=0 \Rightarrow \ln(\frac{p}{1-p})=\beta_0\]</div>
<p>To have a clearer picture, we can exponentiate both sides of the equation and write:</p>
<div class="math notranslate nohighlight">
\[x=0 \Rightarrow \frac{p}{1-p}=e^{\beta_0}\]</div>
<p>Remember that <span class="math notranslate nohighlight">\(\frac{p}{1-p}\)</span> is the odds that the dependent variable is equal to 1 when observing <span class="math notranslate nohighlight">\(x\)</span> and, as such, it has a clear interpretation. For example, if the odds of an event are <span class="math notranslate nohighlight">\(\frac{3}{1}\)</span>, then it is <span class="math notranslate nohighlight">\(3\)</span> times more likely to occur than not to occur. So, <strong>for <span class="math notranslate nohighlight">\(x=0\)</span>, it is <span class="math notranslate nohighlight">\(e^{\beta_0}\)</span> times more likely that the dependent variable is equal to 1, rather than being equal to 0</strong>.</p>
<p>It can be seen that:</p>
<div class="math notranslate nohighlight">
\[p=\frac{odds}{1+odds}\]</div>
<p>Hence, the probability of the event being true when all variables are zero is given by:</p>
<div class="math notranslate nohighlight">
\[p(y=1|x) = \frac{e^\beta_0}{1+e^\beta_0}\]</div>
<p>How can we interpret the <strong>coefficient values</strong>?</p>
<p>We know that:</p>
<div class="math notranslate nohighlight">
\[odds(p|x) = \frac{P(y=1|x)}{1-P(y=1|x)}\]</div>
<p>We can write:</p>
<div class="math notranslate nohighlight">
\[
\log odds(p|x) = \beta_0 + \beta_1 x
\]</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[\log odds(p|x+1) - \log odds(p|x) = \beta_0 + \beta_1 (x+1) - \beta_0 - \beta_1 x = \beta_1 (x+1) - \beta_1 x = \beta_1\]</div>
<p>Exponentiating both sides, we get:</p>
<div class="math notranslate nohighlight">
\[e^{\log odds(p|x+1) - \log odds(p|x)} = e^{\beta_1} \Rightarrow \frac{e^{\log odds(p|x+1)}}{e^{\log odds(p|x)}} = e^{\beta_1} \Rightarrow \frac{odds(p|x+1)}{odds(p|x)} = e^{\beta_1} \Rightarrow odds(p|x+1) = e^{\beta_1}odds(p|x)\]</div>
<p>We can thus say that <strong>increasing the variable <span class="math notranslate nohighlight">\(x\)</span> by one unit corresponds to a multiplicative increase in odds by <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span></strong>.</p>
<p>This analysis can be easily extended to the case of a multiple logistic regressor. Hence in general, given the model:</p>
<div class="math notranslate nohighlight">
\[P(y=1|\mathbf{x}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<p>We can say that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e^\beta_0\)</span> is the odd of <span class="math notranslate nohighlight">\(y\)</span> being equal to <span class="math notranslate nohighlight">\(1\)</span> rather than <span class="math notranslate nohighlight">\(0\)</span> when <span class="math notranslate nohighlight">\(x_i=0 \forall i\)</span>;</p></li>
<li><p>An increment of one unit in the independent variable <span class="math notranslate nohighlight">\(x_i\)</span> corresponds to a multiplicative increment of <span class="math notranslate nohighlight">\(e^\beta_i\)</span> in the odds of <span class="math notranslate nohighlight">\(y=1\)</span>. So if <span class="math notranslate nohighlight">\(e^\beta_i=0.05\)</span>, then <span class="math notranslate nohighlight">\(y=1\)</span> is <span class="math notranslate nohighlight">\(5\%\)</span> more likely for a one-unit increment of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
</section>
<section id="geometrical-interpretation-of-the-coefficients-of-a-logistic-regressor">
<h2>Geometrical Interpretation of the Coefficients of a Logistic Regressor<a class="headerlink" href="#geometrical-interpretation-of-the-coefficients-of-a-logistic-regressor" title="Permalink to this heading">#</a></h2>
<p>Similar to linear regression, also the coefficients of logistic
regression have a geometrical interpretation. We will see that, while
linear regression finds a «curve» that fits the data, logistic
regression finds a hyperplane that separates the data.</p>
<p>Let us consider a simple example with bi-dimensional data
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathfrak{R}^{2}\)</span> as the one shown in the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d3f5c09f0f264dc55c163080d421b6a2e28653b2c3e463f302984e021cdbe2e3.png" src="../_images/d3f5c09f0f264dc55c163080d421b6a2e28653b2c3e463f302984e021cdbe2e3.png" />
</div>
</div>
<p>Let us assume that we fit a logistic regressor model to this data</p>
<div class="math notranslate nohighlight">
\[P\left( y=1 | \mathbf{x} \right) = \frac{1}{1 + e^{- {(\beta}_{0} + \beta_{1}x_{1} + \beta_{2}x_{2})}}\]</div>
<p>and find the following values for the parameters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{ \begin{matrix}
\beta_{0} = - 3.47 \\
\beta_{1} = 1.17 \\
\beta_{2} = 1.43 \\
\end{matrix} \right.\ \end{split}\]</div>
<p>We know that these parameters allow to find a probability value according to the formula above.
We can use these values to <strong>classify the observations</strong> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In practice, a reasonable criterion to classify observations would be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat y = \begin{cases}1 &amp; \text{if } P(y=1|\mathbf{x}) \geq 0.5\\0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>
<p>This makes sense as we are assigning the observations to the group for which the posterior probability <span class="math notranslate nohighlight">\(P(y|\mathbf{x})\)</span> is higher.</p>
<p>To understand how the data is classified, we can look at those points in
which the classifier is uncertain, which is often called <strong>the decision boundary</strong>, i.e.,
those points in which <span class="math notranslate nohighlight">\(P\left( y=1 | \mathbf{x} \right) = 0.5\)</span>.</p>
<p>We note that:</p>
<div class="math notranslate nohighlight">
\[P\left(y=1 | \mathbf{x} \right) = 0.5 \Leftrightarrow e^{- (\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2})} = 1 \Leftrightarrow 0 = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}\]</div>
<p>This last equation is the equation of a line (in the form
<span class="math notranslate nohighlight">\(ax + by + c = 0\)</span>). We can see it in explicit form:</p>
<div class="math notranslate nohighlight">
\[x_{2} = - \frac{\beta_{1}}{\beta_{2}}x_{1} - \frac{\beta_{0}}{\beta_{2}}\]</div>
<p>So, we have found a line which has a</p>
<ul class="simple">
<li><p>Angular coefficient equal to <span class="math notranslate nohighlight">\(- \frac{\beta_{1}}{\beta_{2}}\)</span>;</p></li>
<li><p>Intercept equal to <span class="math notranslate nohighlight">\(- \frac{\beta_{0}}{\beta_{2}}\)</span>;</p></li>
</ul>
<p>If we plot this line, we obtain the <strong>decision boundary</strong> which
separates the elements from the two classes:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c0c82e00d05df69a65ebe76581a0935b0c7c8bdbfdbb4a78dc43c85e69233dc8.png" src="../_images/c0c82e00d05df69a65ebe76581a0935b0c7c8bdbfdbb4a78dc43c85e69233dc8.png" />
</div>
</div>
<p>As can be seen, the decision boundary found by a logistic regressor is a
line. This is because <strong>a logistic regressor is a linear classifier</strong>,
despite the logistic function is not linear!</p>
</section>
<section id="estimation-of-the-parameters-of-a-logistic-regressor">
<h2>Estimation of the Parameters of a Logistic Regressor<a class="headerlink" href="#estimation-of-the-parameters-of-a-logistic-regressor" title="Permalink to this heading">#</a></h2>
<p>To fit the model and find suitable values for the <span class="math notranslate nohighlight">\(\mathbf{\beta_i}\)</span>
parameters, we will define a <strong>cost function</strong>, similarly to what we have
done in the case of linear regression.</p>
<p>Even if we can see the logistic
regression problem as the linear regression problem of fitting the
<span class="math notranslate nohighlight">\(logit(p) = \mathbf{\beta}^{T}\mathbf{x}\)</span> function, differently from
linear regression, <strong>we should note that we do not have the ground truth probabilities p</strong>.
Indeed, our observations only provide input examples
<span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> and the corresponding labels <span class="math notranslate nohighlight">\(y^{(i)}\)</span>.</p>
<p>Starting from the definition:</p>
<div class="math notranslate nohighlight">
\[P\left( y = 1 \middle| \mathbf{x}; \mathbf{\beta} \right) = f_{\mathbf{\beta}}\left( \mathbf{x} \right) = \frac{1}{1 + e^{- \mathbf{\beta}^{T}\mathbf{x}}} = \sigma(\mathbf{\beta}^{T}\mathbf{x})\]</div>
<p>We can write:</p>
<div class="math notranslate nohighlight">
\[P\left( y = 1 \middle| \mathbf{x};\mathbf{\beta} \right) = f_{\mathbf{\beta}}(\mathbf{x})\]</div>
<div class="math notranslate nohighlight">
\[P\left( y = 0 \middle| \mathbf{x};\mathbf{\beta} \right) = 1 - f_{\mathbf{\beta}}(\mathbf{x})\]</div>
<p>Since <span class="math notranslate nohighlight">\(y\)</span> can only take values <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, this can also be written as follows in a more compact form:</p>
<div class="math notranslate nohighlight">
\[P\left( y \middle| \mathbf{x};\mathbf{\beta} \right) = \left( f_{\mathbf{\beta}}\left( \mathbf{x} \right) \right)^{y}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x} \right) \right)^{1 - y}\]</div>
<p>Indeed, when <span class="math notranslate nohighlight">\(y = 1\)</span>, the second factor is equal to <span class="math notranslate nohighlight">\(1\)</span> and the
expression reduces to
<span class="math notranslate nohighlight">\(P\left( y = 1 \middle| \mathbf{x};\mathbf{\beta} \right) = f_{\mathbf{\beta}}(\mathbf{x})\)</span>.
Similarly, if <span class="math notranslate nohighlight">\(y = 0\)</span>, the first factor is equal to <span class="math notranslate nohighlight">\(1\)</span> and the
expression reduces to <span class="math notranslate nohighlight">\(1 - f_{\mathbf{\beta}}(x)\)</span>.</p>
<p>We can estimate the parameters by maximum likelihood, i.e., choosing the
values of the parameters which maximize the probability of the data
under the model identified by the parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[L\left( \mathbf{\beta} \right) = P(Y|X;\mathbf{\beta})\]</div>
<p>If we assume that the training examples are all independent, the
likelihood can be expressed as:</p>
<div class="math notranslate nohighlight">
\[L\left( \mathbf{\beta} \right) = \prod_{i = 1}^{N}{P(y^{(i)}|\mathbf{x}^{(i)};\mathbf{\beta})} = \prod_{i = 1}^{N}{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)^{y^{(i)}}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right) \right)^{{1 - y}^{(i)}}}\]</div>
<p>Maximizing this expression is equivalent to minimizing the negative
logarithm of <span class="math notranslate nohighlight">\(L(\mathbf{\beta})\)</span> (negative log-likelihood - nll):</p>
<div class="math notranslate nohighlight">
\[nll\left( \mathbf{\beta} \right) = - \log{L\left( \mathbf{\beta} \right)} = - \sum_{i = 1}^{N}{\log\left\lbrack f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)^{y^{(i)}}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right) \right)^{{1 - y}^{(i)}} \right\rbrack} =\]</div>
<div class="math notranslate nohighlight">
\[= - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)})\rbrack\]</div>
<p>Hence, we will define our cost function as:</p>
<div class="math notranslate nohighlight">
\[J\left( \mathbf{\beta} \right) = - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)})\rbrack\]</div>
<p>This can be rewritten more explicitly in terms of the <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>
parameters as follows:</p>
<div class="math notranslate nohighlight">
\[J\left( \mathbf{\beta} \right) = - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - \sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)})\rbrack\]</div>
<p>Similarly to linear regression, we now have a cost function to minimize in order to find the values of the <span class="math notranslate nohighlight">\(\beta_i\)</span> parameters. Unfortunately, in this case, <span class="math notranslate nohighlight">\(J(\mathbf{\beta})\)</span> assumes a nonlinear form <strong>which prevents us to use the least square principles</strong> and <strong>there is no closed form solution for the parameter estimation</strong>. In these cases, parameters can be estimated using some form of <strong>iterative solver</strong>, which begins with an initial guess for the parameters and iteratively refine them to find the final solution. Luckily, the logistic regression cost function <strong>is convex, and hence only a single solution is admitted, independently from the initial guess</strong>.</p>
<p>Different iterative solvers can be used in practice. The most commonly used is the <strong>gradient descent algorithm</strong>, which requires the cost function to be differentiable. We will not see this algorithm in details, but an introduction to it and its application to the estimation of the parameters of a logistic regressor are given in the following (hidden) section.</p>
<div class="toggle docutils container">
<p class="rubric">Estimating the Parameters of a Logistic Regressor through Gradient Descent</p>
<p>Given the cost function <span class="math notranslate nohighlight">\(J(\mathcal{\beta})\)</span> above, we want to find suitable values <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> by
solving the following optimization problem:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\beta} = \arg_{\mathbf{\beta}}\min{J(\mathbf{\beta})}\]</div>
<p>As we already discussed, since <span class="math notranslate nohighlight">\(J\)</span> is nonlinear, we cannot find a closed form solution for the estimation of the parameters.</p>
<p>Alternatively, we could <strong>compute</strong>
<span class="math notranslate nohighlight">\(\mathbf{J}\left( \mathbf{\beta} \right)\)</span> <strong>for all possible values
of</strong> <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> <strong>and choose the values of</strong> <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>
<strong>which minimizes the cost</strong>. However, this option is unfeasible in
practice as <span class="math notranslate nohighlight">\(\beta\)</span> may assume an infinite number of values. Hence, we
need a way to <strong>find the values of</strong> <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> <strong>which
minimize</strong> <span class="math notranslate nohighlight">\(\mathbf{J(}\mathbf{\beta}\mathbf{)}\)</span> <strong>without computing</strong>
<span class="math notranslate nohighlight">\(\mathbf{J(}\mathbf{\beta}\mathbf{)}\)</span> <strong>for all possible values of</strong>
<span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span><strong>.</strong></p>
<p>In these cases, we can use <strong>gradient descent</strong>, a numerical optimization strategy
which allows to <strong>minimize differentiable functions</strong> with respect
to their parameters.</p>
<p>We will introduce the gradient descent algorithm considering initially
the problem of minimizing a function of a single variable <span class="math notranslate nohighlight">\(J(\theta)\)</span>.
We will then extend to the case of multiple variables.</p>
<p><strong>The gradient descent algorithm is based on the observation that, if a
function</strong> <span class="math notranslate nohighlight">\(J(\theta)\)</span> <em>is defined and differentiable in a neighborhood
of a point</em> <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span><em>, then</em> <span class="math notranslate nohighlight">\(J(\theta)\)</span> <em>decreases fastest if one
goes from</em> <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span> <em>towards the direction of the negative
derivative of</em> <span class="math notranslate nohighlight">\(J\)</span> <em>computed in</em> <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span><em>.</em> Consider the function
<span class="math notranslate nohighlight">\(J(\theta)\)</span> shown in the plot below:</p>
<p>In these cases, we can use <strong>gradient descent</strong>, a numerical optimization strategy
which allows to <strong>minimize differentiable functions</strong> with respect
to their parameters.</p>
<p>We will introduce the gradient descent algorithm considering initially
the problem of minimizing a function of a single variable <span class="math notranslate nohighlight">\(J(\theta)\)</span>.
We will then extend to the case of multiple variables.</p>
<p><strong>The gradient descent algorithm is based on the observation that, if a
function</strong> <span class="math notranslate nohighlight">\(J(\theta)\)</span> <em>is defined and differentiable in a neighborhood
of a point</em> <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span><em>, then</em> <span class="math notranslate nohighlight">\(J(\theta)\)</span> <em>decreases fastest if one
goes from</em> <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span> <em>towards the direction of the negative
derivative of</em> <span class="math notranslate nohighlight">\(J\)</span> <em>computed in</em> <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span><em>.</em> Consider the function
<span class="math notranslate nohighlight">\(J(\theta)\)</span> shown in the plot below:</p>
<p><img alt="" src="../_images/gd1.png" /></p>
<p>Let us assume that we are at the initial point <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span>. From the
plot, we can see that we should move to the right part of the x axis in
order to reach the minimum of the function.</p>
<p><img alt="" src="../_images/gd2.png" /></p>
<p>The first derivative of the function in that point <span class="math notranslate nohighlight">\(J'(\beta^{(0)})\)</span>
will be equal to the angular coefficient of the tangent to the curve in
the point <span class="math notranslate nohighlight">\((\theta^{(0)},J\left( \theta^{(0)} \right))\)</span>. Since the curve
is decreasing in a neighborhood of <span class="math notranslate nohighlight">\(\beta^{(0)}\)</span>, the tangent line will
also be decreasing. Therefore, its angular coefficient
<span class="math notranslate nohighlight">\(J'(\theta^{(0)})\)</span> will be negative. If we want to move to the right, we
should follow in the <em>inverse direction</em> of the derivative of the curve
in that point.</p>
<p>The gradient descent is an iterative algorithm; hence we are not trying
to reach the minimum of the function in one step. Instead, we would like
to move to another point <span class="math notranslate nohighlight">\(\theta^{(1)}\)</span> such that
<span class="math notranslate nohighlight">\(J\left( \theta^{(1)} \right) &lt; J(\theta^{(0)})\)</span>. If we can do this for
every point, we can reach the minimum in a number of steps.</p>
<p>At each step, we will move proportionally to the value of the
derivative. This is based on the observation that larger absolute values
of the derivative indicate steeper curves. If we choose a multiplier
factor <span class="math notranslate nohighlight">\(\gamma\)</span>, we will move to the point:</p>
<div class="math notranslate nohighlight">
\[\theta^{(1)} = \theta^{(0)} - \gamma J'(\theta^{(0)})\]</div>
<p>For instance, if we choose <span class="math notranslate nohighlight">\(\gamma = 0.02\)</span>, we will move to point
<span class="math notranslate nohighlight">\(\theta^{(1)} = 0.4 + 0.02 \cdot 1.8 = 0.436\)</span>. The procedure works
iteratively until the derivative is so small that no movement is
possible, as shown in the following figure:</p>
<p><img alt="" src="../_images/gd3.png" /></p>
<p>In the next step, we compute the derivative of the function in the
current point <span class="math notranslate nohighlight">\(J\left( \beta^{(1)} \right) = - 0.8\)</span> and move to point
<span class="math notranslate nohighlight">\(\beta^{(2)} = \beta_{1} - \gamma J'(\beta^{(1)})\)</span>.</p>
<p><img alt="" src="../_images/gd4.png" /></p>
<p>Next, we compute the derivative of the function in the current point
<span class="math notranslate nohighlight">\(f\left( \theta^{(2)} \right) = - 0.4\)</span> and move to point
<span class="math notranslate nohighlight">\(\theta^{(3)} = \theta^{(2)} - \gamma J'(\theta^{(2)})\)</span>:</p>
<p><img alt="" src="../_images/gd5.png" /></p>
<p>We then compute the derivative of the current point
<span class="math notranslate nohighlight">\(J\left( \theta^{(3)} \right) \approx 0\)</span>:</p>
<p><img alt="" src="../_images/gd6.png" /></p>
<p>This derivative is so small that we cannot advance further. We are in a
local minimum. The optimization terminates here. We have found the value
<span class="math notranslate nohighlight">\(\theta^{(3)} = \arg_{\theta}\min{J(\theta)}\)</span>.</p>
<p>In practice, the algorithm is terminated following a <strong>given termination
criterion</strong>. Two common criteria are:</p>
<ul class="simple">
<li><p>A maximum number of iterations is reached.</p></li>
<li><p>The value <span class="math notranslate nohighlight">\(\gamma J'(\theta)\)</span> is below a given threshold.</p></li>
</ul>
<p class="rubric">Global vs Local Minima</p>
<p>It is important to note that gradient descent can be applied only if the
cost function is <strong>differentiable with respect to its parameters</strong>.
Moreover, the algorithm is guaranteed to converge to the global minimum
<strong>only if the cost function is convex</strong>. In the general case of
non-convex loss function, the algorithm may converge to a <strong>local
minimum</strong>, which may represent a <strong>suboptimal solution</strong>. Nevertheless,
when the number of parameters is very large, gradient descent <strong>usually
finds a good enough solution</strong>, even if it only converges to a local
minimum.</p>
<p class="rubric">One Variable</p>
<p>The gradient descent algorithm can be written in the following form in
the case of one variable:</p>
<ol class="arabic simple">
<li><p>Choose an initial random point <span class="math notranslate nohighlight">\(\beta\)</span>;</p></li>
<li><p>Compute the first derivative of the function <span class="math notranslate nohighlight">\(J'\)</span> in the current
point <span class="math notranslate nohighlight">\(\theta\)</span>: <span class="math notranslate nohighlight">\(J'(\theta)\)</span>;</p></li>
<li><p>Update the position of the current point using the formula
<span class="math notranslate nohighlight">\(\theta = \theta - \gamma J'(\theta)\)</span>;</p></li>
<li><p>Repeat 2-3 until some termination criteria are met.</p></li>
</ol>
<p class="rubric">Multiple Variables</p>
<p>The gradient descent algorithm generalizes to the case in which the
function <span class="math notranslate nohighlight">\(J\)</span> to optimize depends on multiple variables
<span class="math notranslate nohighlight">\(J(\theta_{1},\theta_{2},\ldots,\theta_{n})\)</span>.</p>
<p>For instance, let’s consider a function of two variables
<span class="math notranslate nohighlight">\(J(\theta_{1},\theta_{2})\)</span>. We can plot such function as a 3D plot
(left) or as a contour plot (right). In both cases, our goal is to reach
the point with the minimum value (the ‘center’ of the two plots). Given
a point <span class="math notranslate nohighlight">\(\mathbf{\theta} = (\theta_{1},\theta_{2})\)</span>, the direction of
steepest descent is the <strong>gradient</strong> of the function in the point.</p>
<p><img alt="" src="../_images/gd7.png" /></p>
<p>The gradient is a multi-variable generalization of the derivative. The
gradient of a function of <span class="math notranslate nohighlight">\(n\)</span> variable computed in a point
<span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span> is a vector whose <span class="math notranslate nohighlight">\(i^{th}\)</span> variable is given by the
partial derivative of the function with respect to the <span class="math notranslate nohighlight">\(i^{th}\)</span>
variable:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla J\left( \mathbf{\theta} \right) = \begin{pmatrix}
J_{\theta_{1}}(\mathbf{\theta}) \\
J_{\theta_{2}}(\mathbf{\theta}) \\
\begin{matrix}
\ldots \\
J_{\theta_{n}}(\mathbf{\theta}) \\
\end{matrix} \\
\end{pmatrix}\end{split}\]</div>
<p>In the case of two variables, the gradient will be a 2D vector (the
gradient) indicating the direction to follow. Since in general we want
to optimize multi-variable functions, the algorithm is called ‘gradient
descent’.</p>
<p>The following figure shows an example of an optimization procedure to
reach the center of the curve from a given starting point:</p>
<p><img alt="" src="../_images/gd8.png" /></p>
<p>The pseudocode of the procedure, in the case of the multiple variables
is as follows:</p>
<ol class="arabic simple">
<li><p>Initialize
<span class="math notranslate nohighlight">\(\mathbf{\theta} = (\theta_{1},\theta_{2},\ldots,\theta_{n})\)</span>
randomly.</p></li>
<li><p>For each variable <span class="math notranslate nohighlight">\(x_{i}\)</span>:</p></li>
<li><p>Compute the partial derivative at the point:</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial\theta_{i}}J\left( \mathbf{\theta} \right)\)</span></p></li>
<li><p>Update the current variable using the formula:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\theta_{i} = \theta_{i} - \gamma\frac{\partial}{\partial\theta_{i}}J(\mathbf{\theta})\]</div>
<ol class="arabic simple" start="6">
<li><p>Repeat 2-3 until the termination criteria are met.</p></li>
</ol>
<p class="rubric">Logistic Regression and Gradient Descent</p>
<p>It can be shown that, in the case of the logistic regressor, the update rule will be:</p>
<div class="math notranslate nohighlight">
\[\beta_{j}\mathbf{=}\beta_{j}\mathbf{-}\gamma\sum_{i = 1\ }^{N}{x_{j}^{(i)}\left( \sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right) - y^{(i)} \right)}\]</div>
<p>For the most curious, the details are in the following section.</p>
<p class="rubric">Derivation of the Update Rule</p>
<p>Let us first consider the derivative of the logistic function:</p>
<div class="math notranslate nohighlight">
\[\sigma(x) = \frac{1}{1 + e^{- x}}\]</div>
<p>This will be:</p>
<p><span class="math notranslate nohighlight">\(\sigma^{'}(x) = - \frac{1}{\left( 1 + e^{- x} \right)^{2}}D\left\lbrack e^{- x} \right\rbrack = \frac{e^{- x}}{\left( 1 + e^{- x} \right)^{2}}\)</span>
[Applying the rule
<span class="math notranslate nohighlight">\(D\left\lbrack \frac{1}{f(x)} \right\rbrack = - \frac{D\left\lbrack f(x) \right\rbrack}{f(x)^{2}}\)</span>]</p>
<p><span class="math notranslate nohighlight">\(\sigma^{'}(x) = \frac{1 + e^{- x} - 1}{\left( 1 + e^{- x} \right)^{2}}\)</span>
[Summing and subtracting <span class="math notranslate nohighlight">\(1\)</span> to the numerator]</p>
<p><span class="math notranslate nohighlight">\(\sigma^{'}(x) = \frac{1 + e^{- x}}{\left( 1 + e^{- x} \right)^{2}} - \frac{1}{\left( 1 + e^{- x} \right)^{2}}\)</span>
[Splitting the numerator in two terms]</p>
<p><span class="math notranslate nohighlight">\(\sigma^{'}(x) = \frac{1}{1 + e^{- x}}\left( 1 - \frac{1}{1 + e^{- x}} \right)\)</span>
[making the <span class="math notranslate nohighlight">\(\frac{1}{1 + e^{- x}}\)</span> factor explicit]</p>
<p><span class="math notranslate nohighlight">\(\sigma^{'}(x) = \sigma(x)\left( 1 - \sigma(x) \right)\)</span> [replacing the
formula <span class="math notranslate nohighlight">\(\sigma(x) = \frac{1}{1 + e^{- x}}\)</span>]</p>
<p class="rubric">Partial Derivatives of the Cost Function</p>
<p>We now need to obtain the partial derivatives of the cost function with
respect to the j-th parameter:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J\left( \mathbf{\beta} \right)}{\partial\beta_{j}}\]</div>
<p>Remember that the cost function is defined as:</p>
<div class="math notranslate nohighlight">
\[J\left( \mathbf{\beta} \right) = - \sum_{i = 1}^{N}{y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - \sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)})\]</div>
<p>Let us first compute the derivative of the first term of the addition:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial\beta_{j}}y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\)</span>
[Initial expression]</p>
<p><span class="math notranslate nohighlight">\(= y^{(i)}\frac{1}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\frac{\partial}{\beta_{j}}\left\lbrack \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right\rbrack\)</span>
[Applying
<span class="math notranslate nohighlight">\(D\left\lbrack \log\left( f(x) \right) \right\rbrack = \frac{1}{f(x)}f'(x)\)</span>]</p>
<p><span class="math notranslate nohighlight">\(= y^{(i)}\frac{1}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)\frac{\partial}{\beta_{j}}\left\lbrack {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right\rbrack\)</span>
[Applying
<span class="math notranslate nohighlight">\(\sigma^{'}\left( f(x) \right) = \sigma\left( f(x) \right)\left( 1 - \sigma\left( f(x) \right) \right)f'(x)\rbrack\)</span></p>
<p><span class="math notranslate nohighlight">\(= y^{(i)}\frac{1}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)x_{j}^{(i)}\)</span>
[Applying
<span class="math notranslate nohighlight">\(\frac{\partial}{\beta_{j}}\left\lbrack {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right\rbrack = x_{j}\)</span>]</p>
<p>The derivative of the second term will be:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial\beta_{j}}{(1 - y}^{(i)})\log\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)\)</span>
[Initial Expression]</p>
<p><span class="math notranslate nohighlight">\(= \left( 1 - y^{(i)} \right)\frac{1}{1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\frac{\partial}{\beta_{j}}\ \lbrack 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\rbrack\)</span>
[Applying
<span class="math notranslate nohighlight">\(D\left\lbrack \log\left( f(x) \right) \right\rbrack = \frac{1}{f(x)}f'(x)\)</span>]</p>
<p><span class="math notranslate nohighlight">\(= \left( 1 - y^{(i)} \right)\frac{1}{1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}( - 1)\sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)\left( 1 - \sigma\left( \mathbf{\beta}^{T}x^{(i)} \right) \right)x_{j}^{(i)}\)</span>
[Applying <span class="math notranslate nohighlight">\(\sigma(x)\left( 1 - \sigma(x) \right)\)</span> and
<span class="math notranslate nohighlight">\(\frac{\partial}{\beta_{j}}\left\lbrack {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right\rbrack = x_{j}\)</span>]</p>
<p><span class="math notranslate nohighlight">\(= \frac{y^{(i)} - 1}{1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)\left( 1 - \sigma\left( \mathbf{\beta}^{T}x^{(i)} \right) \right)x_{j}^{(i)}\)</span>
[Simplifying]</p>
<p>We can write the sum of the last two derivatives as follows:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial}{\partial\beta_{j}}y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)} + \frac{\partial}{\partial\beta_{j}}{(1 - y}^{(i)})\log\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)\)</span>
[Initial Expression]</p>
<p><span class="math notranslate nohighlight">\(= \frac{y^{(i)}}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)x_{j}^{(i)} + \frac{y^{(i)} - 1}{1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}\sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)\left( 1 - \sigma\left( \mathbf{\beta}^{T}x^{(i)} \right) \right)x_{j}^{(i)}\)</span>
[Replacing derivatives]</p>
<p><span class="math notranslate nohighlight">\(= \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)x_{j}^{(i)}\left\lbrack \frac{y^{(i)}}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)} + \frac{y^{(i)} - 1}{1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)} \right\rbrack\)</span>
[Simplifying]</p>
<p><span class="math notranslate nohighlight">\(= \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)x_{j}^{(i)}\left\lbrack \frac{y^{(i)}\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right) + (y^{(i)} - 1)\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)} \right\rbrack\)</span>
[Simplifying]</p>
<p><span class="math notranslate nohighlight">\(= \frac{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)}{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)}x_{j}^{(i)}\left\lbrack y^{(i)}\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right) + (y^{(i)} - 1)\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right\rbrack\)</span>
[Simplifying]</p>
<p><span class="math notranslate nohighlight">\(= x_{j}^{(i)}\left\lbrack y^{(i)} - y^{(i)}\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) + y^{(i)}\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right\rbrack\)</span>
[Simplifying]</p>
<p><span class="math notranslate nohighlight">\(= x_{j}^{(i)}\left( y^{(i)} - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)\)</span>
[Simplifying]</p>
<p>The derivative of the cost function with respect to the j-th parameter
will hence be:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J\left( \mathbf{\beta} \right)}{\partial\beta_{j}} = - \sum_{i = 1}^{N}{\frac{\partial}{\partial\beta_{j}}y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)} + \frac{\partial}{\partial\beta_{j}}{(1 - y}^{(i)})\log\left( 1 - \sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right) \right)} = \sum_{i}^{N}{x_{j}^{(i)}\left( y^{(i)} - \sigma\left( \beta^{T}x^{(i)} \right) \right)}\]</div>
<p>The gradient descent update rule for each parameter will be:</p>
<div class="math notranslate nohighlight">
\[\beta_{j}\mathbf{=}\beta_{j}\mathbf{-}\gamma\sum_{i = 1\ }^{N}{x_{j}^{(i)}\left( \sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right) - y^{(i)} \right)}\]</div>
</div>
</section>
<section id="example-of-logistic-regression">
<h2>Example of Logistic Regression<a class="headerlink" href="#example-of-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>Let us now apply logistic regression to a larger set of variables in our regression problem. We will consider the following independent variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">radius1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">texture1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">perimeter1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">area1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">smoothness1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compactness1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">concavity1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">symmetry1</span></code></p></li>
</ul>
<p>The dependent variable is again <code class="docutils literal notranslate"><span class="pre">Diagnosis</span></code>.</p>
<p>Once fit to the data, we will obtain the following parameters:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>Adj. <span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>Prob(F-statistic)</p></th>
<th class="head"><p>Log-Likelihood</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.670</p></td>
<td><p>0.666</p></td>
<td><p>142.4</p></td>
<td><p>1.16e-129</p></td>
<td><p>-78.055</p></td>
</tr>
</tbody>
</table>
<p>All values have interpretations similar to the ones obtained in the case of linear regression. The Log-Likelihood reports the value of the logarithm of the likelihood which was used to train the data.</p>
<p>The estimates for the coefficients are as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>   -2.6591</td> <td>    0.224</td> <td>  -11.896</td> <td> 0.000</td> <td>   -3.098</td> <td>   -2.220</td>
</tr>
<tr>
  <th>radius1</th>      <td>    0.4688</td> <td>    0.133</td> <td>    3.532</td> <td> 0.000</td> <td>    0.208</td> <td>    0.730</td>
</tr>
<tr>
  <th>texture1</th>     <td>    0.0219</td> <td>    0.003</td> <td>    7.376</td> <td> 0.000</td> <td>    0.016</td> <td>    0.028</td>
</tr>
<tr>
  <th>perimeter1</th>   <td>   -0.0473</td> <td>    0.021</td> <td>   -2.272</td> <td> 0.023</td> <td>   -0.088</td> <td>   -0.006</td>
</tr>
<tr>
  <th>area1</th>        <td>   -0.0009</td> <td>    0.000</td> <td>   -3.985</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>
</tr>
<tr>
  <th>smoothness1</th>  <td>    5.1389</td> <td>    1.221</td> <td>    4.208</td> <td> 0.000</td> <td>    2.740</td> <td>    7.538</td>
</tr>
<tr>
  <th>compactness1</th> <td>    0.3080</td> <td>    0.854</td> <td>    0.360</td> <td> 0.719</td> <td>   -1.370</td> <td>    1.986</td>
</tr>
<tr>
  <th>concavity1</th>   <td>    2.0973</td> <td>    0.414</td> <td>    5.065</td> <td> 0.000</td> <td>    1.284</td> <td>    2.911</td>
</tr>
<tr>
  <th>symmetry1</th>    <td>    1.2739</td> <td>    0.568</td> <td>    2.244</td> <td> 0.025</td> <td>    0.159</td> <td>    2.389</td>
</tr>
</table></div></div>
</div>
<p>We notice that not all variables have a statistically relevant relationship with the dependent variable. Applying backward elimination, we remove <code class="docutils literal notranslate"><span class="pre">compactness1</span></code> and obtain the following estimates:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>   <td>   -2.6708</td> <td>    0.221</td> <td>  -12.086</td> <td> 0.000</td> <td>   -3.105</td> <td>   -2.237</td>
</tr>
<tr>
  <th>radius1</th>     <td>    0.4360</td> <td>    0.097</td> <td>    4.517</td> <td> 0.000</td> <td>    0.246</td> <td>    0.626</td>
</tr>
<tr>
  <th>texture1</th>    <td>    0.0219</td> <td>    0.003</td> <td>    7.405</td> <td> 0.000</td> <td>    0.016</td> <td>    0.028</td>
</tr>
<tr>
  <th>perimeter1</th>  <td>   -0.0419</td> <td>    0.014</td> <td>   -2.915</td> <td> 0.004</td> <td>   -0.070</td> <td>   -0.014</td>
</tr>
<tr>
  <th>area1</th>       <td>   -0.0010</td> <td>    0.000</td> <td>   -4.477</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>
</tr>
<tr>
  <th>smoothness1</th> <td>    5.3093</td> <td>    1.125</td> <td>    4.719</td> <td> 0.000</td> <td>    3.099</td> <td>    7.519</td>
</tr>
<tr>
  <th>concavity1</th>  <td>    2.1479</td> <td>    0.389</td> <td>    5.517</td> <td> 0.000</td> <td>    1.383</td> <td>    2.913</td>
</tr>
<tr>
  <th>symmetry1</th>   <td>    1.3132</td> <td>    0.557</td> <td>    2.359</td> <td> 0.019</td> <td>    0.220</td> <td>    2.407</td>
</tr>
</table></div></div>
</div>
<p>These are now all statistically relevant. For instance, we can see that:</p>
<ul class="simple">
<li><p>When all variables are set to zero, the odds of the benign tumor are <span class="math notranslate nohighlight">\(e^{-2.6708} \approx 0.07\)</span>, or <span class="math notranslate nohighlight">\(\frac{7}{100}\)</span>. This is a base value.</p></li>
<li><p>An increment in one unit of <code class="docutils literal notranslate"><span class="pre">texture1</span></code> increments the odds of a benign tumor multiplicatively by a factor of <span class="math notranslate nohighlight">\(e^{0.0219} \approx 1.02\)</span> (a +<span class="math notranslate nohighlight">\(2\%\)</span>).</p></li>
<li><p>An increment of one unit of <code class="docutils literal notranslate"><span class="pre">perimeter1</span></code> decrements the odds of benign tumor multiplicatively by a factor of <span class="math notranslate nohighlight">\(e^{-0.0419} \approx 0.96\)</span> (a -<span class="math notranslate nohighlight">\(4\%\)</span>).</p></li>
</ul>
</section>
<section id="multinomial-logistic-regression">
<h2>Multinomial Logistic Regression<a class="headerlink" href="#multinomial-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>In many cases, we want to study the relationship between a set of <strong>continuous or categorical independent variable and a non-binary categorical dependent variable</strong>.</p>
<section id="the-iris-dataset">
<h3>The Iris Dataset<a class="headerlink" href="#the-iris-dataset" title="Permalink to this heading">#</a></h3>
<p>An example is given by Fisher’s Iris dataset.</p>
<p>The dataset was introduced by Ronald Fisher in 1936 and contains observations related to 150 specimens of iris flowers belonging to 3 different species: “setosa”, “versicolor”, and “virginica”. Example images of the three flowers are given below:</p>
<p><img alt="" src="../_images/iris.png" /></p>
<p>For each of the observations, the dataset provides measurements of 4 physical characteristics (length and width of sepals and petals), as illustrated in the image below:</p>
<p><img alt="" src="../_images/sepal_petal.png" /></p>
<p>All the variables in the dataset are numeric, except for <code class="docutils literal notranslate"><span class="pre">species</span></code>, which is categorical. Here are some observations from the dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal_length</th>
      <th>sepal_width</th>
      <th>petal_length</th>
      <th>petal_width</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div></div></div>
</div>
<p>Looking at the scatterplots we can see how the measurements have different ranges (same units, all centimeters):</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a9889b3ca87b0828a39742ef9d9639169a3855d9e75338e5f277de0a9ae5d88d.png" src="../_images/a9889b3ca87b0828a39742ef9d9639169a3855d9e75338e5f277de0a9ae5d88d.png" />
</div>
</div>
<p>Let us observe the scatter matrix</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d46123de31e58dfeb8edb21141cb1f953c65d24573a69c0853fcc211c8f60834.png" src="../_images/d46123de31e58dfeb8edb21141cb1f953c65d24573a69c0853fcc211c8f60834.png" />
</div>
</div>
</section>
<section id="additional-limitations-of-a-linear-regression-in-the-presence-of-more-than-two-classes">
<h3>Additional Limitations of a Linear Regression in the Presence of More than Two Classes<a class="headerlink" href="#additional-limitations-of-a-linear-regression-in-the-presence-of-more-than-two-classes" title="Permalink to this heading">#</a></h3>
<p>Let us consider again a case with a single independent feature. We will select <code class="docutils literal notranslate"><span class="pre">petal_length</span></code>. The following image plots it with respect to the three classes:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/f7d88680b9af0d55dae7bf8b3eabddaebbd9c5bb31e4705c4cba32f8c5e7bb6c.png" src="../_images/f7d88680b9af0d55dae7bf8b3eabddaebbd9c5bb31e4705c4cba32f8c5e7bb6c.png" />
</div>
</div>
<p>From the figure above, one may think that a linear regressor, even if not perfect, could still be an option. However, <strong>to fit a linear regressor, we should first convert species values to numeric values</strong>. A possible outcome could be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/71a8f8a8c549a685f461dd583e0107d992a711b5d9cad5d4fd824f20e2958a93.png" src="../_images/71a8f8a8c549a685f461dd583e0107d992a711b5d9cad5d4fd824f20e2958a93.png" />
</div>
</div>
<p>However, also the following mapping would be valid:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/0c0257412e34ab0a51337949aa06858231fa01169437fb46e8eaa5572e167482.png" src="../_images/0c0257412e34ab0a51337949aa06858231fa01169437fb46e8eaa5572e167482.png" />
</div>
</div>
<p>Even if the two mappings should be equivalent, we can see how a linear model would find very different results.</p>
<p>This is an example of how the linear model is <strong>even more limited when more than two possible outcomes of the dependent variable are possible</strong>.</p>
</section>
<section id="the-multinomial-logistic-regression-model">
<h3>The Multinomial Logistic Regression Model<a class="headerlink" href="#the-multinomial-logistic-regression-model" title="Permalink to this heading">#</a></h3>
<p>When the dependent variable can assume more than two values, <strong>we can define the multinomial logistic regression model</strong>. In this case, we select one fo the values of the dependent variable <span class="math notranslate nohighlight">\(Y\)</span> as <strong>a baseline class</strong>. Without loss of generality, let <span class="math notranslate nohighlight">\(K\)</span> be the number of classes and let <span class="math notranslate nohighlight">\(Y=1\)</span> be the baseline class. Recall that in the case of the logistic regressor, we modeled the logarithm of the odd as our linear function:</p>
<div class="math notranslate nohighlight">
\[\log \left(\frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\right) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<p>Since we have more than one possible outcomes for the dependent variable, rather than modeling the odds, a multinomial logistic regressor models the logarithm of the ratio between a given class <span class="math notranslate nohighlight">\(k\)</span> and the baseline class <span class="math notranslate nohighlight">\(1\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\log\left( \frac{P(Y=k|X=\mathbf{x})}{P(Y=1|X=\mathbf{x})} \right) = \beta_{k0} + \beta_{k1} x_1 + \ldots + \beta_{kn} x_n\]</div>
<p>Note that, in practice, we need to define a different linear function for each class <span class="math notranslate nohighlight">\(k = 1 \ldots K\)</span>, hence we need <span class="math notranslate nohighlight">\((n+1) \times (k-1)\)</span> parameters.</p>
<p>Let <span class="math notranslate nohighlight">\(P(k)=(Y=k|X=\mathcal{x})\)</span> for brevity, <span class="math notranslate nohighlight">\(\mathbf{\beta_k}=(\beta_0,\beta_1,\ldots,\beta_k)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}=(1,x_1,\ldots,x_n)\)</span>. We can see that:</p>
<div class="math notranslate nohighlight">
\[\log\left(\frac{P(k)}{P(1)}\right) = \mathbf{\beta_k}^T\mathbf{X} \Rightarrow P(k)= e^{\mathbf{\beta_k}^T\mathbf{X}} P(1)\]</div>
<p>We can divide the last term by <span class="math notranslate nohighlight">\(1=\sum_{l=1}^K P(l)\)</span>:</p>
<div class="math notranslate nohighlight">
\[ P(k) = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}} P(1)}{\sum_{l=1}^K P(l)} = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{\sum_{l=1}^K \frac{P(l)}{P(1)}} = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{\frac{P(1)}{P(1)}+\sum_{l=2}^K \frac{P(l)}{P(1)}} = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{1+\sum_{l=2}^K \mathbf{e^{\beta_l^T\mathbf{X}}}} \text{   (A)}\]</div>
<p>Note that the expression above can also be seen as:</p>
<div class="math notranslate nohighlight">
\[ P(k) = \frac{\frac{P(k)}{P(1)}}{1+\sum_{l=2}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}\]</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[ P(1) = \frac{1}{1+\sum_{l=2}^K e^{\mathbf{\beta}_l^T\mathbf{X}}} \text{   (B)}\]</div>
<p>We can rewrite the results (A) and (B) in the full form:</p>
<div class="math notranslate nohighlight">
\[ P(Y=k|X=\mathbf{x}) = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{1+\sum_{l=2}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}\]</div>
<div class="math notranslate nohighlight">
\[ P(Y=1|X=\mathbf{x}) = \frac{1}{1+\sum_{l=2}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}\]</div>
<p>These two expressions can be used to compute the probabilities of the classes once that the parameters have been estimated and <span class="math notranslate nohighlight">\(\mathcal{x}\)</span> is observed.</p>
<section id="estimating-the-parameters-of-the-multinomial-logistic-regressor-model">
<h4>Estimating the Parameters of the Multinomial Logistic Regressor Model<a class="headerlink" href="#estimating-the-parameters-of-the-multinomial-logistic-regressor-model" title="Permalink to this heading">#</a></h4>
<p>We can estimate the paramters of the multinomial logistic regressor model again by maximizing the likelihood of the model on the data. Assuming that all observations are i.i.d., we can write:</p>
<div class="math notranslate nohighlight">
\[L(\mathbf{\beta}) = \prod_{i=1}^N P(Y=y^{(i)}|X=\mathbf{x}^{(i)};\mathbf{\beta})\]</div>
<p>Again, we minimize the negative log likelihood:</p>
<div class="math notranslate nohighlight">
\[nll(\mathcal{\beta})=-\log L(\mathbf{\beta}) = -\sum_{i=1}^N \log P(Y=y^{(i)}|X=\mathbf{x}^{(i)};\mathbf{\beta}) = -\sum_{i=1}^N [y^{(i)} \neq 1](e^{-\mathbf{\beta}_k^T\mathbf{X}} - \log(1+\sum_{l=2}^K {\beta}_k^T\mathbf{X})) + [y^{(i)} = 1] (- \log(1+\sum_{l=2}^K {\beta}_k^T\mathbf{X}))\]</div>
<p>Where <span class="math notranslate nohighlight">\([\cdot]\)</span> denotes the Iverson brackets:</p>
<div class="math notranslate nohighlight">
\[\begin{split}[x] = \begin{cases}1 &amp; \text{if } x \text{ is true} \\ 0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>
<p>Similarly to the logistic regressor, there is no close form for the estimation of the parameters, but iterative algorithms such as <strong>gradient descent</strong> are in general used for optimization.</p>
</section>
<section id="interpretation-of-the-parameters-of-a-multinomial-logistic-regressor">
<h4>Interpretation of the Parameters of a Multinomial Logistic Regressor<a class="headerlink" href="#interpretation-of-the-parameters-of-a-multinomial-logistic-regressor" title="Permalink to this heading">#</a></h4>
<p>The statistical interpretation of the parameters of a multinomial logistic regressor is similar to the one of a logistic regressor, but we should pay attention to the choice of the baseline.</p>
<p>Let us consider our example of studying the relationship between <code class="docutils literal notranslate"><span class="pre">sepal_lenght</span></code> and <code class="docutils literal notranslate"><span class="pre">species</span></code>. We will map <code class="docutils literal notranslate"><span class="pre">virginica</span></code> to <span class="math notranslate nohighlight">\(0\)</span>, <code class="docutils literal notranslate"><span class="pre">versicolor</span></code> to <span class="math notranslate nohighlight">\(1\)</span> and <code class="docutils literal notranslate"><span class="pre">setosa</span></code> to <span class="math notranslate nohighlight">\(2\)</span>. Once the model is fit, we will find the following values:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Pseudo <span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>LogLikelihood</p></th>
<th class="head"><p>LLR p-value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.4476</p></td>
<td><p>-91.034</p></td>
<td><p>9.276e-33</p></td>
</tr>
</tbody>
</table>
<p>The value of LLR p-value is telling us that the model is statistically relevant, even if the pseudo <span class="math notranslate nohighlight">\(R^2\)</span> is not very high. The relationship is not completely explained by the multinomial logistic regressor.</p>
<p>The coefficients will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.606893
         Iterations 8
</pre></div>
</div>
<div class="output text_html"><table class="simpletable">
<tr>
    <th>species=1</th>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>   12.6771</td> <td>    2.906</td> <td>    4.362</td> <td> 0.000</td> <td>    6.981</td> <td>   18.373</td>
</tr>
<tr>
  <th>sepal_length</th> <td>   -2.0307</td> <td>    0.466</td> <td>   -4.361</td> <td> 0.000</td> <td>   -2.943</td> <td>   -1.118</td>
</tr>
<tr>
    <th>species=2</th>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>   38.7590</td> <td>    5.691</td> <td>    6.811</td> <td> 0.000</td> <td>   27.605</td> <td>   49.913</td>
</tr>
<tr>
  <th>sepal_length</th> <td>   -6.8464</td> <td>    1.022</td> <td>   -6.698</td> <td> 0.000</td> <td>   -8.850</td> <td>   -4.843</td>
</tr>
</table></div></div>
</div>
<p>We can see that there are two sets of coefficients. One for <code class="docutils literal notranslate"><span class="pre">species=1</span></code> (versicolor) and one for <code class="docutils literal notranslate"><span class="pre">species=2</span></code> (setosa). No coefficients have been estimated for <code class="docutils literal notranslate"><span class="pre">virginica</span></code>, because it has been chosen as the baseline. We can see that all p-values are small, so we can keep all variables. Let us see how to interpret the coefficients:</p>
<ul class="simple">
<li><p>The intercept for <code class="docutils literal notranslate"><span class="pre">species=1</span></code> is <span class="math notranslate nohighlight">\(12.6771\)</span>. This indicates that the odd of <code class="docutils literal notranslate"><span class="pre">versicolor</span></code> versus <code class="docutils literal notranslate"><span class="pre">virginica</span></code> is <span class="math notranslate nohighlight">\(e^{12.6771}=320327.76\)</span>, when <code class="docutils literal notranslate"><span class="pre">sepal_length</span></code> is set to zero. This is a very large number, probably due to the fact that <code class="docutils literal notranslate"><span class="pre">sepal_lenght=0</span></code> is not a realistic observation.</p></li>
<li><p>The intercept for <code class="docutils literal notranslate"><span class="pre">species=2</span></code> is <span class="math notranslate nohighlight">\(38.7590\)</span>. This indicates that the odd of <code class="docutils literal notranslate"><span class="pre">setosa</span></code> versus <code class="docutils literal notranslate"><span class="pre">virginica</span></code> is <span class="math notranslate nohighlight">\(e^{38.7590}=6.8e+16\)</span>, when <code class="docutils literal notranslate"><span class="pre">sepal_length</span></code> is set to zero. Also in this case, we obtain a very large number, probably due to the fact that <code class="docutils literal notranslate"><span class="pre">sepal_lenght=0</span></code> is not a realistic observation.</p></li>
<li><p>The coefficient <span class="math notranslate nohighlight">\(-2.0307\)</span> of <code class="docutils literal notranslate"><span class="pre">sepal_length</span></code> for <code class="docutils literal notranslate"><span class="pre">species=1</span></code> indicates that when we observe an increase in one centimeter of <code class="docutils literal notranslate"><span class="pre">sepal_length</span></code>, the odd of <code class="docutils literal notranslate"><span class="pre">versicolor</span></code> versus <code class="docutils literal notranslate"><span class="pre">virginica</span></code> decreases multiplicatively by <span class="math notranslate nohighlight">\(e^{-2.0307} = 0.13\)</span> (a large -<span class="math notranslate nohighlight">\(87\%\)</span>!).</p></li>
<li><p>The coefficient <span class="math notranslate nohighlight">\(-6.8564\)</span> of <code class="docutils literal notranslate"><span class="pre">sepal_length</span></code> for <code class="docutils literal notranslate"><span class="pre">species=2</span></code> indicates that when we observe an increase in one centimeter of <code class="docutils literal notranslate"><span class="pre">sepal_length</span></code>, the odd of <code class="docutils literal notranslate"><span class="pre">setosa</span></code> versus <code class="docutils literal notranslate"><span class="pre">virginica</span></code> decreases multiplicatively by <span class="math notranslate nohighlight">\(e^{-6.8464} = 0.001\)</span> (a large -<span class="math notranslate nohighlight">\(99.9\%\)</span>!).</p></li>
</ul>
</section>
<section id="the-softmax-regressor">
<h4>The Softmax Regressor<a class="headerlink" href="#the-softmax-regressor" title="Permalink to this heading">#</a></h4>
<p>Softmax regression is an alternative formulation of multinomial logistic regression which is designed to avoid the definition of a baseline and it is hence symmetrical. In a softmax regressor, the probabilities are modeled as follows:</p>
<div class="math notranslate nohighlight">
\[ P(Y=k|X=\mathbf{x}) = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{\sum_{l=1}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}, \ \ \ \forall k=1,\ldots,K\]</div>
<p>So, rather than estimating <span class="math notranslate nohighlight">\(K-1\)</span> coefficients, we estimate <span class="math notranslate nohighlight">\(K\)</span> coefficients.</p>
<p>The optimization of the model is performed defining a similar cost function and optimizing it with iterative methods.</p>
<p>The softmax formulation is widely used in predictive analysis and machine learning, but less pervasive in statistics.</p>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Chapter <span class="math notranslate nohighlight">\(4\)</span> of [1]</p></li>
</ul>
<p>[1] James, Gareth Gareth Michael. An introduction to statistical learning: with applications in Python, 2023.<a class="reference external" href="https://www.statlearning.com">https://www.statlearning.com</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-data">Example Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limits-of-linear-regression">Limits of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-binary-values-to-probabilities">From Binary Values to Probabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-function">The Logistic Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-regression-model">The Logistic Regression Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation-of-the-coefficients-of-a-linear-regressor">Statistical Interpretation of the Coefficients of a Linear Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-interpretation-of-the-coefficients-of-a-logistic-regressor">Geometrical Interpretation of the Coefficients of a Logistic Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-the-parameters-of-a-logistic-regressor">Estimation of the Parameters of a Logistic Regressor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-logistic-regression">Example of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression">Multinomial Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-iris-dataset">The Iris Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-limitations-of-a-linear-regression-in-the-presence-of-more-than-two-classes">Additional Limitations of a Linear Regression in the Presence of More than Two Classes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-multinomial-logistic-regression-model">The Multinomial Logistic Regression Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-parameters-of-the-multinomial-logistic-regressor-model">Estimating the Parameters of the Multinomial Logistic Regressor Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-parameters-of-a-multinomial-logistic-regressor">Interpretation of the Parameters of a Multinomial Logistic Regressor</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-softmax-regressor">The Softmax Regressor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>