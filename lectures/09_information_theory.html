

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>6. Basic Elements of Information Theory &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/09_information_theory';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Introduzione a Numpy" href="../laboratories/03_intro_numpy.html" />
    <link rel="prev" title="5. Probability Distributions" href="08_common_distributions.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">1. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">2. Introduzione a Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_main_data_analysis_concepts.html">3. Introduction to Data Analysis and Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_probability.html">4. Probability for Data Manipulation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_common_distributions.html">5. Probability Distributions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Basic Elements of Information Theory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">7. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">8. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">9. Introduzione a Pandas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">10. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_misure_di_tendenza_centrale_dispersione_e_forma.html">11. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_associazione_variabili.html">12. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">13. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">14. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/08_associazione_variabili.html">15. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_statistical_inference.html">16. Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7 &amp; 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_linear_regression.html">17. Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">18. Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_causal_analysis.html">19. Causal Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/10_statistical_inference.html">20. Laboratory on Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/11_regressione_lineare.html">21. Laboratorio su Regressione Lineare</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/12_regressione_logistica.html">22. Laboratorio su regressione logistica</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_data_as_nd_points.html">23. Data as N-Dimensional Points</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_clustering.html">24. Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="16_density_estimation.html">25. Density Estimation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/09_information_theory.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/09_information_theory.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/09_information_theory.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Basic Elements of Information Theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-information">6.1. Self-information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">6.2. Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-of-a-bernoulli-variable">6.2.1. Entropy of a Bernoulli variable</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-concepts-optional">6.3. Other Concepts (Optional)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-entropy-and-conditional-entropy">6.3.1. Joint Entropy and Conditional Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kullback-leibler-divergence">6.3.2. Kullback-Leibler Divergence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-information">6.3.3. Mutual Information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-processing-inequality">6.3.4. Data Processing Inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">6.4. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="basic-elements-of-information-theory">
<h1><span class="section-number">6. </span>Basic Elements of Information Theory<a class="headerlink" href="#basic-elements-of-information-theory" title="Permalink to this heading">#</a></h1>
<p>Probability Theory allows to deal with the uncertain, however, it does
not allow to quantify it. Information theory provides the tools to
<strong>quantify how much information is present in a signal</strong>.</p>
<p>When dealing with data, it can be useful to understand which data is
informative and which is not, with respect to a given goal.</p>
<p>The basic intuition behind information theory is that learning an
unlikely event has occurred is more informative than learning a likely
event has occurred.</p>
<p>For instance, the event “the sun rose this morning” is very likely and
hence it is not very informative. The message “there was a solar eclipse
this morning” is instead very unlikely, and hence informative.</p>
<section id="self-information">
<h2><span class="section-number">6.1. </span>Self-information<a class="headerlink" href="#self-information" title="Permalink to this heading">#</a></h2>
<p>We would like to quantify information in a way that formalizes the
intuition discussed before:</p>
<ul class="simple">
<li><p>Likely events should have <em>low information content</em></p></li>
<li><p>Less likely events should have <em>higher information content</em></p></li>
</ul>
<p>Moreover, <strong>independent events should have additive
information</strong>. For example, finding out that a tossed coin has
come up as heads twice should convey twice as much information
as finding out that a tossed coin has come up as heads once.</p>
<p>To satisfy these three properties, the notion of <strong>self-information</strong> is
introduced. Given a random variable <span class="math notranslate nohighlight">\(X \sim P\)</span>, the self-information of
an event <span class="math notranslate nohighlight">\(X = x\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[I(x) = - logP(x)\]</div>
<p>If the base of the logarithm is 2, then the self-information is measured
in <strong>bits</strong>.</p>
<p>If the base of the logarithm is e, then the self-information is measures
in <strong>nats.</strong></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/8384810674ba8a06fa7472b57dfe12d2f109b66a3681b9b10d29092dbd5a0771.png" src="../_images/8384810674ba8a06fa7472b57dfe12d2f109b66a3681b9b10d29092dbd5a0771.png" />
</div>
</div>
<p>It should be noted that this definition of self-information satisfies
the three conditions. Indeed:</p>
<ul class="simple">
<li><p>As <span class="math notranslate nohighlight">\(P(x)\)</span> gets larger, <span class="math notranslate nohighlight">\(I(x)\)</span> gets smaller and vice versa</p></li>
<li><p>The self-information for two independent variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is
given by:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(X = x,Y = y) = - \log P(X = x,Y = y)\]</div>
<ul class="simple">
<li><p>Since the variables are independent, <span class="math notranslate nohighlight">\(P(X,Y) = P(X)P(Y)\)</span>, hence:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(X = x,Y = y) = - \log\left\lbrack P(X = x)P(Y = y) \right\rbrack = - logP(X = x) - \log P(Y = y) = I(x) + I(y)\]</div>
<ul class="simple">
<li><p>Hence, also the additivity of the entropies of independent variables
is satisfied.</p></li>
</ul>
</section>
<section id="entropy">
<h2><span class="section-number">6.2. </span>Entropy<a class="headerlink" href="#entropy" title="Permalink to this heading">#</a></h2>
<p><em>Self-information is defined on a single outcome</em>. We can quantify the
amount of <strong>uncertainty</strong> in an entire probability distribution using
the <strong>Shannon entropy</strong>:</p>
<div class="math notranslate nohighlight">
\[H(X) = E_{X \sim P}\left\lbrack I(x) \right\rbrack = - E_{X \sim P}\left\lbrack \log{P(x)} \right\rbrack\]</div>
<p>In the case of a discrete variable:</p>
<div class="math notranslate nohighlight">
\[H(X) = - \sum_{x}^{}{P(x)logP(x)}\]</div>
<p>Where the summation is over all values of <span class="math notranslate nohighlight">\(x \in \ \chi\)</span>, with <span class="math notranslate nohighlight">\(\chi\)</span>
alphabet of <span class="math notranslate nohighlight">\(X\)</span>. In the case of a continuous variable, the sum is
substituted by an integral:</p>
<div class="math notranslate nohighlight">
\[H(X) = - \int_{}^{}{f(x)\log f(x)dx}\]</div>
<p>The Shannon entropy of a distribution is <strong>the expected amount of
information in an event drawn from that distribution</strong>.</p>
<p>The entropy is measured in <strong>bits</strong> or <strong>nats</strong> (depending on the base
of the logarithm) and can be interpreted as <strong>the amount of information
required on the average to describe the random variable</strong>.</p>
<p><strong>Example</strong></p>
<ul class="simple">
<li><p>Let’s consider a die with six faces.</p></li>
<li><p>We can describe the outcomes of rolling the die with a random
variable <span class="math notranslate nohighlight">\(X\)</span> with alphabet <span class="math notranslate nohighlight">\(\chi = \ \left\{ 1,\ldots,6 \right\}\)</span>.</p></li>
<li><p>If the die is fair, then <span class="math notranslate nohighlight">\(P(x) = \frac{1}{6},\ \forall x \in \chi\)</span>.</p></li>
<li><p>The entropy associated with this random variable is:
<span class="math notranslate nohighlight">\(H(X) = - \sum_{i = 1}^{6}{\frac{1}{6}\log}\frac{1}{6} = \log(6) = 2.58\ bits\)</span>.</p>
<ul>
<li><p>This makes sense as the minimum number of bits needed to
represent 8 numbers is 3 (<span class="math notranslate nohighlight">\(2^{3} = 8\)</span>). Since we have less than
8 numbers, in average we need less than 3 bits.</p></li>
</ul>
</li>
<li><p>Let’s suppose the die is not fair and in particular Let’s suppose
it follows a categorical distribution with
<span class="math notranslate nohighlight">\(\mathbf{p} = \lbrack 0.1,0.1,0.1,0.1,0.1,0.5\rbrack\)</span>.</p></li>
<li><p>Intuitively, we need less bits to represent this variable. Indeed,
since event <span class="math notranslate nohighlight">\(X = 6\)</span> is much more frequent, we can represent it with
the least number of bits, whereas we can represent less frequent
outcomes with more bits. In particular, the entropy of the variable
will be:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[H(X) = - \sum_{i = 1}^{5}{0.1\log{0.1}} - 0.5\log(0.5) = 2.16\ bits\]</div>
<ul class="simple">
<li><p>The number of bits required to represent the variable in average is
2.16 bits, smaller than in the case of the fair die.</p></li>
</ul>
<section id="entropy-of-a-bernoulli-variable">
<h3><span class="section-number">6.2.1. </span>Entropy of a Bernoulli variable<a class="headerlink" href="#entropy-of-a-bernoulli-variable" title="Permalink to this heading">#</a></h3>
<p>Let’s consider a variable <span class="math notranslate nohighlight">\(X\)</span> following a Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p\)</span>, i.e.:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X_p = \begin{cases}
1 &amp; \text{with probability } p\\
0 &amp; \text{with probability } 1-p
\end{cases}
\end{split}\]</div>
<p>Then the entropy of this variable will be:</p>
<div class="math notranslate nohighlight">
\[H(X_p) = -p\log p -(-1-p)\log(1-p)\]</div>
<p>If we plot <span class="math notranslate nohighlight">\(H(X_p)\)</span> with respect to <span class="math notranslate nohighlight">\(p\)</span>, we obtain the following graph:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/3eeb3c60bce46cead73ab1cf7fdf032e306d1997a51dee6136cd5ce7920b0291.png" src="../_images/3eeb3c60bce46cead73ab1cf7fdf032e306d1997a51dee6136cd5ce7920b0291.png" />
</div>
</div>
<p>The diagram above illustrates that entropy is maximum for <span class="math notranslate nohighlight">\(p=0.5\)</span>, which is coherent with our intuitive understanding of entropy.</p>
</section>
</section>
<section id="other-concepts-optional">
<h2><span class="section-number">6.3. </span>Other Concepts (Optional)<a class="headerlink" href="#other-concepts-optional" title="Permalink to this heading">#</a></h2>
<section id="joint-entropy-and-conditional-entropy">
<h3><span class="section-number">6.3.1. </span>Joint Entropy and Conditional Entropy<a class="headerlink" href="#joint-entropy-and-conditional-entropy" title="Permalink to this heading">#</a></h3>
<p>Given two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we can define the <strong>joint entropy</strong> as:</p>
<div class="math notranslate nohighlight">
\[H(X,Y) = -\sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y)\]</div>
<p>We will define the <strong>conditional entropy</strong> as follows:</p>
<div class="math notranslate nohighlight">
\[H(X|Y) = -\sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log p(x|y)\]</div>
<p>The conditional entropy measures the uncertainty over <span class="math notranslate nohighlight">\(X\)</span> when <span class="math notranslate nohighlight">\(Y\)</span> is known.</p>
</section>
<section id="kullback-leibler-divergence">
<h3><span class="section-number">6.3.2. </span>Kullback-Leibler Divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Permalink to this heading">#</a></h3>
<p>The Kullback-Leibler divergence (also called KL-divergence) is a statistical distance measure which can be used to quantify the distance between two probability distributions (i.e., how much different they are from one another). For discrete probability distributions, this is defined as:</p>
<div class="math notranslate nohighlight">
\[D_{KL}(P || Q) = \sum_{x \in \chi} P(x)\log\left(\frac{P(x)}{Q(x)}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> are the two distributions we want to compare and <span class="math notranslate nohighlight">\(\chi\)</span> is the sample space of such distributions. In practice, it is <strong>the expectation under the distribution P of the logarithmic difference (<span class="math notranslate nohighlight">\(\log\frac{P(x)}{Q(x)} = \log P(x)-\log Q(x)\)</span>) between the values of <span class="math notranslate nohighlight">\(P(x)\)</span> and <span class="math notranslate nohighlight">\(Q(x)\)</span>.</strong></p>
<p>The KL divergence measures the <strong>inefficiency of assuming that the distribution is <span class="math notranslate nohighlight">\(q\)</span> when the true distribution is <span class="math notranslate nohighlight">\(p\)</span></strong>.</p>
<p><strong>It should be noted that this distance measure is not symmetric.</strong></p>
<p>For two continuous densities <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>:</p>
<div class="math notranslate nohighlight">
\[D_{KL}(p || q) = \int_{\chi} p(x)\log\frac{p(x)}{q(x)}dx\]</div>
<p>The KL divergence is also called <strong>relative entropy</strong>.</p>
</section>
<section id="mutual-information">
<h3><span class="section-number">6.3.3. </span>Mutual Information<a class="headerlink" href="#mutual-information" title="Permalink to this heading">#</a></h3>
<p>Let us consider two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with a joint probability function <span class="math notranslate nohighlight">\(P(X,Y)\)</span>. We define the <strong>mutual information</strong> <span class="math notranslate nohighlight">\(I(X;Y)\)</span> as the <strong>relative entropy</strong> between the joint distribution and the product distribution:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log \frac{P(x,y)}{P(x)P(y)}\]</div>
<p>if the KL divergence measures the inefficiency of using <span class="math notranslate nohighlight">\(q\)</span> when <span class="math notranslate nohighlight">\(p\)</span> is the real distribution, the mutual information measures the inefficiency of using the product <span class="math notranslate nohighlight">\(P(x)P(y)\)</span> instead of <span class="math notranslate nohighlight">\(P(x,y)\)</span>. As you may recall:</p>
<div class="math notranslate nohighlight">
\[P(X,Y)=P(X)P(Y) \Leftrightarrow X \perp Y \]</div>
<p>So:</p>
<ul class="simple">
<li><p>If the two variables are independent, then the mutual information will be equal to zero;</p></li>
<li><p>If the two variables are dependent, then the mutual information will be larger than zero.</p></li>
</ul>
<p>The mutual information allows to quantify how much information a variable will give about another one. <strong>Note that the mutual information is symmetrical.</strong></p>
<p>It can be shown that:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) = H(X) - H(X|Y)\]</div>
<p>From which we observe that <strong>the mutual information between two variables X and Y is the reduction in the uncertainty about one variable when the other one is known.</strong></p>
</section>
<section id="data-processing-inequality">
<h3><span class="section-number">6.3.4. </span>Data Processing Inequality<a class="headerlink" href="#data-processing-inequality" title="Permalink to this heading">#</a></h3>
<p>We will say that random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(Z\)</span> form a Markov Chain in that order (denoted as <span class="math notranslate nohighlight">\(X \to Y \to Z\)</span>) if we can write:</p>
<div class="math notranslate nohighlight">
\[P(X,Y,Z) = P(Z|Y) P(Y|X) P(X)\]</div>
<p>which means that we can obtain <span class="math notranslate nohighlight">\(Y\)</span> as a function of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> as a function of only <span class="math notranslate nohighlight">\(Y\)</span> (without considering <span class="math notranslate nohighlight">\(X\)</span> anymore). In some sense, <span class="math notranslate nohighlight">\(Y\)</span> encodes whatever we need to encode about <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Under these conditions, it can be proved that:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) \geq I(X;Z)\]</div>
<p>which is known as the <strong>data processing inequality</strong>.</p>
<p>This is a fundamental results for data analysis which tells us that, no matter what if <span class="math notranslate nohighlight">\(Y\)</span> has been obtained from <span class="math notranslate nohighlight">\(X\)</span> with some process which induces information loss, there is no way to recover the lost information from <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</section>
</section>
<section id="references">
<h2><span class="section-number">6.4. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Some parts of chapter 1 of [3].</p></li>
</ul>
<p>[3] Cover, Thomas M., and Joy A. Thomas. <em>Elements of information
theory</em>. John Wiley &amp; Sons, 2012.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="08_common_distributions.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Probability Distributions</p>
      </div>
    </a>
    <a class="right-next"
       href="../laboratories/03_intro_numpy.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Introduzione a Numpy</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-information">6.1. Self-information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">6.2. Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-of-a-bernoulli-variable">6.2.1. Entropy of a Bernoulli variable</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-concepts-optional">6.3. Other Concepts (Optional)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-entropy-and-conditional-entropy">6.3.1. Joint Entropy and Conditional Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kullback-leibler-divergence">6.3.2. Kullback-Leibler Divergence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-information">6.3.3. Mutual Information</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-processing-inequality">6.3.4. Data Processing Inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">6.4. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>