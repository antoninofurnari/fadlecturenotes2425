

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>28. Introduction to Predictive Modelling and Regression Models &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/18_predictive_modeling';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="29. Classification Task and Evaluation Measures" href="19_classification.html" />
    <link rel="prev" title="27. Exploratory/Inferential Analysis on the Heart Disease Dataset" href="../practicals/01_heart_disease_explorative_inferential_analysis.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">1. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">2. Introduzione a Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_main_data_analysis_concepts.html">3. Introduction to Data Analysis and Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_probability.html">4. Probability for Data Manipulation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_common_distributions.html">5. Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_information_theory.html">6. Basic Elements of Information Theory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">7. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">8. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">9. Introduzione a Pandas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">10. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_misure_di_tendenza_centrale_dispersione_e_forma.html">11. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_associazione_variabili.html">12. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">13. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">14. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/08_associazione_variabili.html">15. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_statistical_inference.html">16. Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7 &amp; 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_linear_regression.html">17. Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">18. Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_causal_analysis.html">19. Causal Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/10_statistical_inference.html">20. Laboratory on Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/11_regressione_lineare.html">21. Laboratorio su Regressione Lineare</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/12_regressione_logistica.html">22. Laboratorio su regressione logistica</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_data_as_nd_points.html">23. Data as N-Dimensional Points</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_clustering.html">24. Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="16_density_estimation.html">25. Density Estimation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 13</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="17_principal_component_analysis.html">26. Principal Component Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../practicals/01_heart_disease_explorative_inferential_analysis.html">27. Exploratory/Inferential Analysis on the Heart Disease Dataset</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 14</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">28. Introduction to Predictive Modelling and Regression Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 15</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="19_classification.html">29. Classification Task and Evaluation Measures</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_discriminative_models_for_classification.html">30. Discriminative Models for Classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 16</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="21_generative_models_for_classification.html">31. Generative Models for Classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/14_clustering_density_estimation_pca.html">32. Clustering, Density Estimation, and Principal Component Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/15_regression_predictive.html">33. Linear Regression from a Predictive Perspective</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/16_classificazione.html">34. Classificazione</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/18_predictive_modeling.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/18_predictive_modeling.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/18_predictive_modeling.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to Predictive Modelling and Regression Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">28.1. Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">28.1.1. Example 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2">28.1.2. Example 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-learning">28.2. Statistical Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">28.2.1. Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">28.2.2. Supervised vs Unsupervised Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-performance-of-a-predictive-model">28.3. Evaluating the Performance of a Predictive Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">28.3.1. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-and-cross-validation">28.3.2. Generalization and Cross-Validation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#holdout-validation-or-single-split">28.3.2.1. Holdout Validation or Single Split</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">28.3.2.2. K-Fold Cross-Validation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation">28.3.2.3. Leave-One-Out Cross-Validation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-and-hyperparmeter-optimization">28.4. Model Selection and Hyperparmeter Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-predictive-models">28.5. Regression Predictive Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-measures">28.5.1. Evaluation Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">28.5.1.1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">28.5.1.2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">28.5.1.3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization">28.5.2. Linear Regression as a Predictive Model learned with Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-regression">28.5.3. Multivariate Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">28.5.4. Non-linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">28.6. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-predictive-modelling-and-regression-models">
<h1><span class="section-number">28. </span>Introduction to Predictive Modelling and Regression Models<a class="headerlink" href="#introduction-to-predictive-modelling-and-regression-models" title="Permalink to this heading">#</a></h1>
<p>Algorithms and techniques seen so far have been focusing on <strong>understanding the data</strong> in different ways. We have seen methods to <strong>summarize the data</strong>, <strong>study the relationship between variables</strong>, including <strong>cause-effect</strong> relationships, <strong>infer properties of the population which the observed data follow</strong>, <strong>study the internal structure of data</strong>, its <strong>density</strong>, and perform <strong>automated feature extraction with Principal Component Analysis</strong>.</p>
<p>We will now look at data analysis from another perspective, the one of <strong>predictive modelling</strong>. The main goal of predictive modelling is to <strong>build models which can make be used to make predictions on unseen data</strong>. Predictive modelling is a broad field of data analysis, which is very overlapped with <strong>machine learning</strong>.</p>
<p>It is worth mentioning that <strong>predictive modelling is a shift in thinking with respect to the approaches seen so far</strong>. For instance, we have seen how a <strong>linear regressor attempts to predict <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span></strong>. This is a form of predictive modelling, but the goal so far has been to understand the relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, rather than making accurate predictions. Indeed, we have focused on <strong>interpreting the coefficients</strong> and making <strong>hypothesis tests to check if our conclusions would extend to the population as well</strong>. We have also seen how a <strong>simpler model with inferior predictive abilities can sometimes be preferred to a more complex model due to its interpretability</strong>.</p>
<p>The goal of predictive modelling is to <strong>create data models that can support decision making</strong>. Examples of uses of such models are:</p>
<ul class="simple">
<li><p>Determining the probability of a given patient <strong>developing a given pathology from a blood test</strong>;</p></li>
<li><p>Determining if a given bank transaction is <strong>a fraudulent one</strong> (anomaly detection);</p></li>
<li><p>Predicting the <strong>future value of a company stock</strong>;</p></li>
<li><p>Estimating the <strong>energy produced by a solar plant based on weather forecast</strong>;</p></li>
<li><p>Predict whether an <strong>autonomous vehicle should turn left or right given sensor readings and an RGB image observation of the scene</strong>.</p></li>
<li><p>Detect the <strong>presence of people from surveillance cameras</strong>.</p></li>
</ul>
<p>In these cases, we aim to obtain <strong>accurate systems</strong>, but we are less interested in their interpretability. Indeed, while being able to interpret such models is certainly a plus, we will often <strong>prefer more complex but accurate models</strong> even if we loose the ability to interpret them.</p>
<section id="problem-formulation">
<h2><span class="section-number">28.1. </span>Problem Formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this heading">#</a></h2>
<p>We will give a general formulation of predictive modelling problems. Later, we will see instantiations of this formulation to specific predictive modeling problems.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> be two spaces of objects. We can see them as the sample spaces of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, with <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> being two realizations of the random variables: <span class="math notranslate nohighlight">\(X=x\)</span> and <span class="math notranslate nohighlight">\(Y=y\)</span>. The goal of predictive modeling is to find a function:</p>
<div class="math notranslate nohighlight">
\[h : \mathcal{X} \to \mathcal{Y}\]</div>
<p>which outputs an object <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span>, given an object <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>. We often use the symbol <span class="math notranslate nohighlight">\(y\)</span> to refer to the <strong>true value of <span class="math notranslate nohighlight">\(Y=y\)</span> associated to the observation <span class="math notranslate nohighlight">\(X=x\)</span></strong>, while <strong><span class="math notranslate nohighlight">\(\hat y\)</span> will represent the prediction obtained using function <span class="math notranslate nohighlight">\(h\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[\hat y = h(x)\]</div>
<p>The true value <span class="math notranslate nohighlight">\(y\)</span> is often called <strong>ground truth</strong>. The function <span class="math notranslate nohighlight">\(h\)</span> is usually a <strong>data model</strong> and often referred to as <strong>an hypothesis</strong>.</p>
<section id="example-1">
<h3><span class="section-number">28.1.1. </span>Example 1<a class="headerlink" href="#example-1" title="Permalink to this heading">#</a></h3>
<p>As a simple example, we can imagine <span class="math notranslate nohighlight">\(\mathcal{X}=\Re^m\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0,1\}\)</span>, with <span class="math notranslate nohighlight">\(\mathbf{x} \in \Re^m\)</span> being a numerical vector representing the results of different blood tests made on a given subject, while <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span> being a response variable indicating whether the given subject has (<span class="math notranslate nohighlight">\(1\)</span>) or does not have (<span class="math notranslate nohighlight">\(0\)</span>) a given disease.</p>
<p>Finding an appropriate function <span class="math notranslate nohighlight">\(h\)</span> will allow us to predict if a given subject has a given disease from the blood test <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat y = h(\mathbf{x})\]</div>
<p>If we obtain <span class="math notranslate nohighlight">\(\hat y = 1\)</span>, then we will assume that the given person has the disease. While this example is very abstract, we will later see algorithms which assign a specific form to <span class="math notranslate nohighlight">\(h\)</span>.</p>
</section>
<section id="example-2">
<h3><span class="section-number">28.1.2. </span>Example 2<a class="headerlink" href="#example-2" title="Permalink to this heading">#</a></h3>
<p>We want to build a <strong>spam filter</strong> which decides whether a given email is spam or not. In this case, we can consider <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as the <strong>set of all possible emails</strong> and <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0,1\}\)</span>, with <span class="math notranslate nohighlight">\(1\)</span> indicating spam and <span class="math notranslate nohighlight">\(0\)</span> indicating non-spam. Let <span class="math notranslate nohighlight">\(f\)</span> be a function counting the number of orthographical mistakes of a given email <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[f : \mathcal{X} \to R^+\]</div>
<p>We can determine if a given email <span class="math notranslate nohighlight">\(x\)</span> is spam or not by <strong>counting the number of mistakes and checking whether this number exceeds a pre-determined threshold <span class="math notranslate nohighlight">\(\theta\)</span></strong>. We can hence define our function <span class="math notranslate nohighlight">\(h\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x) = \begin{cases} 1 &amp; \text{ if } f(x)&gt;\theta \\ 0 &amp; \text{otherwise} \end{cases}\end{split}\]</div>
<p>As we will later see, this is a <strong>trivial example of a parametric classifier with one parameter</strong>.</p>
</section>
</section>
<section id="statistical-learning">
<h2><span class="section-number">28.2. </span>Statistical Learning<a class="headerlink" href="#statistical-learning" title="Permalink to this heading">#</a></h2>
<p>While there are different ways to define the function <span class="math notranslate nohighlight">\(h\)</span>, the model is often obtained following an approach called <strong>statistical learning</strong>. According to the statistical learning approach, we will assume to have at our disposal a TRaining set (TR) of <span class="math notranslate nohighlight">\(N\)</span> examples:</p>
<div class="math notranslate nohighlight">
\[\text{TR}=\{(x_i,y_i)\}_{i=1}^N\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i \in \mathcal{X}\)</span> is the input object and <span class="math notranslate nohighlight">\(y_i \in \mathcal{Y}\)</span> is the corresponding response that we aim to obtain with <span class="math notranslate nohighlight">\(h(x_i)\)</span>. <strong>The term “training set” is borrowed from the machine learning terminology, where a model is “trained” on a given set of data to improve its performance in tackling a given task</strong>.</p>
<p>More specifically, we will assume that <strong>the examples are independent and identically distributed (i.i.d.)</strong>, following the joint probability <span class="math notranslate nohighlight">\(P(X,Y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[(x,y) \sim P(X,Y)\]</div>
<p>The i.i.d. hypothesis is reasonable and, as we will see later, very useful. Moreover, the assumption that the data follows the joint probability, <strong>allows us to reason in probabilistic terms, modeling the uncertainty in predictions</strong>. Hence, the predicted values will be realization of a random variable following the <strong>conditional distribution</strong> P(y|x):</p>
<div class="math notranslate nohighlight">
\[\hat y = h(x) \text{, with } \hat y \sim P(y|x)\]</div>
<p>To find the appropriate function <span class="math notranslate nohighlight">\(h\)</span>, we will define (depending on the problem) a <strong>real-valued loss (or cost) function</strong></p>
<div class="math notranslate nohighlight">
\[L(\hat y, y)\]</div>
<p>which measures <strong>the loss, or cost</strong> of predicting <span class="math notranslate nohighlight">\(\hat y\)</span> when the true value is <span class="math notranslate nohighlight">\(y\)</span>. If the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> is very similar to the real value <span class="math notranslate nohighlight">\(y\)</span>, then we will obtain a small cost (or loss value) for <span class="math notranslate nohighlight">\(L(\hat y, y)\)</span>. In practice, the form of <span class="math notranslate nohighlight">\(L\)</span> will vary depending on the problem and model at hand.</p>
<p>We will define <strong>the risk <span class="math notranslate nohighlight">\(R(h)\)</span> of a given hypothesis function <span class="math notranslate nohighlight">\(h\)</span></strong> as the <strong>expected loss value under the joint probability distribution</strong>:</p>
<div class="math notranslate nohighlight">
\[R(h) = E_{(x,y) \sim P(X,Y)}[L(h(x),y)]\]</div>
<p><strong>Statistical learning aims to find the optimal <span class="math notranslate nohighlight">\(h^*\)</span> from a fixed class of functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> by solving the following optimization problem</strong>:</p>
<div class="math notranslate nohighlight">
\[h^* = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R(h)\]</div>
<p>Borrowing from the machine learning terminology, we will say that a model as a <strong>large capacity if the number of the associated functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is large</strong>.</p>
<section id="empirical-risk-minimization">
<h3><span class="section-number">28.2.1. </span>Empirical Risk Minimization<a class="headerlink" href="#empirical-risk-minimization" title="Permalink to this heading">#</a></h3>
<p>In general, the risk <span class="math notranslate nohighlight">\(R(h)\)</span> cannot be computed because the joint distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span> is unknown. However, <strong>by the law of large numbers</strong>, we can estimate it empirically using the training set TR:</p>
<div class="math notranslate nohighlight">
\[R_{emp}(h) = \frac{1}{N}\sum_{i=1}^N L(h(x_i),y_i)\]</div>
<p>The <strong>empirical risk minimization principle states that the we should choose a hypothesis <span class="math notranslate nohighlight">\(\hat h\)</span> from the class of hypothesis functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> which minimizes the empirical risk</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)\]</div>
<p>We thus define a <strong>learning algorithm</strong> (i.e., an algorithm used to “learn” the model <span class="math notranslate nohighlight">\(\hat h\)</span> from the training set TR) which solves the above defined optimization problem.</p>
</section>
<section id="supervised-vs-unsupervised-learning">
<h3><span class="section-number">28.2.2. </span>Supervised vs Unsupervised Learning<a class="headerlink" href="#supervised-vs-unsupervised-learning" title="Permalink to this heading">#</a></h3>
<p>In the machine learning terminology, predictive models as just defined are also referred to as <strong>supervised learning algorithms</strong>, where the world “supervised” indicates that we are given input objects <span class="math notranslate nohighlight">\(x\)</span> and expected (or desired) out values <span class="math notranslate nohighlight">\(y\)</span>. This is in contrast with <strong>unsupervised</strong> algorithms, in which we are only given the <span class="math notranslate nohighlight">\(x\)</span> values. Examples of such algorithms are the Principal Component Analysis and Clustering Algorithms.</p>
</section>
</section>
<section id="evaluating-the-performance-of-a-predictive-model">
<h2><span class="section-number">28.3. </span>Evaluating the Performance of a Predictive Model<a class="headerlink" href="#evaluating-the-performance-of-a-predictive-model" title="Permalink to this heading">#</a></h2>
<p>Once we choose a model <span class="math notranslate nohighlight">\(\hat h\)</span> minimizing the empirical risk computed on the training set, we need to <strong>evaluate the performance of the model</strong>. This is a fundamental step, as we want to know <strong>how well will the model do “in the wild”, beyond the training set</strong>.</p>
<section id="overfitting">
<h3><span class="section-number">28.3.1. </span>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this heading">#</a></h3>
<p>One may think that a model minimizing the empirical risk will be the best possible one. However, especially <strong>when the training set is not large enough (as compared to the complexity of the model) or not representative enough of the joint probability distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span></strong>, the estimation of the empirical risk may be limited. As an example, consider the following model:</p>
<div class="math notranslate nohighlight">
\[\hat h(x) = y \text{ s.t. } (x,y) \in \text{TR}\]</div>
<p>The hypothesis <span class="math notranslate nohighlight">\(\hat h\)</span> defined above will lead to an empirical risk equal to <span class="math notranslate nohighlight">\(0\)</span> as long as the loss function is defined in such a way that <span class="math notranslate nohighlight">\(L(y,y)=0\)</span>. Indeed:</p>
<div class="math notranslate nohighlight">
\[R_{emp}(\hat{h}(x)) = \frac{1}{N}\sum_{i=1}^N L(y,y) = 0\]</div>
<p>However, the function above will not be defined for any value:</p>
<div class="math notranslate nohighlight">
\[x' \in \mathcal{X} \text{ s.t. } (x',y) \notin \text{ TR } \forall y \in \mathcal{Y}\]</div>
<p>Since <strong>we expect the training set to be a sample from the distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span>, we expect such values <span class="math notranslate nohighlight">\(x'\)</span> to exist</strong> (otherwise the training <em>is</em> the population, which is in general not true).</p>
<p>The example above is an extreme case of a phenomenon called <strong>overfitting</strong>, which happens when <strong>a given hypothesis achieves a low error on the training set, but the solution is not general enough to obtain similar performance (i.e., a similar risk) on data not belonging to the training set</strong>.</p>
<p>The phenomenon of overfitting is often related to the <strong>capacity of the model</strong>. A model with a <strong>large capacity</strong> can represent very complex functions <span class="math notranslate nohighlight">\(h\)</span> and result in overfitting, while a model is a <strong>small capacity</strong> can represent relatively simple functions <span class="math notranslate nohighlight">\(h\)</span>, making overfitting harder, but possibly resulting in an <strong>underfitting model</strong>, i.e., a model which is too simple, and as a result cannot reach a low empirical risk.</p>
<p>This is best seen with the simple regression example below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d5daacb1d4dbb45a484458331f314f0a202a9ff1e3e7c609ae9d931266fdb8c9.png" src="../_images/d5daacb1d4dbb45a484458331f314f0a202a9ff1e3e7c609ae9d931266fdb8c9.png" />
</div>
</div>
<p>The example shows the fit of three polynomial regression models of different degrees (the first one is degree 1, which corresponds to a linear regressor). As can be noted:</p>
<ul class="simple">
<li><p>A lower degree polynomial (e.g., a line - left) will lead to <strong>underfitting</strong>. In this case, the model is too simple to model the data well, so the empirical risk will be large;</p></li>
<li><p>A higher degree polynomial (right) will minimize the empirical risk but find a complex model which will not describe well data which has not been seen at training time. This is a case of <strong>overfitting</strong>. The model is trying to model the idiosyncrasies of the training set (which can be noisy) finding a solution which will work only on the data at hand;</p></li>
<li><p>An appropriate degree (center) will lead to a model with an appropriate capacity. The empirical risk is minimized and the solution also works with unseen data.</p></li>
</ul>
<blockquote>
<div><p>Note that this is another instance of the <strong>bias-variance</strong> tradeoff. Complex models (right) have a large variance and a large bias: small variations of the training set are modeled and can lead to wrong solutions. Models that are too simple  (left) have a low variance, but can still have a large bias (the model is too simple and the solution is not good). Choosing an appropriate capacity (in this case by choosing an appropriate polynomial degree) leads to a good trade-off between variance and bias.</p>
</div></blockquote>
<p><strong>Regularization and Bias-Variance tradeoff</strong>
In practice, there are different ways to reduce the model capacity. In the example above, the degree of the polynomial has been used as an hyperparameter to reduce the capacity of the model. A different approach would be to use regularization techniques. The plot below shows the result of fitting a polynomial regressor on the same data when Ridge regression is used:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/004f153413b125255aa08a2ced079b7e47672757fe5adfa5ab161fe1ebdc0b8f.png" src="../_images/004f153413b125255aa08a2ced079b7e47672757fe5adfa5ab161fe1ebdc0b8f.png" />
</div>
</div>
<p>Note that even a polynomial of degree equal to <span class="math notranslate nohighlight">\(50\)</span> (right) achieves reasonable results now.</p>
</section>
<section id="generalization-and-cross-validation">
<h3><span class="section-number">28.3.2. </span>Generalization and Cross-Validation<a class="headerlink" href="#generalization-and-cross-validation" title="Permalink to this heading">#</a></h3>
<p>The example above has shown how it is possible to obtain models that work well on the training data but do not generalize to unseen data, while ideally we would want models which work well on the test data as well. We call the ability of the model to obtain a similar error (e.g., the empirical risk)  on both the training set and unseen data <strong>generalization</strong>.</p>
<p>To measure the ability of a model to generalize, we could just take the model and use it “<strong>in the wild</strong>”. However, since we want our tests to be repeatable, we instead resort to a series of techniques which <strong>consist in using a part of our dataset as a training set and different part of the dataset as a test set</strong>. These techniques are generally referred to as <strong>cross validation</strong>. In the following, we see the main approaches to cross validation.</p>
<p>However, before to proceed, we should point out that, while we could evaluate the performance of a model with the <strong>empirical risk</strong>, in practice, it is common to use <strong>performance measures</strong> (the higher the better) or <strong>error measures</strong> (the lower the better) which may be different from the <strong>loss function chosen to train the algorithm</strong>. This is due to the fact that loss functions often need to have some properties to facilitate learning, so they can represent an <strong>approximation of or a deviation from our true objective measure of performance</strong>. We will see more in details the main performance measures later, but for the moment we will note that we will evaluate models using a given performance measure:</p>
<div class="math notranslate nohighlight">
\[p(\hat Y, Y) : \mathcal{Y}^N \times \mathcal{Y}^N \to \Re\]</div>
<p>where <span class="math notranslate nohighlight">\(Y=\{y_i | (x_i,y_i) \in S \}_{i=1}^N\)</span> is a set of ground truth values from a set of data <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(\hat Y = \{h(x_i) | (x_i,y_i) \in S \}_{i=1}^N\)</span> is the set of corresponding predictions. Note that, given the definition above, the empirical risk can be seen as a performance measure, but we need not restrict to the empirical risk to evaluate predictive models.</p>
<section id="holdout-validation-or-single-split">
<h4><span class="section-number">28.3.2.1. </span>Holdout Validation or Single Split<a class="headerlink" href="#holdout-validation-or-single-split" title="Permalink to this heading">#</a></h4>
<p>The simplest form of cross-validation is the holdout test. In this case, the initial set of data is split into two different sets: a training set, which will be used to optimize the model (e.g., with empirical risk minimization) and a test set, which is used to evaluate the performance of the model. The act of optimizing the model on the training set is often called <strong>training</strong>, while the act of evaluating the performance of the model on the test set is called <strong>testing</strong>. This approach <strong>pretends that the test data is not available at training time and only uses it to evaluate performance</strong>.</p>
<p>The <strong>rule number one</strong> when using this technique is <strong>to avoid in any way to choose any characteristic of the model based on the training data</strong>. Indeed, if we did so, we could end up in some form of unmeasurable overfitting. To make sure that both the training and test set are i.i.d., before the split, the data is <strong>randomly shuffled</strong>.</p>
<p>Also, since training data is usually precious and datasets are often not large enough, it is common to split the data asymmetrically, choosing <span class="math notranslate nohighlight">\(70-80\%\)</span> of the data for training and <span class="math notranslate nohighlight">\(20-30\%\)</span> of the data for testing.</p>
<p>The figure below illustrates the splitting process:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/904debe02349b0102836dd0a1893a7dbfa1596960185f043b1c13976904724b9.png" src="../_images/904debe02349b0102836dd0a1893a7dbfa1596960185f043b1c13976904724b9.png" />
</div>
</div>
</section>
<section id="k-fold-cross-validation">
<h4><span class="section-number">28.3.2.2. </span>K-Fold Cross-Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this heading">#</a></h4>
<p>When the dataset is very small, we may not want to sacrifice a part of it for testing only as this may lead to a biased model. Also, we should note that a small test set could lead to a biased estimation of the model performance. In these cases, rather than randomly splitting the data into two parts, we randomly split it into <span class="math notranslate nohighlight">\(K\)</span> different parts, which will be called <strong>folds</strong>.</p>
<p>We then perform training and testing <span class="math notranslate nohighlight">\(K\)</span> times, each time using fold <span class="math notranslate nohighlight">\(i\)</span> as the test set, and the remaining folds as the training set. The final model performance is obtained by averaging the performance scores computed in each iteration. Note that the obtained performance is unbiased, as each number in the final average is computed on data which has not been seen during training.</p>
<p>The figure below illustrates the case of a 4-fold cross-validation.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/06fcb8d0ce0bbaae070c5a2d454d700cedffb4be6c2457ad0eea4b02b6cf495f.png" src="../_images/06fcb8d0ce0bbaae070c5a2d454d700cedffb4be6c2457ad0eea4b02b6cf495f.png" />
</div>
</div>
<p>The K-Fold cross validation has the advantage of allowing to validate the model without throwing away a significant part of the data, but it is practically feasible only when the training procedure is not too computationally expensive. Indeed, if training a model takes one week, a K-Fold validation will typically require four weeks.</p>
</section>
<section id="leave-one-out-cross-validation">
<h4><span class="section-number">28.3.2.3. </span>Leave-One-Out Cross-Validation<a class="headerlink" href="#leave-one-out-cross-validation" title="Permalink to this heading">#</a></h4>
<p>In leave-one-out cross-validation, the validation stage is performed in <span class="math notranslate nohighlight">\(N\)</span> iterations, where <span class="math notranslate nohighlight">\(N\)</span> is the number of elements in the dataset. At the <span class="math notranslate nohighlight">\(i^{th}\)</span> iteration, the model is trained on all data points except <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> and tested on <span class="math notranslate nohighlight">\((x_i,y_i)\)</span>. The final performance is obtained by averaging the performance scores obtained at each iterations. Note that this is the same as K-Fold cross-validation with <span class="math notranslate nohighlight">\(K=N\)</span>. The figure below illustrates the process:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e0dea6eee59bd3b9d724273fcf072c1df6940ac006597eb865944124e29fcd07.png" src="../_images/e0dea6eee59bd3b9d724273fcf072c1df6940ac006597eb865944124e29fcd07.png" />
</div>
</div>
<p>This approach is useful when the dataset is extremely small, but, similarly to K-Fold cross-validation, it can increase the computation time by a large margin.</p>
</section>
</section>
</section>
<section id="model-selection-and-hyperparmeter-optimization">
<h2><span class="section-number">28.4. </span>Model Selection and Hyperparmeter Optimization<a class="headerlink" href="#model-selection-and-hyperparmeter-optimization" title="Permalink to this heading">#</a></h2>
<p>Many algorithms have some parameters which are not explicitly part of the final model <span class="math notranslate nohighlight">\(h\)</span>, but they need to be set in order to solve the optimization problem. <strong>For example, in a ridge or lasso regressor, the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is used to control the amount of regularization during the learning process, however, the final parameter is not part of the model and is not automatically found during the optimization process</strong>.</p>
<p>Hyperparameters are usually found using grid searches: we train models using different values of the hyperparamters and choose the model which performs best. However, <strong>caution should be taken when hyperparameters are selected with grid search</strong>.</p>
<p>Recall that <strong>we are not allowed to make any choice on the final model using the test set</strong>. Indeed, if we did so, we may incur in a form of overfitting which we would not be able to measure. For instance, we can choose a given parameter which works well only for that specific test set. How can we be sure that performance will be good when new data is analyzed?</p>
<p>To avoid this problem, we should work with three different sets of data: a <strong>training set</strong>, a <strong>validation set</strong>, and a <strong>test set</strong>. We will use the training set to train the model, the validation set to choose the hyperparameters and the test set to test the final performance. This is done by training different models on the test set, choosing the hyperparamter values leading to best values on the validation set, and then re-training the model on the training set (or on the union of training and validation set) to final test on the test set.</p>
<p>The most common scheme is to use a fixed split with a <span class="math notranslate nohighlight">\(60:20:20\)</span> ratio, as shown in the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1e5c54bb346ddb1e36503f523b6424a632178fe37f884642bf7ad57e45cb4720.png" src="../_images/1e5c54bb346ddb1e36503f523b6424a632178fe37f884642bf7ad57e45cb4720.png" />
</div>
</div>
<p>However, other combinations are possible. An approach which often used when the dataset is small is a follows:</p>
<ul class="simple">
<li><p>The dataset is split into a training and test set;</p></li>
<li><p>Hyperparameters are optimized using cross-validation on the training set - this consists in executing different cross-validations with different hyperparameter values - then the parameters achieving the best performance are chosen;</p></li>
<li><p>Once the best hyperparameters are found, the model is re-trained on the full training set and tested on the test set.</p></li>
</ul>
<p>This is illustrated in the following figure:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/bf56183ef633c49b5e2fc09bc6b97ca58263f6a4339c6ed9141bf9d936231140.png" src="../_images/bf56183ef633c49b5e2fc09bc6b97ca58263f6a4339c6ed9141bf9d936231140.png" />
</div>
</div>
<p>Again, if the dataset is large enough and the training procedure is computationally expensive, it is common to use a fixed split as illustrated above.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library allows to easily perform hyperparameter search using cross-validation for different algorithms.</p>
<p>More in general, these techniques can be used to compare different models and select the best performing one.</p>
</section>
<section id="regression-predictive-models">
<h2><span class="section-number">28.5. </span>Regression Predictive Models<a class="headerlink" href="#regression-predictive-models" title="Permalink to this heading">#</a></h2>
<p>We will now investigate an important family of predictive models: regression models. While we have already seen regression models as approaches to understand relationships within the data, in this context we are interested in defining good hypothesis functions <span class="math notranslate nohighlight">\(\hat h\)</span> able to predict continuous values <span class="math notranslate nohighlight">\(\hat y\)</span> from continuous inputs <span class="math notranslate nohighlight">\(x\)</span>. We can see the defined hypothesis function <span class="math notranslate nohighlight">\(\hat h\)</span> as <strong>an approximation of the true function <span class="math notranslate nohighlight">\(f\)</span> generating <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(y=f(x)\)</span></strong>. We will say that we are tackling a <strong>regression problem</strong> whenever the hypothesis function takes the form:</p>
<div class="math notranslate nohighlight">
\[h:\Re^{n} \rightarrow \Re^{m}\]</div>
<p>Note that we have seen the case in which <span class="math notranslate nohighlight">\(n=m=1\)</span> (simple regression) and the case in which <span class="math notranslate nohighlight">\(n&gt;1, m=1\)</span> (multiple regression). When both <span class="math notranslate nohighlight">\(n&gt;1, m&gt;1\)</span>, both the inputs and outputs of the regression model will be vectors of continuous values. We will see how linear regression models can be easily generalized to work in this case as well, but for the moment we will consider the form above as the general form of a regression model.</p>
<section id="evaluation-measures">
<h3><span class="section-number">28.5.1. </span>Evaluation Measures<a class="headerlink" href="#evaluation-measures" title="Permalink to this heading">#</a></h3>
<p>As noted, independently from the employed loss functions, we need evaluation measures to assess the performance of the trained models. Common evaluation measures for regression problems are discussed in the following.</p>
<section id="mean-squared-error-mse">
<h4><span class="section-number">28.5.1.1. </span>Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Permalink to this heading">#</a></h4>
<p>Consider a ground truth label <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathfrak{R}^{m}\)</span> and a
predicted label  <span class="math notranslate nohighlight">\(\widehat{\mathbf{y}} \in \mathfrak{R}^{m}\)</span>. Since both
values are m-dimensional vectors, a natural way to measure if
 <span class="math notranslate nohighlight">\(\widehat{\mathbf{y}}\)</span> is a good approximation of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is to
simply measure their Euclidean distance:</p>
<div class="math notranslate nohighlight">
\[\left\| \widehat{\mathbf{y}} - \mathbf{y} \right\|_{2} = \sqrt{\sum_{i = 1}^{m}\left( {\widehat{y}}_{i} - y_{i} \right)^{2}}\]</div>
<p>In practice, <strong>we often use the squared Euclidean distance to penalize
more large errors</strong>:</p>
<div class="math notranslate nohighlight">
\[error\left( \widehat{\mathbf{y}},\mathbf{y} \right) = \left\| \widehat{\mathbf{y}} - \mathbf{y} \right\|_{2}^{2} = \sum_{i = 1}^{m}\left( {\widehat{y}}_{i} - y_{i} \right)^{2}\]</div>
<p>We can compute the average error over the whole test set to obtain a
performance estimator, which is usually called Mean Squared Error (MSE):</p>
<div class="math notranslate nohighlight">
\[MSE\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{1}{|TE|}\sum_{j = 1}^{|TE|}\left\| {\widehat{\mathbf{y}}}^{(j)} - \mathbf{y}^{(j)} \right\|_{2}^{2}\]</div>
<p>Where <span class="math notranslate nohighlight">\(TE\)</span> denotes the TEst set.</p>
<p>This performance measure is an <strong>error measure</strong>. A good
regressor will obtain a <strong>small error.</strong></p>
<p>Note that the MSE on the test set is equal to the RSS divided by the number of test data points <span class="math notranslate nohighlight">\(|TE|\)</span>.</p>
</section>
<section id="root-mean-squared-error-rmse">
<h4><span class="section-number">28.5.1.2. </span>Root Mean Squared Error (RMSE)<a class="headerlink" href="#root-mean-squared-error-rmse" title="Permalink to this heading">#</a></h4>
<p>The unit of measure of the MSE is the squared unit of measure of the
dependent variable (target value). In practice, if <span class="math notranslate nohighlight">\(y\)</span> is measured in
meters, then the MSE is measures in square meters. This is not very
intuitive. To obtain an error which can be measured in meters, we can
take the squared root of the MSE and obtain the Root Mean Square Error
(RMSE):</p>
<div class="math notranslate nohighlight">
\[RMSE\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \sqrt{\frac{1}{|TE|}\sum_{j = 1}^{|TE|}\left\| {\widehat{\mathbf{y}}}^{(j)} - \mathbf{y}^{(j)} \right\|_{2}^{2}}\]</div>
</section>
<section id="mean-absolute-error-mae">
<h4><span class="section-number">28.5.1.3. </span>Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this heading">#</a></h4>
<p><em>If the target values are scalars</em> (hence <span class="math notranslate nohighlight">\(m = 1\)</span>), another possible
measure which has the same unit of measurement as the target variable is
the MAE, which is simply the average of the absolute errors:</p>
<div class="math notranslate nohighlight">
\[MAE\left( Y,\widehat{Y} \right) = \frac{1}{|TE|}\sum_{i}^{}{|y_{i} - \widehat{y_{i}}|}\]</div>
<p>The main difference between the RMSE/MSE and MAE, is that RMSE/MSE tend
to give less importance to small errors, while giving more importance to
large errors. This comes from the observation that the squared of a
number smaller than one is even smaller than the original number,
whereas the squared of a large number is even larger. On the contrary,
MAE is often more intuitive (it is the average error we make when
estimating y).</p>
</section>
</section>
<section id="linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization">
<h3><span class="section-number">28.5.2. </span>Linear Regression as a Predictive Model learned with Empirical Risk Minimization<a class="headerlink" href="#linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization" title="Permalink to this heading">#</a></h3>
<p>We can see linear regression as a form of predictive modeling. Note that, <strong>when looking at the linear regressor as a predictive model, we are more interested in its accuracy than in its interpretability</strong>. In this context:</p>
<ul class="simple">
<li><p>We will assume <span class="math notranslate nohighlight">\(\mathcal{X}= \Re^n\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}= \Re\)</span></p></li>
<li><p>The hypothesis function will take the form:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[h(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<ul class="simple">
<li><p>We will call our dataset the <strong>training set</strong>;</p></li>
<li><p>We will define our loss function as the error function: <span class="math notranslate nohighlight">\(L(h(x),y) = (h(x)-y)^2\)</span>;</p></li>
</ul>
<p>In this case the empirical risk is defined as follows:</p>
<div class="math notranslate nohighlight">
\[R_{emp} = \frac{1}{N} \sum_{i=1}^N (h(x)-y)^2 = \frac{1}{N} RSS\]</div>
<p>Note that, since:</p>
<div class="math notranslate nohighlight">
\[\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h) = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ \frac{1}{N} \sum_{i=1}^N (h(x)-y)^2 = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ \frac{1}{N} RSS =  \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ RSS\]</div>
<p>we can solve the optimization problem with <strong>Ordinary Least Squares (OLS)</strong> (the same method we used previously to minimize RSS).</p>
<p>From a <strong>learning perspective</strong>, solving the optimization problem <span class="math notranslate nohighlight">\(\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)\)</span> corresponds to <strong>finding the optimal set of parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> minimizing the empirical risk</strong>, which corresponds to minimizing the Residual Sum of Squares, as previously defined.</p>
<p>It is worth to emphasize that we are now interested in minimizing the empirical risk as much as possible, even if the model loses interpretability. Hence, <strong>we can normalize the data, use many variables, include interaction terms, use polynomial regression, regularize with lasso and ridge, as long as it improves the performance of the model</strong>.</p>
</section>
<section id="multivariate-linear-regression">
<h3><span class="section-number">28.5.3. </span>Multivariate Linear Regression<a class="headerlink" href="#multivariate-linear-regression" title="Permalink to this heading">#</a></h3>
<p>We will now show how the same training algorithm can be used in the case of multivariate linear regression, in which
we want map vectors to vectors:</p>
<div class="math notranslate nohighlight">
\[h:\Re^{n} \rightarrow \Re^{m}\]</div>
<p>Multivariate linear regression solves the problem by defining <span class="math notranslate nohighlight">\(m\)</span>
independent multiple regressors <span class="math notranslate nohighlight">\(h_{i}(\mathbf{x})\)</span> which process the
same input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, but are allowed to have different weights:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
y_{1} \\
\ldots \\
y_{m} \\
\end{pmatrix} = \begin{pmatrix}
h_{1}(\mathbf{x}) \\
\ldots \\
h_{m}(\mathbf{x}) \\
\end{pmatrix}\end{split}\]</div>
<p>Each regressor <span class="math notranslate nohighlight">\(h_{i}\)</span> has its own parameters and their optimizations
are carried out independently.</p>
</section>
<section id="non-linear-regression">
<h3><span class="section-number">28.5.4. </span>Non-linear Regression<a class="headerlink" href="#non-linear-regression" title="Permalink to this heading">#</a></h3>
<p>The consideration made above are valid also in the case of other regression algorithms which allow to go beyond the assumption of linearity. Common examples of non-linear regression models are models with interaction terms, quadratic models, and polynomial models. Other approaches to nonlinear regression such as neural networks exist, but they will not be covered in this course.</p>
</section>
</section>
<section id="references">
<h2><span class="section-number">28.6. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">https://en.wikipedia.org/wiki/Empirical_risk_minimization</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../practicals/01_heart_disease_explorative_inferential_analysis.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">27. </span>Exploratory/Inferential Analysis on the Heart Disease Dataset</p>
      </div>
    </a>
    <a class="right-next"
       href="19_classification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">29. </span>Classification Task and Evaluation Measures</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">28.1. Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">28.1.1. Example 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2">28.1.2. Example 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-learning">28.2. Statistical Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">28.2.1. Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">28.2.2. Supervised vs Unsupervised Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-performance-of-a-predictive-model">28.3. Evaluating the Performance of a Predictive Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">28.3.1. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-and-cross-validation">28.3.2. Generalization and Cross-Validation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#holdout-validation-or-single-split">28.3.2.1. Holdout Validation or Single Split</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">28.3.2.2. K-Fold Cross-Validation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation">28.3.2.3. Leave-One-Out Cross-Validation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-and-hyperparmeter-optimization">28.4. Model Selection and Hyperparmeter Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-predictive-models">28.5. Regression Predictive Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-measures">28.5.1. Evaluation Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">28.5.1.1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">28.5.1.2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">28.5.1.3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization">28.5.2. Linear Regression as a Predictive Model learned with Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-regression">28.5.3. Multivariate Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">28.5.4. Non-linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">28.6. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>