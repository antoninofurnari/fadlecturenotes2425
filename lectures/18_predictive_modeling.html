

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>17. Introduction to Predictive Modelling and Regression Models &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/18_predictive_modeling';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="18. Classification Task and Evaluation Measures" href="19_classification.html" />
    <link rel="prev" title="16. Principal Component Analysis" href="17_principal_component_analysis.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Theory</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro_data_analysis.html">1. Introduction to Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_main_data_analysis_concepts.html">2. Main data analysis concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">3. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_misure_di_tendenza_centrale_dispersione_e_forma.html">4. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_associazione_variabili.html">5. Associazione tra Variabili</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_probability.html">6. Probability for Data Manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_common_distributions.html">7. Common Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_information_theory.html">8. Basic Elements of Information Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_statistical_inference.html">9. Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_linear_regression.html">10. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">11. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_causal_analysis.html">12. Causal Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_data_as_nd_points.html">13. Data as N-Dimensional Points</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_clustering.html">14. Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_density_estimation.html">15. Density Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_principal_component_analysis.html">16. Principal Component Analysis</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">17. Introduction to Predictive Modelling and Regression Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_classification.html">18. Classification Task and Evaluation Measures</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_discriminative_models_for_classification.html">19. Discriminative Models for Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="21_generative_models_for_classification.html">20. Generative Models for Classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratories</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">21. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">22. Introduzione a Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">23. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">24. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">25. Introduzione a Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">26. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">27. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/08_associazione_variabili.html">28. Associazione tra Variabili</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/09_heart-disease-analysis.html">29. Exploratory Analysis on the Heart Disease Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/10_statistical_inference.html">30. Laboratory on Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/11_regressione_lineare.html">31. Laboratorio su Regressione Lineare</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/12_regressione_logistica.html">32. Laboratorio su regressione logistica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/13_linear_logistic_regression_example_analysis.html">33. Linear and Logistic Regression Laboratory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/14_clustering_density_estimation_pca.html">34. Clustering, Density Estimation, and Principal Component Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/15_customer_segmentation_analysis.html">35. Customer Segmentation Analysis Example</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/18_predictive_modeling.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/18_predictive_modeling.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/18_predictive_modeling.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to Predictive Modelling and Regression Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">17.1. Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">17.1.1. Example 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2">17.1.2. Example 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-learning">17.2. Statistical Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">17.2.1. Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">17.2.2. Supervised vs Unsupervised Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-performance-of-a-predictive-model">17.3. Evaluating the Performance of a Predictive Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">17.3.1. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-and-cross-validation">17.3.2. Generalization and Cross-Validation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#holdout-validation-or-single-split">17.3.2.1. Holdout Validation or Single Split</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">17.3.2.2. K-Fold Cross-Validation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation">17.3.2.3. Leave-One-Out Cross-Validation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-and-hyperparmeter-optimization">17.4. Model Selection and Hyperparmeter Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-predictive-models">17.5. Regression Predictive Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-measures">17.5.1. Evaluation Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">17.5.1.1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">17.5.1.2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">17.5.1.3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization">17.5.2. Linear Regression as a Predictive Model learned with Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-regression">17.5.3. Multivariate Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">17.5.4. Non-linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-regression-problem">17.6. Example Regression Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#california-housing-dataset">17.6.1. California Housing Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-splitting">17.6.2. Data Splitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-normalization">17.6.3. Data Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regressor">17.6.4. Linear Regressor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">17.6.5. Non-Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regularization">17.6.6. Ridge Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">17.6.7. Lasso Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">17.6.8. Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">17.6.9. Scikit-Learn Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search-with-cross-validation">17.6.10. Grid Search with Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-regression-algorithms">17.6.11. Other Regression Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-and-model-selection">17.6.12. Comparison and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">17.7. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-predictive-modelling-and-regression-models">
<h1><span class="section-number">17. </span>Introduction to Predictive Modelling and Regression Models<a class="headerlink" href="#introduction-to-predictive-modelling-and-regression-models" title="Permalink to this heading">#</a></h1>
<p>Algorithms and techniques seen so far have been focusing on <strong>understanding the data</strong> in different ways. We have seen methods to <strong>summarize the data</strong>, <strong>study the relationship between variables</strong>, including <strong>cause-effect</strong> relationships, <strong>infer properties of the population which the observed data follow</strong>, <strong>study the internal structure of data</strong>, its <strong>density</strong>, and perform <strong>automated feature extraction with Principal Component Analysis</strong>.</p>
<p>We will now look at data analysis from another perspective, the one of <strong>predictive modelling</strong>. The main goal of predictive modelling is to <strong>build models which can make be used to make predictions on unseen data</strong>. Predictive modelling is a broad field of data analysis, which is very overlapped with <strong>machine learning</strong>.</p>
<p>It is worth mentioning that <strong>predictive modelling is a shift in thinking with respect to the approaches seen so far</strong>. For instance, we have seen how a <strong>linear regressor attempts to predict <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span></strong>. This is a form of predictive modelling, but the goal so far has been to understand the relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, rather than making accurate predictions. Indeed, we have focused on <strong>interpreting the coefficients</strong> and making <strong>hypothesis tests to check if our conclusions would extend to the population as well</strong>. We have also seen how a <strong>simpler model with inferior predictive abilities can sometimes be preferred to a more complex model due to its interpretability</strong>.</p>
<p>The goal of predictive modelling is to <strong>create data models that can support decision making</strong>. Examples of uses of such models are:</p>
<ul class="simple">
<li><p>Determining the probability of a given patient <strong>developing a given pathology from a blood test</strong>;</p></li>
<li><p>Determining if a given bank transaction is <strong>a fraudulent one</strong> (anomaly detection);</p></li>
<li><p>Predicting the <strong>future value of a company stock</strong>;</p></li>
<li><p>Estimating the <strong>energy produced by a solar plant based on weather forecast</strong>;</p></li>
<li><p>Predict whether an <strong>autonomous vehicle should turn left or right given sensor readings and an RGB image observation of the scene</strong>.</p></li>
<li><p>Detect the <strong>presence of people from surveillance cameras</strong>.</p></li>
</ul>
<p>In these cases, we aim to obtain <strong>accurate systems</strong>, but we are less interested in their interpretability. Indeed, while being able to interpret such models is certainly a plus, we will often <strong>prefer more complex but accurate models</strong> even if we loose the ability to interpret them.</p>
<section id="problem-formulation">
<h2><span class="section-number">17.1. </span>Problem Formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this heading">#</a></h2>
<p>We will give a general formulation of predictive modelling problems. Later, we will see instantiations of this formulation to specific predictive modeling problems.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> be two spaces of objects. We can see them as the sample spaces of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, with <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> being two realizations of the random variables: <span class="math notranslate nohighlight">\(X=x\)</span> and <span class="math notranslate nohighlight">\(Y=y\)</span>. The goal of predictive modeling is to find a function:</p>
<div class="math notranslate nohighlight">
\[h : \mathcal{X} \to \mathcal{Y}\]</div>
<p>which outputs an object <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span>, given an object <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>. We often use the symbol <span class="math notranslate nohighlight">\(y\)</span> to refer to the <strong>true value of <span class="math notranslate nohighlight">\(Y=y\)</span> associated to the observation <span class="math notranslate nohighlight">\(X=x\)</span></strong>, while <strong><span class="math notranslate nohighlight">\(\hat y\)</span> will represent the prediction obtained using function <span class="math notranslate nohighlight">\(h\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[\hat y = h(x)\]</div>
<p>The true value <span class="math notranslate nohighlight">\(y\)</span> is often called <strong>ground truth</strong>. The function <span class="math notranslate nohighlight">\(h\)</span> is usually a <strong>data model</strong> and often referred to as <strong>an hypothesis</strong>.</p>
<section id="example-1">
<h3><span class="section-number">17.1.1. </span>Example 1<a class="headerlink" href="#example-1" title="Permalink to this heading">#</a></h3>
<p>As a simple example, we can imagine <span class="math notranslate nohighlight">\(\mathcal{X}=\Re^m\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0,1\}\)</span>, with <span class="math notranslate nohighlight">\(\mathbf{x} \in \Re^m\)</span> being a numerical vector representing the results of different blood tests made on a given subject, while <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span> being a response variable indicating whether the given subject has (<span class="math notranslate nohighlight">\(1\)</span>) or does not have (<span class="math notranslate nohighlight">\(0\)</span>) a given disease.</p>
<p>Finding an appropriate function <span class="math notranslate nohighlight">\(h\)</span> will allow us to predict if a given subject has a given disease from the blood test <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat y = h(\mathbf{x})\]</div>
<p>If we obtain <span class="math notranslate nohighlight">\(\hat y = 1\)</span>, then we will assume that the given person has the disease. While this example is very abstract, we will later see algorithms which assign a specific form to <span class="math notranslate nohighlight">\(h\)</span>.</p>
</section>
<section id="example-2">
<h3><span class="section-number">17.1.2. </span>Example 2<a class="headerlink" href="#example-2" title="Permalink to this heading">#</a></h3>
<p>We want to build a <strong>spam filter</strong> which decides whether a given email is spam or not. In this case, we can consider <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as the <strong>set of all possible emails</strong> and <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0,1\}\)</span>, with <span class="math notranslate nohighlight">\(1\)</span> indicating spam and <span class="math notranslate nohighlight">\(0\)</span> indicating non-spam. Let <span class="math notranslate nohighlight">\(f\)</span> be a function counting the number of orthographical mistakes of a given email <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[f : \mathcal{X} \to R^+\]</div>
<p>We can determine if a given email <span class="math notranslate nohighlight">\(x\)</span> is spam or not by <strong>counting the number of mistakes and checking whether this number exceeds a pre-determined threshold <span class="math notranslate nohighlight">\(\theta\)</span></strong>. We can hence define our function <span class="math notranslate nohighlight">\(h\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x) = \begin{cases} 1 &amp; \text{ if } f(x)&gt;\theta \\ 0 &amp; \text{otherwise} \end{cases}\end{split}\]</div>
<p>As we will later see, this is a <strong>trivial example of a parametric classifier with one parameter</strong>.</p>
</section>
</section>
<section id="statistical-learning">
<h2><span class="section-number">17.2. </span>Statistical Learning<a class="headerlink" href="#statistical-learning" title="Permalink to this heading">#</a></h2>
<p>While there are different ways to define the function <span class="math notranslate nohighlight">\(h\)</span>, the model is often obtained following an approach called <strong>statistical learning</strong>. According to the statistical learning approach, we will assume to have at our disposal a TRaining set (TR) of <span class="math notranslate nohighlight">\(N\)</span> examples:</p>
<div class="math notranslate nohighlight">
\[\text{TR}=\{(x_i,y_i)\}_{i=1}^N\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i \in \mathcal{X}\)</span> is the input object and <span class="math notranslate nohighlight">\(y_i \in \mathcal{Y}\)</span> is the corresponding response that we aim to obtain with <span class="math notranslate nohighlight">\(h(x_i)\)</span>. <strong>The term “training set” is borrowed from the machine learning terminology, where a model is “trained” on a given set of data to improve its performance in tackling a given task</strong>.</p>
<p>More specifically, we will assume that <strong>the examples are independent and identically distributed (i.i.d.)</strong>, following the joint probability <span class="math notranslate nohighlight">\(P(X,Y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[(x,y) \sim P(X,Y)\]</div>
<p>The i.i.d. hypothesis is reasonable and, as we will see later, very useful. Moreover, the assumption that the data follows the joint probability, <strong>allows us to reason in probabilistic terms, modeling the uncertainty in predictions</strong>. Hence, the predicted values will be realization of a random variable following the <strong>conditional distribution</strong> P(y|x):</p>
<div class="math notranslate nohighlight">
\[\hat y = h(x) \text{, with } \hat y \sim P(y|x)\]</div>
<p>To find the appropriate function <span class="math notranslate nohighlight">\(h\)</span>, we will define (depending on the problem) a <strong>non-negative, real-valued loss (or cost) function</strong> <span class="math notranslate nohighlight">\(L(\hat y, y)\)</span> which measures <strong>the loss, or cost</strong> of predicting <span class="math notranslate nohighlight">\(\hat y\)</span> when the true value is <span class="math notranslate nohighlight">\(y\)</span>. If the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> is very similar to the real value <span class="math notranslate nohighlight">\(y\)</span>, then we will obtain a small cost (or loss value) for <span class="math notranslate nohighlight">\(L(\hat y, y)\)</span>. In practice, the form of <span class="math notranslate nohighlight">\(L\)</span> will vary depending on the problem and model at hand.</p>
<p>We will define <strong>the risk <span class="math notranslate nohighlight">\(R(h)\)</span> of a given hypothesis function <span class="math notranslate nohighlight">\(h\)</span></strong> as the <strong>expected loss value under the joint probability distribution</strong>:</p>
<div class="math notranslate nohighlight">
\[R(h) = E_{(x,y) \sim P(X,Y)}[L(h(x),y)]\]</div>
<p><strong>Statistical learning aims to find the optimal <span class="math notranslate nohighlight">\(h^*\)</span> from a fixed class of functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> by solving the following optimization problem</strong>:</p>
<div class="math notranslate nohighlight">
\[h^* = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R(h)\]</div>
<p>Borrowing from the machine learning terminology, we will say that a model as a <strong>large capacity if the number of the associated functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is large</strong>.</p>
<section id="empirical-risk-minimization">
<h3><span class="section-number">17.2.1. </span>Empirical Risk Minimization<a class="headerlink" href="#empirical-risk-minimization" title="Permalink to this heading">#</a></h3>
<p>In general, the risk <span class="math notranslate nohighlight">\(R(h)\)</span> cannot be computed because the joint distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span> is unknown. However, <strong>by the law of large numbers</strong>, we can estimate it empirically using the training set TR:</p>
<div class="math notranslate nohighlight">
\[R_{emp}(h) = \frac{1}{N}\sum_{i=1}^N L(h(x_i),y_i)\]</div>
<p>The <strong>empirical risk minimization principle states that the we should choose a hypothesis <span class="math notranslate nohighlight">\(\hat h\)</span> from the class of hypothesis functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> which minimizes the empirical risk</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)\]</div>
<p>We thus define a <strong>learning algorithm</strong> (i.e., an algorithm used to “learn” the model <span class="math notranslate nohighlight">\(\hat h\)</span> from the training set TR) which solves the above defined optimization problem.</p>
</section>
<section id="supervised-vs-unsupervised-learning">
<h3><span class="section-number">17.2.2. </span>Supervised vs Unsupervised Learning<a class="headerlink" href="#supervised-vs-unsupervised-learning" title="Permalink to this heading">#</a></h3>
<p>In the machine learning terminology, predictive models as just defined are also referred to as <strong>supervised learning algorithms</strong>, where the world “supervised” indicates that we are given input objects <span class="math notranslate nohighlight">\(x\)</span> and expected (or desired) out values <span class="math notranslate nohighlight">\(y\)</span>. This is in contrast with <strong>unsupervised</strong> algorithms, in which we are only given the <span class="math notranslate nohighlight">\(x\)</span> values. Examples of such algorithms are the Principal Component Analysis and Clustering Algorithms.</p>
</section>
</section>
<section id="evaluating-the-performance-of-a-predictive-model">
<h2><span class="section-number">17.3. </span>Evaluating the Performance of a Predictive Model<a class="headerlink" href="#evaluating-the-performance-of-a-predictive-model" title="Permalink to this heading">#</a></h2>
<p>Once we choose a model <span class="math notranslate nohighlight">\(\hat h\)</span> minimizing the empirical risk computed on the training set, we need to <strong>evaluate the performance of the model</strong>. This is a fundamental step, as we want to know <strong>how well will the model do “in the wild”, beyond the training set</strong>.</p>
<section id="overfitting">
<h3><span class="section-number">17.3.1. </span>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this heading">#</a></h3>
<p>One may think that a model minimizing the empirical risk will be the best possible one. However, especially <strong>when the training set is not large enough (as compared to the complexity of the model) or not representative enough of the joint probability distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span></strong>, the estimation of the empirical risk may be limited. As an example, consider the following model:</p>
<div class="math notranslate nohighlight">
\[\hat h(x) = y \text{ s.t. } (x,y) \in \text{TR}\]</div>
<p>The hypothesis <span class="math notranslate nohighlight">\(\hat h\)</span> defined above will lead to an empirical risk equal to <span class="math notranslate nohighlight">\(0\)</span> as long as the loss function is defined in such a way that <span class="math notranslate nohighlight">\(L(y,y)=0\)</span>. Indeed:</p>
<div class="math notranslate nohighlight">
\[R_{emp}(\hat (x)) = \frac{1}{N}\sum_{i=1}^N L(y,y) = 0\]</div>
<p>However, the function above will not be defined for any value:</p>
<div class="math notranslate nohighlight">
\[x' \in \mathcal{X} \text{ s.t. } (x',y) \notin \text{ TR } \forall y \in \mathcal{Y}\]</div>
<p>Since <strong>we expect the training set to be a sample from the distribution <span class="math notranslate nohighlight">\(P(X,Y)\)</span>, we expect such values <span class="math notranslate nohighlight">\(x'\)</span> to exist</strong> (otherwise the training <em>is</em> the population, which is in general not true).</p>
<p>The example above is an extreme case of a phenomenon called <strong>overfitting</strong>, which happens when <strong>a given hypothesis achieves a low error on the training set, but the solution is not general enough to obtain similar performance (i.e., a similar risk) on data not belonging to the training set</strong>.</p>
<p>The phenomenon of overfitting is often related to the <strong>capacity of the model</strong>. A model with a <strong>large capacity</strong> can represent very complex functions <span class="math notranslate nohighlight">\(h\)</span> and result in overfitting, while a model is a <strong>small capacity</strong> can represent relatively simple functions <span class="math notranslate nohighlight">\(h\)</span>, making overfitting harder, but possibly resulting in an <strong>underfitting model</strong>, i.e., a model which is too simple, and as a result cannot reach a low empirical risk.</p>
<p>This is best seen with the simple regression example below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d5daacb1d4dbb45a484458331f314f0a202a9ff1e3e7c609ae9d931266fdb8c9.png" src="../_images/d5daacb1d4dbb45a484458331f314f0a202a9ff1e3e7c609ae9d931266fdb8c9.png" />
</div>
</div>
<p>The example shows the fit of three polynomial regression models of different degrees (the first one is degree 1, which corresponds to a linear regressor). As can be noted:</p>
<ul class="simple">
<li><p>A lower degree polynomial (e.g., a line - left) will lead to <strong>underfitting</strong>. In this case, the model is too simple to model the data well, so the empirical risk will be large;</p></li>
<li><p>A higher degree polynomial (right) will minimize the empirical risk but find a complex model which will not describe well data which has not been seen at training time. This is a case of <strong>overfitting</strong>. The model is trying to model the idiosyncrasies of the training set (which can be noisy) finding a solution which will work only on the data at hand;</p></li>
<li><p>An appropriate degree (center) will lead to a model with an appropriate capacity. The empirical risk is minimized and the solution also works with unseen data.</p></li>
</ul>
<blockquote>
<div><p>Note that this is another instance of the <strong>bias-variance</strong> tradeoff. Complex models (right) have a large variance and a large bias: small variations of the training set are modeled and can lead to wrong solutions. Models that are too simple  (left) have a low variance, but can still have a large bias (the model is too simple and the solution is not good). Choosing an appropriate capacity (in this case by choosing an appropriate polynomial degree) leads to a good trade-off between variance and bias.</p>
</div></blockquote>
<p><strong>Regularization and Bias-Variance tradeoff</strong>
In practice, there are different ways to reduce the model capacity. In the example above, the degree of the polynomial has been used as an hyperparameter to reduce the capacity of the model. A different approach would be to use regularization techniques. The plot below shows the result of fitting a polynomial regressor on the same data when Ridge regression is used:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/004f153413b125255aa08a2ced079b7e47672757fe5adfa5ab161fe1ebdc0b8f.png" src="../_images/004f153413b125255aa08a2ced079b7e47672757fe5adfa5ab161fe1ebdc0b8f.png" />
</div>
</div>
<p>Note that even a polynomial of degree equal to <span class="math notranslate nohighlight">\(50\)</span> (right) achieves reasonable results now.</p>
</section>
<section id="generalization-and-cross-validation">
<h3><span class="section-number">17.3.2. </span>Generalization and Cross-Validation<a class="headerlink" href="#generalization-and-cross-validation" title="Permalink to this heading">#</a></h3>
<p>The example above has shown how it is possible to obtain models that work well on the training data but do not generalize to unseen data, while ideally we would want models which work well on the test data as well. We call the ability of the model to obtain a similar error (e.g., the empirical risk)  on both the training set and unseen data <strong>generalization</strong>.</p>
<p>To measure the ability of a model to generalize, we could just take the model and use it “<strong>in the wild</strong>”. However, since we want our tests to be repeatable, we instead resort to a series of techniques which <strong>consist in using a part of our dataset as a training set and different part of the dataset as a test set</strong>. These techniques are generally referred to as <strong>cross validation</strong>. In the following, we see the main approaches to cross validation.</p>
<p>However, before to proceed, we should point out that, while we could evaluate the performance of a model with the <strong>empirical risk</strong>, in practice, it is common to use <strong>performance measures</strong> (the higher the better) or <strong>error measures</strong> (the lower the better) which may be different from the <strong>loss function chosen to train the algorithm</strong>. This is due to the fact that loss functions often need to have some properties to facilitate learning, so they can represent an <strong>approximation of or a deviation from our true objective measure of performance</strong>. We will see more in details the main performance measures later, but for the moment we will note that we will evaluate models using a given performance measure:</p>
<div class="math notranslate nohighlight">
\[p(\hat Y, Y) : \mathcal{Y}^N \times \mathcal{Y}^N \to \Re\]</div>
<p>where <span class="math notranslate nohighlight">\(Y=\{y_i | (x_i,y_i) \in S \}_{i=1}^N\)</span> is a set of ground truth values from a set of data <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(\hat Y = \{h(x_i) | (x_i,y_i) \in S \}_{i=1}^N\)</span> is the set of corresponding predictions. Note that, given the definition above, the empirical risk can be seen as a performance measure, but we need not restrict to the empirical risk to evaluate predictive models.</p>
<section id="holdout-validation-or-single-split">
<h4><span class="section-number">17.3.2.1. </span>Holdout Validation or Single Split<a class="headerlink" href="#holdout-validation-or-single-split" title="Permalink to this heading">#</a></h4>
<p>The simplest form of cross-validation is the holdout test. In this case, the initial set of data is split into two different sets: a training set, which will be used to optimize the model (e.g., with empirical risk minimization) and a test set, which is used to evaluate the performance of the model. The act of optimizing the model on the training set is often called <strong>training</strong>, while the act of evaluating the performance of the model on the test set is called <strong>testing</strong>. This approach <strong>pretends that the test data is not available at training time and only uses it to evaluate performance</strong>.</p>
<p>The <strong>rule number one</strong> when using this technique is <strong>to avoid in any way to choose any characteristic of the model based on the training data</strong>. Indeed, if we did so, we could end up in some form of unmeasurable overfitting. To make sure that both the training and test set are i.i.d., before the split, the data is <strong>randomly shuffled</strong>.</p>
<p>Also, since training data is usually precious and datasets are often not large enough, it is common to split the data asymmetrically, choosing <span class="math notranslate nohighlight">\(70-80\%\)</span> of the data for training and <span class="math notranslate nohighlight">\(20-30\%\)</span> of the data for testing.</p>
<p>The figure below illustrates the splitting process:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/904debe02349b0102836dd0a1893a7dbfa1596960185f043b1c13976904724b9.png" src="../_images/904debe02349b0102836dd0a1893a7dbfa1596960185f043b1c13976904724b9.png" />
</div>
</div>
</section>
<section id="k-fold-cross-validation">
<h4><span class="section-number">17.3.2.2. </span>K-Fold Cross-Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this heading">#</a></h4>
<p>When the dataset is very small, we may not want to sacrifice a part of it for testing only as this may lead to a biased model. Also, we should note that a small test set could lead to a biased estimation of the model performance. In these cases, rather than randomly splitting the data into two parts, we randomly split it into <span class="math notranslate nohighlight">\(K\)</span> different parts, which will be called <strong>folds</strong>.</p>
<p>We then perform training and testing <span class="math notranslate nohighlight">\(K\)</span> times, each time using fold <span class="math notranslate nohighlight">\(i\)</span> as the test set, and the remaining folds as the training set. The final model performance is obtained by averaging the performance scores computed in each iteration. Note that the obtained performance is unbiased, as each number in the final average is computed on data which has not been seen during training.</p>
<p>The figure below illustrates the case of a 4-fold cross-validation.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/06fcb8d0ce0bbaae070c5a2d454d700cedffb4be6c2457ad0eea4b02b6cf495f.png" src="../_images/06fcb8d0ce0bbaae070c5a2d454d700cedffb4be6c2457ad0eea4b02b6cf495f.png" />
</div>
</div>
<p>The K-Fold cross validation has the advantage of allowing to validate the model without throwing away a significant part of the data, but it is practically feasible only when the training procedure is not too computationally expensive. Indeed, if training a model takes one week, a K-Fold validation will typically require four weeks.</p>
</section>
<section id="leave-one-out-cross-validation">
<h4><span class="section-number">17.3.2.3. </span>Leave-One-Out Cross-Validation<a class="headerlink" href="#leave-one-out-cross-validation" title="Permalink to this heading">#</a></h4>
<p>In leave-one-out cross-validation, the validation stage is performed in <span class="math notranslate nohighlight">\(N\)</span> iterations, where <span class="math notranslate nohighlight">\(N\)</span> is the number of elements in the dataset. At the <span class="math notranslate nohighlight">\(i^{th}\)</span> iteration, the model is trained on all data points except <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> and tested on <span class="math notranslate nohighlight">\((x_i,y_i)\)</span>. The final performance is obtained by averaging the performance scores obtained at each iterations. Note that this is the same as K-Fold cross-validation with <span class="math notranslate nohighlight">\(K=N\)</span>. The figure below illustrates the process:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e0dea6eee59bd3b9d724273fcf072c1df6940ac006597eb865944124e29fcd07.png" src="../_images/e0dea6eee59bd3b9d724273fcf072c1df6940ac006597eb865944124e29fcd07.png" />
</div>
</div>
<p>This approach is useful when the dataset is extremely small, but, similarly to K-Fold cross-validation, it can increase the computation time by a large margin.</p>
</section>
</section>
</section>
<section id="model-selection-and-hyperparmeter-optimization">
<h2><span class="section-number">17.4. </span>Model Selection and Hyperparmeter Optimization<a class="headerlink" href="#model-selection-and-hyperparmeter-optimization" title="Permalink to this heading">#</a></h2>
<p>Many algorithms have some parameters which are not explicitly part of the final model <span class="math notranslate nohighlight">\(h\)</span>, but they need to be set in order to solve the optimization problem. <strong>For example, in a ridge or lasso regressor, the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is used to control the amount of regularization during the learning process, however, the final parameter is not part of the model and is not automatically found during the optimization process</strong>.</p>
<p>Hyperparameters are usually found using grid searches: we train models using different values of the hyperparamters and choose the model which performs best. However, <strong>caution should be taken when hyperparameters are selected with grid search</strong>.</p>
<p>Recall that <strong>we are not allowed to make any choice on the final model using the test set</strong>. Indeed, if we did so, we may incur in a form of overfitting which we would not be able to measure. For instance, we can choose a given parameter which works well only for that specific test set. How can we be sure that performance will be good when new data is analyzed?</p>
<p>To avoid this problem, we should work with three different sets of data: a <strong>training set</strong>, a <strong>validation set</strong>, and a <strong>test set</strong>. We will use the training set to train the model, the validation set to choose the hyperparameters and the test set to test the final performance. This is done by training different models on the test set, choosing the hyperparamter values leading to best values on the validation set, and then re-training the model on the training set (or on the union of training and validation set) to final test on the test set.</p>
<p>The most common scheme is to use a fixed split with a <span class="math notranslate nohighlight">\(60:20:20\)</span> ratio, as shown in the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1e5c54bb346ddb1e36503f523b6424a632178fe37f884642bf7ad57e45cb4720.png" src="../_images/1e5c54bb346ddb1e36503f523b6424a632178fe37f884642bf7ad57e45cb4720.png" />
</div>
</div>
<p>However, other combinations are possible. An approach which often used when the dataset is small is a follows:</p>
<ul class="simple">
<li><p>The dataset is split into a training and test set;</p></li>
<li><p>Hyperparameters are optimized using cross-validation on the training set - this consists in executing different cross-validations with different hyperparameter values - then the parameters achieving the best performance are chosen;</p></li>
<li><p>Once the best hyperparameters are found, the model is re-trained on the full training set and tested on the test set.</p></li>
</ul>
<p>This is illustrated in the following figure:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/bf56183ef633c49b5e2fc09bc6b97ca58263f6a4339c6ed9141bf9d936231140.png" src="../_images/bf56183ef633c49b5e2fc09bc6b97ca58263f6a4339c6ed9141bf9d936231140.png" />
</div>
</div>
<p>Again, if the dataset is large enough and the training procedure is computationally expensive, it is common to use a fixed split as illustrated above.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library allows to easily perform hyperparameter search using cross-validation for different algorithms.</p>
<p>More in general, these techniques can be used to compare different models and select the best performing one.</p>
</section>
<section id="regression-predictive-models">
<h2><span class="section-number">17.5. </span>Regression Predictive Models<a class="headerlink" href="#regression-predictive-models" title="Permalink to this heading">#</a></h2>
<p>We will now investigate an important family of predictive models: regression models. While we have already seen regression models as approaches to understand relationships within the data, in this context we are interested in defining good hypothesis functions <span class="math notranslate nohighlight">\(\hat h\)</span> able to predict continuous values <span class="math notranslate nohighlight">\(\hat y\)</span> from continuous inputs <span class="math notranslate nohighlight">\(x\)</span>. We can see the defined hypothesis function <span class="math notranslate nohighlight">\(\hat h\)</span> as <strong>an approximation of the true function <span class="math notranslate nohighlight">\(f\)</span> generating <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(y=f(x)\)</span></strong>. We will say that we are tackling a <strong>regression problem</strong> whenever the hypothesis function takes the form:</p>
<div class="math notranslate nohighlight">
\[h:\Re^{n} \rightarrow \Re^{m}\]</div>
<p>Note that we have seen the case in which <span class="math notranslate nohighlight">\(n=m=1\)</span> (simple regression) and the case in which <span class="math notranslate nohighlight">\(n&gt;1, m=1\)</span> (multiple regression). When both <span class="math notranslate nohighlight">\(n&gt;1, m&gt;1\)</span>, both the inputs and outputs of the regression model will be vectors of continuous values. We will see how linear regression models can be easily generalized to work in this case as well, but for the moment we will consider the form above as the general form of a regression model.</p>
<section id="evaluation-measures">
<h3><span class="section-number">17.5.1. </span>Evaluation Measures<a class="headerlink" href="#evaluation-measures" title="Permalink to this heading">#</a></h3>
<p>As noted, independently from the employed loss functions, we need evaluation measures to assess the performance of the trained models. Common evaluation measures for regression problems are discussed in the following.</p>
<section id="mean-squared-error-mse">
<h4><span class="section-number">17.5.1.1. </span>Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Permalink to this heading">#</a></h4>
<p>Consider a ground truth label <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathfrak{R}^{m}\)</span> and a
predicted label  <span class="math notranslate nohighlight">\(\widehat{\mathbf{y}} \in \mathfrak{R}^{m}\)</span>. Since both
values are m-dimensional vectors, a natural way to measure if
 <span class="math notranslate nohighlight">\(\widehat{\mathbf{y}}\)</span> is a good approximation of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is to
simply measure their Euclidean distance:</p>
<div class="math notranslate nohighlight">
\[\left\| \widehat{\mathbf{y}} - \mathbf{y} \right\|_{2} = \sqrt{\sum_{i = 1}^{m}\left( {\widehat{y}}_{i} - y_{i} \right)^{2}}\]</div>
<p>In practice, <strong>we often use the squared Euclidean distance to penalize
more large errors</strong>:</p>
<div class="math notranslate nohighlight">
\[error\left( \widehat{\mathbf{y}},\mathbf{y} \right) = \left\| \widehat{\mathbf{y}} - \mathbf{y} \right\|_{2}^{2} = \sum_{i = 1}^{m}\left( {\widehat{y}}_{i} - y_{i} \right)^{2}\]</div>
<p>We can compute the average error over the whole test set to obtain a
performance estimator, which is usually called Mean Squared Error (MSE):</p>
<div class="math notranslate nohighlight">
\[MSE\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{1}{|TE|}\sum_{j = 1}^{|TE|}\left\| {\widehat{\mathbf{y}}}^{(j)} - \mathbf{y}^{(j)} \right\|_{2}^{2}\]</div>
<p>Where <span class="math notranslate nohighlight">\(TE\)</span> denotes the TEst set.</p>
<p>This performance measure is an <strong>error measure</strong>. A good
regressor will obtain a <strong>small error.</strong></p>
<p>Note that the MSE on the test set is equal to the RSS divided by the number of test data points <span class="math notranslate nohighlight">\(|TE|\)</span>.</p>
</section>
<section id="root-mean-squared-error-rmse">
<h4><span class="section-number">17.5.1.2. </span>Root Mean Squared Error (RMSE)<a class="headerlink" href="#root-mean-squared-error-rmse" title="Permalink to this heading">#</a></h4>
<p>The unit of measure of the MSE is the squared unit of measure of the
dependent variable (target value). In practice, if <span class="math notranslate nohighlight">\(y\)</span> is measured in
meters, then the MSE is measures in square meters. This is not very
intuitive. To obtain an error which can be measured in meters, we can
take the squared root of the MSE and obtain the Root Mean Square Error
(RMSE):</p>
<div class="math notranslate nohighlight">
\[RMSE\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \sqrt{\frac{1}{|TE|}\sum_{j = 1}^{|TE|}\left\| {\widehat{\mathbf{y}}}^{(j)} - \mathbf{y}^{(j)} \right\|_{2}^{2}}\]</div>
</section>
<section id="mean-absolute-error-mae">
<h4><span class="section-number">17.5.1.3. </span>Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this heading">#</a></h4>
<p><em>If the target values are scalars</em> (hence <span class="math notranslate nohighlight">\(m = 1\)</span>), another possible
measure which has the same unit of measurement as the target variable is
the MAE, which is simply the average of the absolute errors:</p>
<div class="math notranslate nohighlight">
\[MAE\left( Y,\widehat{Y} \right) = \frac{1}{|TE|}\sum_{i}^{}{|y_{i} - \widehat{y_{i}}|}\]</div>
<p>The main difference between the RMSE/MSE and MAE, is that RMSE/MSE tend
to give less importance to small errors, while giving more importance to
large errors. This comes from the observation that the squared of a
number smaller than one is even smaller than the original number,
whereas the squared of a large number is even larger. On the contrary,
MAE is often more intuitive (it is the average error we make when
estimating y).</p>
</section>
</section>
<section id="linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization">
<h3><span class="section-number">17.5.2. </span>Linear Regression as a Predictive Model learned with Empirical Risk Minimization<a class="headerlink" href="#linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization" title="Permalink to this heading">#</a></h3>
<p>We can see linear regression as a form of predictive modeling. Note that, <strong>when looking at the linear regressor as a predictive model, we are more interested in its accuracy than in its interpretability</strong>. In this context:</p>
<ul class="simple">
<li><p>We will assume <span class="math notranslate nohighlight">\(\mathcal{X}= \Re^n\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y}= \Re\)</span></p></li>
<li><p>The hypothesis function will take the form:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[h(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n\]</div>
<ul class="simple">
<li><p>We will call our dataset the <strong>training set</strong>;</p></li>
<li><p>We will define our loss function as the error function: <span class="math notranslate nohighlight">\(L(h(x),y) = (h(x)-y)^2\)</span>;</p></li>
</ul>
<p>In this case the empirical risk is defined as follows:</p>
<div class="math notranslate nohighlight">
\[R_{emp} = \frac{1}{N} \sum_{i=1}^N (h(x)-y)^2 = \frac{1}{N} RSS\]</div>
<p>Note that, since:</p>
<div class="math notranslate nohighlight">
\[\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h) = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ \frac{1}{N} \sum_{i=1}^N (h(x)-y)^2 = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ \frac{1}{N} RSS =  \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ RSS\]</div>
<p>we can solve the optimization problem with <strong>Ordinary Least Squares (OLS)</strong> (the same method we used previously to minimize RSS).</p>
<p>From a <strong>learning perspective</strong>, solving the optimization problem <span class="math notranslate nohighlight">\(\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)\)</span> corresponds to <strong>finding the optimal set of parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> minimizing the empirical risk</strong>, which corresponds to minimizing the Residual Sum of Squares, as previously defined.</p>
<p>It is worth to emphasize that we are now interested in minimizing the empirical risk as much as possible, even if the model loses interpretability. Hence, <strong>we can normalize the data, use many variables, include interaction terms, use polynomial regression, regularize with lasso and ridge, as long as it improves the performance of the model</strong>.</p>
</section>
<section id="multivariate-linear-regression">
<h3><span class="section-number">17.5.3. </span>Multivariate Linear Regression<a class="headerlink" href="#multivariate-linear-regression" title="Permalink to this heading">#</a></h3>
<p>We will now show how the same training algorithm can be used in the case of multivariate linear regression, in which
we want map vectors to vectors:</p>
<div class="math notranslate nohighlight">
\[h:\Re^{n} \rightarrow \Re^{m}\]</div>
<p>Multivariate linear regression solves the problem by defining <span class="math notranslate nohighlight">\(m\)</span>
independent multiple regressors <span class="math notranslate nohighlight">\(h_{i}(\mathbf{x})\)</span> which process the
same input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, but are allowed to have different weights:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
y_{1} \\
\ldots \\
y_{m} \\
\end{pmatrix} = \begin{pmatrix}
h_{1}(\mathbf{x}) \\
\ldots \\
h_{m}(\mathbf{x}) \\
\end{pmatrix}\end{split}\]</div>
<p>Each regressor <span class="math notranslate nohighlight">\(h_{i}\)</span> has its own parameters and their optimizations
are carried out independently.</p>
</section>
<section id="non-linear-regression">
<h3><span class="section-number">17.5.4. </span>Non-linear Regression<a class="headerlink" href="#non-linear-regression" title="Permalink to this heading">#</a></h3>
<p>The consideration made above are valid also in the case of other regression algorithms which allow to go beyond the assumption of linearity. Common examples of non-linear regression models are models with interaction terms, quadratic models, and polynomial models. Other approaches to nonlinear regression such as neural networks exist, but they will not be covered in this course.</p>
</section>
</section>
<section id="example-regression-problem">
<h2><span class="section-number">17.6. </span>Example Regression Problem<a class="headerlink" href="#example-regression-problem" title="Permalink to this heading">#</a></h2>
<p>We will now see an example in which we try to address a regression problem. We will use all best practices to select the model with best performance.</p>
<section id="california-housing-dataset">
<h3><span class="section-number">17.6.1. </span>California Housing Dataset<a class="headerlink" href="#california-housing-dataset" title="Permalink to this heading">#</a></h3>
<p>We’ll use the California Housing dataset provided by the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library. Let us load the data as a dataframe and have a look at the data description:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;DESCR&#39;</span><span class="p">])</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block group
        - HouseAge      median house age in block group
        - AveRooms      average number of rooms per household
        - AveBedrms     average number of bedrooms per household
        - Population    block group population
        - AveOccup      average number of household members
        - Latitude      block group latitude
        - Longitude     block group longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20635</th>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
    </tr>
  </tbody>
</table>
<p>20640 rows × 8 columns</p>
</div></div></div>
</div>
<p>The dataset contains <span class="math notranslate nohighlight">\(8\)</span> variables. The independent variable is <code class="docutils literal notranslate"><span class="pre">MedInc</span></code>, the average value of houses in a given suburb, while all other variables are independent. For our aims, we will treat the data as a matrix of numerical variables. We could easily convert the dataframe in this format, but scikit-learn allows to load the data directly in this format:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let us load the data without passing as_frame=True</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="c1"># the features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span> <span class="c1"># the targets</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(20640, 8) (20640,)
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-splitting">
<h3><span class="section-number">17.6.2. </span>Data Splitting<a class="headerlink" href="#data-splitting" title="Permalink to this heading">#</a></h3>
<p>We will split the dataset into a training, a validation and a test set using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># We&#39;ll do a 60:20:20 split</span>
<span class="n">val_prop</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># We&#39;ll split the data in two steps - first let&#39;s create a test set and a combined trainval set</span>
<span class="n">X_trainval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># We&#39;ll now split the combined trainval into train and val set with the chosen proportions</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">test_prop</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Let us check shapes and proportions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12384, 8) (4128, 8) (4128, 8)
0.6 0.2 0.2
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function will split the data randomly. We are passing a fixed <code class="docutils literal notranslate"><span class="pre">random_state</span></code> to be able to replicate the results, but, in general, we should avoid that if we want the split to be truly random (though it is common to use random seeds for splitting in research). Note that, while the split is random, the function makes sure that the i-th element of the y variable corresponds to the i-th element of the X variable after the split.</p>
<p>We will now reason mainly on the validation set, comparing different models and parameter configurations. Once we are done with our explorations, we’ll check the final results on the test set.</p>
</section>
<section id="data-normalization">
<h3><span class="section-number">17.6.3. </span>Data Normalization<a class="headerlink" href="#data-normalization" title="Permalink to this heading">#</a></h3>
<p>We’ll start by normalizing the data with z-scoring. This will prove useful later when we use certain algorithms (e.g., regularization). Note that we have not normalized data before because we need to <strong>make sure that even mean and standard deviation parameters are not computed on the validation or test set</strong>. While this may seem a trivial detail, it is important to follow this rule as strictly as possible to avoid bias. We can normalize the data with the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># tunes the internal parameters of the standard scaler</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># does not tune the parameters anymore</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Scikit-learn objects have a unified object-oriented interface. Each algorithm is an object (e.g., <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>) with standard methods, such as:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to tune the internal parameters of the algorithm. In this case, it is a vector of means and a vector of standard deviations, but in the case of a linear regression it will be a vector of weights;</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">transform</span></code> method to transform the data. Note that in this stage no parameters are tuned, so we can safely apply this method to validation and test data. This method only applies to objects which transform the data, such as the standard scaler;</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">predict</span></code> method to obtain predictions. This applies only to predictive models, such as a linear regressor;</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">score</span></code> method to obtain a standard performance measure on the test or validation data. Also this only applies to predictive models.</p></li>
</ul>
<p>We will see examples of the last two methods later.</p>
</section>
<section id="linear-regressor">
<h3><span class="section-number">17.6.4. </span>Linear Regressor<a class="headerlink" href="#linear-regressor" title="Permalink to this heading">#</a></h3>
<p>We will start by training a linear regressor. We will use scikit-learn’s implementation which does not provide statistical details (e.g., p-values) but is optimized for predictive modeling. The train/test interface is the same as above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">linear_regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linear_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># this tunes the internal parameters of the model</span>

<span class="c1"># Let us print the model&#39;s parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_regressor</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">linear_regressor</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.86025287  0.1200073  -0.28039183  0.31208687 -0.00957447 -0.02615781
 -0.88821331 -0.86190739]
2.0680774192504314
</pre></div>
</div>
</div>
</div>
<p>We can obtain predictions on the validation set using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_val_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4128,)
</pre></div>
</div>
</div>
</div>
<p>The function returns a vector of <span class="math notranslate nohighlight">\(4128\)</span> predictions, one for each example in the validation set. We can now evaluate the predictions using regression evaluation measures. We will use the standard implementation of the main evaluation measures as provided by scikit-learn:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.533334644741504 0.5297481095803488 0.727837969317587
</pre></div>
</div>
</div>
</div>
<p>All evaluation measures in scikit-learn follow the <code class="docutils literal notranslate"><span class="pre">evaluation_measure(y_true,</span> <span class="pre">y_pred)</span></code> convention. Note that the target variable <code class="docutils literal notranslate"><span class="pre">MedInc</span></code> is measured in tens of thousands of dollars, so an MAE of about <span class="math notranslate nohighlight">\(0.5\)</span> corresponds to an average error of about <span class="math notranslate nohighlight">\(5000\)</span> dollars. This is not that bad if we consider the mean and standard deviation of targets:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2.068077419250646, 1.1509151433486544)
</pre></div>
</div>
</div>
</div>
<p>Each predictor in scikit-learn also provides a <code class="docutils literal notranslate"><span class="pre">score</span></code> method which takes as input the validation (or test) inputs and outputs and computes some standard evaluation measures. By default the linear regressor in scikit-learn returns the <span class="math notranslate nohighlight">\(R^2\)</span> value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_regressor</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6142000785497264
</pre></div>
</div>
</div>
</div>
<p>While we are mainly interested in the performance of the model on the validation set (and ultimately on those on the test set), it is still useful to assess the performance on the training set for model diagnostics. For instance, if we see a big discrepancy between training and validation errors, then we can imagine that some overfitting is going on:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">linear_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">rmse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5266487515751342 0.5143795055231386 0.7172025554354492
</pre></div>
</div>
</div>
</div>
<p>We can see that, while there are some differences between training and test performance, those are minor, so we can deduce that there is no significant overfitting going on.</p>
<blockquote>
<div><p>We should note that we should <strong>always expect a certain degree of overfitting, depending on the task, the data and the model</strong>. When the difference between train and test error is large, and hence there is significant overfitting, we can try to reduce this effect with regularization techniques.</p>
</div></blockquote>
<p>To better compare models, we will now store the results of our analyses in a dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Method&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Linear Regressor&#39;</span><span class="p">],</span>
    <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">],</span>
    <span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mae</span><span class="p">],</span>
    <span class="s1">&#39;MSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">mse</span><span class="p">],</span>
    <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">rmse</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It is common to use the word “method” to refer to a predictive algorithm or pipeline.</p>
</section>
<section id="id1">
<h3><span class="section-number">17.6.5. </span>Non-Linear Regression<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>Let us now try to fit a non-linear regressor. We will use polynomial regression with different polynomial degrees. To do so, we will perform an explicit polynomial expansion of the features using the <code class="docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code> object. For convenience, we will define a function performing training and validation and returning both training and validation performance:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="k">def</span> <span class="nf">trainval_polynomial</span><span class="p">(</span><span class="n">degree</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1"># While the model does not have any learnable parameters, the &quot;fit&quot; method here is used to compute the output number of features</span>
    <span class="n">pf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_val_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span> <span class="c1"># a Polynomial regressor is simply a linear regressor using polynomial features</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_poly_train_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">)</span>
    <span class="n">y_poly_val_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_poly</span><span class="p">)</span>

    <span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">mae_val</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">rmse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">mae_val</span><span class="p">,</span> <span class="n">mse_val</span><span class="p">,</span> <span class="n">rmse_val</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now see what happens with different degrees:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;DEGREE: </span><span class="si">{}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial</span><span class="p">(</span><span class="n">d</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DEGREE: 1 
           MAE      MSE     RMSE
TRAIN     0.53     0.51     0.72 
VAL       0.53     0.53     0.73


DEGREE: 2 
           MAE      MSE     RMSE
TRAIN     0.46     0.42     0.65 
VAL       0.48     0.91     0.95


DEGREE: 3 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.58 
VAL      23.48 2157650.15  1468.89
</pre></div>
</div>
</div>
</div>
</section>
<section id="ridge-regularization">
<h3><span class="section-number">17.6.6. </span>Ridge Regularization<a class="headerlink" href="#ridge-regularization" title="Permalink to this heading">#</a></h3>
<p>We can see that, as the polynomial gets larger, the effect of overfitting increases. We can try to reduce this effect with Ridge or Lasso regularization. We’ll focus on degree <span class="math notranslate nohighlight">\(2\)</span> and try to apply ridge regression to it. Since Ridge regression relies on a parameter, we will try some values of the regularization parameter <span class="math notranslate nohighlight">\(\alpha\)</span> (as it is called by sklearn). Let us define a function for convenience:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="k">def</span> <span class="nf">trainval_polynomial_ridge</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1"># While the model does not have any learnable parameters, the &quot;fit&quot; method here is used to compute the output number of features</span>
    <span class="n">pf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_val_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># a Polynomial regressor is simply a linear regressor using polynomial features</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_poly_train_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">)</span>
    <span class="n">y_poly_val_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_poly</span><span class="p">)</span>

    <span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">mae_val</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">rmse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">mae_val</span><span class="p">,</span> <span class="n">mse_val</span><span class="p">,</span> <span class="n">rmse_val</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now see the results for different values of <span class="math notranslate nohighlight">\(\alpha\)</span>. <span class="math notranslate nohighlight">\(\alpha=0\)</span> means no regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RIDGE, DEGREE: 2&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">300</span><span class="p">,</span><span class="mi">400</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Alpha: </span><span class="si">{:0.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RIDGE, DEGREE: 2
Alpha: 0.00 
           MAE      MSE     RMSE
TRAIN     0.46     0.42     0.65 
VAL       0.48     0.91     0.96


Alpha: 100.00 
           MAE      MSE     RMSE
TRAIN     0.47     0.43     0.66 
VAL       0.48     0.54     0.74


Alpha: 200.00 
           MAE      MSE     RMSE
TRAIN     0.48     0.44     0.67 
VAL       0.49     0.51     0.72


Alpha: 300.00 
           MAE      MSE     RMSE
TRAIN     0.49     0.46     0.68 
VAL       0.50     0.50     0.71


Alpha: 400.00 
           MAE      MSE     RMSE
TRAIN     0.50     0.47     0.68 
VAL       0.51     0.51     0.71
</pre></div>
</div>
</div>
</div>
<p>We can see how, as alpha increases, the error on the training set increases, while the error on the test set decreases. For <span class="math notranslate nohighlight">\(\alpha=300\)</span> we obtained a slightly better result than our linear regressor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let us see if we can improve the results with a polynomial of degree 3:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RIDGE, DEGREE: 3&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Alpha: </span><span class="si">{:0.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DEGREE: 3
Alpha: 0.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.58 
VAL      23.50 2162209.37  1470.45


Alpha: 1.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.58 
VAL      15.59 934580.07   966.74


Alpha: 10.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.34     0.59 
VAL       1.57  4867.65    69.77


Alpha: 20.00 
           MAE      MSE     RMSE
TRAIN     0.42     0.35     0.59 
VAL       1.78  6690.78    81.80
</pre></div>
</div>
</div>
</div>
<p>Let us add the results of Polynomial regression of degree 2 with and without regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poly2</span> <span class="o">=</span> <span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">poly2_ridge300</span> <span class="o">=</span> <span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">300</span><span class="p">)</span>
<span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Polynomial Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">poly2</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">poly2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">poly2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Polynomial Ridge Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2, alpha=300&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">poly2_ridge300</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">poly2_ridge300</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">poly2_ridge300</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480448</td>
      <td>0.912976</td>
      <td>0.955498</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="lasso-regression">
<h3><span class="section-number">17.6.7. </span>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this heading">#</a></h3>
<p>Let us now try the same with Lasso regression:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="k">def</span> <span class="nf">trainval_polynomial_lasso</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="c1"># While the model does not have any learnable parameters, the &quot;fit&quot; method here is used to compute the output number of features</span>
    <span class="n">pf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_train_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_val_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># a Polynomial regressor is simply a linear regressor using polynomial features</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">y_poly_train_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_poly</span><span class="p">)</span>
    <span class="n">y_poly_val_pred</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_poly</span><span class="p">)</span>

    <span class="n">mae_train</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">mse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">)</span>
    <span class="n">rmse_train</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_poly_train_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">mae_val</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">)</span>
    <span class="n">rmse_val</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_poly_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mae_train</span><span class="p">,</span> <span class="n">mse_train</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">mae_val</span><span class="p">,</span> <span class="n">mse_val</span><span class="p">,</span> <span class="n">rmse_val</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LSSO, DEGREE: 2&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.02</span><span class="p">,</span><span class="mf">0.03</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Alpha: </span><span class="si">{:0.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">      </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="s2"> </span><span class="si">{:&gt;8s}</span><span class="se">\n</span><span class="s2">TRAIN </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">VAL   </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="s2"> </span><span class="si">{:8.2f}</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="s2">&quot;MAE&quot;</span><span class="p">,</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSE&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">trainval_polynomial_lasso</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">alpha</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LSSO, DEGREE: 2
Alpha: 0.02 
           MAE      MSE     RMSE
TRAIN     0.52     0.51     0.71 
VAL       0.54     1.19     1.09


Alpha: 0.03 
           MAE      MSE     RMSE
TRAIN     0.55     0.55     0.74 
VAL       0.55     0.59     0.77


Alpha: 0.04 
           MAE      MSE     RMSE
TRAIN     0.56     0.57     0.76 
VAL       0.57     0.59     0.77


Alpha: 0.05 
           MAE      MSE     RMSE
TRAIN     0.58     0.60     0.78 
VAL       0.58     0.61     0.78


Alpha: 0.06 
           MAE      MSE     RMSE
TRAIN     0.60     0.63     0.80 
VAL       0.60     0.64     0.80
</pre></div>
</div>
</div>
</div>
<p>Lasso regression does not seem to improve results. Let us put the results obtained for <span class="math notranslate nohighlight">\(\alpha=0.04\)</span> to the dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poly2_lasso004</span> <span class="o">=</span> <span class="n">trainval_polynomial_lasso</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.04</span><span class="p">)</span>
<span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Polynomial Lasso Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2, alpha=0.04&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">poly2_lasso004</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">poly2_lasso004</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">poly2_lasso004</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480448</td>
      <td>0.912976</td>
      <td>0.955498</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="grid-search">
<h3><span class="section-number">17.6.8. </span>Grid Search<a class="headerlink" href="#grid-search" title="Permalink to this heading">#</a></h3>
<p>Polynomial regression and ridge regression have parameters to optimize. We have so far optimized them manually. However, in practice, it is common to perform a grid search. This consists in defining a grid of possible values to try and train/validate many models, to finally choose the one with best performance.</p>
<p>This can be done manually as shown in the following example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grid_search</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">25</span><span class="p">),</span> <span class="n">degrees</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)):</span>
    <span class="n">best_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating a=</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2"> d=</span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2"> MSE=&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">trainval_polynomial_ridge</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
            <span class="n">mse</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">mse</span><span class="o">&lt;</span><span class="n">best_mse</span><span class="p">:</span>
                <span class="n">best_mse</span> <span class="o">=</span> <span class="n">mse</span>
                <span class="n">best_alpha</span> <span class="o">=</span> <span class="n">a</span>
                <span class="n">best_degree</span> <span class="o">=</span> <span class="n">d</span>

    <span class="k">return</span> <span class="n">best_mse</span><span class="p">,</span> <span class="n">best_alpha</span><span class="p">,</span> <span class="n">best_degree</span>
<span class="n">grid_search</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluating a=200 d=0 MSE=1.37
Evaluating a=200 d=1 MSE=0.53
Evaluating a=200 d=2 MSE=0.51
Evaluating a=200 d=3 MSE=8161.95
Evaluating a=200 d=4 MSE=10884172.40
Evaluating a=225 d=0 MSE=1.37
Evaluating a=225 d=1 MSE=0.53
Evaluating a=225 d=2 MSE=0.51
Evaluating a=225 d=3 MSE=6710.18
Evaluating a=225 d=4 MSE=7243991.68
Evaluating a=250 d=0 MSE=1.37
Evaluating a=250 d=1 MSE=0.53
Evaluating a=250 d=2 MSE=0.51
Evaluating a=250 d=3 MSE=5542.92
Evaluating a=250 d=4 MSE=4880680.92
Evaluating a=275 d=0 MSE=1.37
Evaluating a=275 d=1 MSE=0.53
Evaluating a=275 d=2 MSE=0.50
Evaluating a=275 d=3 MSE=4598.99
Evaluating a=275 d=4 MSE=3312886.74
Evaluating a=300 d=0 MSE=1.37
Evaluating a=300 d=1 MSE=0.54
Evaluating a=300 d=2 MSE=0.50
Evaluating a=300 d=3 MSE=3831.00
Evaluating a=300 d=4 MSE=2255901.13
Evaluating a=325 d=0 MSE=1.37
Evaluating a=325 d=1 MSE=0.54
Evaluating a=325 d=2 MSE=0.50
Evaluating a=325 d=3 MSE=3202.40
Evaluating a=325 d=4 MSE=1534556.72
Evaluating a=350 d=0 MSE=1.37
Evaluating a=350 d=1 MSE=0.54
Evaluating a=350 d=2 MSE=0.51
Evaluating a=350 d=3 MSE=2684.99
Evaluating a=350 d=4 MSE=1038222.45
Evaluating a=375 d=0 MSE=1.37
Evaluating a=375 d=1 MSE=0.54
Evaluating a=375 d=2 MSE=0.51
Evaluating a=375 d=3 MSE=2256.89
Evaluating a=375 d=4 MSE=695227.56
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.504155356305115, 300, 2)
</pre></div>
</div>
</div>
</div>
<p>Testing a range of values, we found that best results are obtained with degree equal to <span class="math notranslate nohighlight">\(2\)</span> and <span class="math notranslate nohighlight">\(\alpha=300\)</span>.</p>
</section>
<section id="scikit-learn-pipelines">
<h3><span class="section-number">17.6.9. </span>Scikit-Learn Pipelines<a class="headerlink" href="#scikit-learn-pipelines" title="Permalink to this heading">#</a></h3>
<p>Often, a predictive model is obtained by stacking different components. For instance, in our example, a Polynomial regressor is obtained by following this pipeline:</p>
<ul class="simple">
<li><p>Data standardization;</p></li>
<li><p>Polynomial feature expansion:</p></li>
<li><p>Ridge regression.</p></li>
</ul>
<p>While these steps can be carried out independently as seen before, scikit-learn offers the  powerful interface of <code class="docutils literal notranslate"><span class="pre">Pipelines</span></code> to automate this process. A pipeline stacks different components together and makes it convenient to change some of elements of the pipeline or to optimize its parameters. Let us define a pipeline as the one discussed above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="n">polynomial_regressor</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;polynomial_expansion&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;ridge_regression&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>To take full advantage of the pipeline, we will re-load the dataset and avoid applying the standard scaler manually:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let us load the data without passing as_frame=True</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="c1"># the features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span> <span class="c1"># the targets</span>

<span class="c1"># We&#39;ll do a 60:20:20 split</span>
<span class="n">val_prop</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="c1"># We&#39;ll split the data in two steps - first let&#39;s create a test set and a combined trainval set</span>
<span class="n">X_trainval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># We&#39;ll now split the combined trainval into train and val set with the chosen proportions</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">test_prop</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Let us check shapes and proportions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(12384, 8) (4128, 8) (4128, 8)
0.6 0.2 0.2
</pre></div>
</div>
</div>
</div>
<p>Our pipeline has two parameters that we need to set: the Ridge regressor’s <span class="math notranslate nohighlight">\(\alpha\)</span> and the degree of the polynomial. We can set these parameters as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We use the notation &quot;object__parameter&quot; to identify parameter names</span>
<span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">polynomial_expansion__degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ridge_regression__alpha</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                (&#x27;polynomial_expansion&#x27;, PolynomialFeatures()),
                (&#x27;ridge_regression&#x27;, Ridge(alpha=300))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" ><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                (&#x27;polynomial_expansion&#x27;, PolynomialFeatures()),
                (&#x27;ridge_regression&#x27;, Ridge(alpha=300))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" ><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge(alpha=300)</pre></div></div></div></div></div></div></div></div></div>
</div>
<p>We can now fit and test the model as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.4992282794518701, 0.504155356305115, 0.7100389822433096)
</pre></div>
</div>
</div>
</div>
</section>
<section id="grid-search-with-cross-validation">
<h3><span class="section-number">17.6.10. </span>Grid Search with Cross Validation<a class="headerlink" href="#grid-search-with-cross-validation" title="Permalink to this heading">#</a></h3>
<p>Scikit-learn offers a powerful interface to perform grid search with cross validation. In this case, rather than using a fixed training set, a K-Fold validation is performed for each parameter choice in order to find the best performing parameter combination. This is convenient when a validation set is not available. We will combine this approach with the pipelines to easily automate the search of optimal parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">polynomial_regressor</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;polynomial_expansion__degree&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="s1">&#39;ridge_regression__alpha&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">25</span><span class="p">)},</span> <span class="n">scoring</span><span class="o">=</span><span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">,</span><span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We will now fit the model on the union of training and validation set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-39 {color: black;}#sk-container-id-39 pre{padding: 0;}#sk-container-id-39 div.sk-toggleable {background-color: white;}#sk-container-id-39 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-39 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-39 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-39 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-39 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-39 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-39 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-39 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-39 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-39 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-39 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-39 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-39 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-39 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-39 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-39 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-39 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-39 div.sk-item {position: relative;z-index: 1;}#sk-container-id-39 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-39 div.sk-item::before, #sk-container-id-39 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-39 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-39 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-39 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-39 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-39 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-39 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-39 div.sk-label-container {text-align: center;}#sk-container-id-39 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-39 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-39" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                                       (&#x27;polynomial_expansion&#x27;,
                                        PolynomialFeatures(degree=0)),
                                       (&#x27;ridge_regression&#x27;, Ridge(alpha=200))]),
             param_grid={&#x27;polynomial_expansion__degree&#x27;: range(0, 5),
                         &#x27;ridge_regression__alpha&#x27;: range(200, 400, 25)},
             scoring=make_scorer(mean_squared_error, greater_is_better=False))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-184" type="checkbox" ><label for="sk-estimator-id-184" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                                       (&#x27;polynomial_expansion&#x27;,
                                        PolynomialFeatures(degree=0)),
                                       (&#x27;ridge_regression&#x27;, Ridge(alpha=200))]),
             param_grid={&#x27;polynomial_expansion__degree&#x27;: range(0, 5),
                         &#x27;ridge_regression__alpha&#x27;: range(200, 400, 25)},
             scoring=make_scorer(mean_squared_error, greater_is_better=False))</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-185" type="checkbox" ><label for="sk-estimator-id-185" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),
                (&#x27;polynomial_expansion&#x27;, PolynomialFeatures(degree=0)),
                (&#x27;ridge_regression&#x27;, Ridge(alpha=200))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-186" type="checkbox" ><label for="sk-estimator-id-186" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-187" type="checkbox" ><label for="sk-estimator-id-187" class="sk-toggleable__label sk-toggleable__label-arrow">PolynomialFeatures</label><div class="sk-toggleable__content"><pre>PolynomialFeatures(degree=0)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-188" type="checkbox" ><label for="sk-estimator-id-188" class="sk-toggleable__label sk-toggleable__label-arrow">Ridge</label><div class="sk-toggleable__content"><pre>Ridge(alpha=200)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<p>Let us check the best parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;polynomial_expansion__degree&#39;: 2, &#39;ridge_regression__alpha&#39;: 250}
</pre></div>
</div>
</div>
</div>
<p>These are similar to the ones found with our previous grid search. We can now fit the final model on the training set and evaluate on the validation set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">polynomial_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.49556977845052963 0.506286626421441 0.711538211497767
</pre></div>
</div>
</div>
</div>
<p>Let us add this result to our dataframe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Cross-Validated Polynomial Ridge Regressor&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;degree=2, alpha=250&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">mae</span><span class="p">,</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">mse</span><span class="p">,</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">rmse</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480448</td>
      <td>0.912976</td>
      <td>0.955498</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cross-Validated Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=250</td>
      <td>0.495570</td>
      <td>0.506287</td>
      <td>0.711538</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="other-regression-algorithms">
<h3><span class="section-number">17.6.11. </span>Other Regression Algorithms<a class="headerlink" href="#other-regression-algorithms" title="Permalink to this heading">#</a></h3>
<p>Thanks to the unified interface of scikit-learn objects, we can easily train other algorithms even if we do not know how they work inside. Of course, to be able to optimize them in the most complex situations we will need to know how they work internally. The following code shows how to train a neural network (we will not see the algorithm formally):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPRegressor</span>

<span class="n">mlp_regressor</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;mlp_regression&#39;</span><span class="p">,</span> <span class="n">MLPRegressor</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">mlp_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">mlp_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_val_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/furnari/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.3802246120708036, 0.3067554686836898, 0.5538550971903119)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">california_housing_val_results</span><span class="p">,</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:</span><span class="s1">&#39;Neural Network&#39;</span><span class="p">,</span> <span class="s1">&#39;Parameters&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">mae</span><span class="p">,</span> <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">mse</span><span class="p">,</span> <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">rmse</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
<span class="p">])</span>
<span class="n">california_housing_val_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480448</td>
      <td>0.912976</td>
      <td>0.955498</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cross-Validated Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=250</td>
      <td>0.495570</td>
      <td>0.506287</td>
      <td>0.711538</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Neural Network</td>
      <td></td>
      <td>0.380225</td>
      <td>0.306755</td>
      <td>0.553855</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="comparison-and-model-selection">
<h3><span class="section-number">17.6.12. </span>Comparison and Model Selection<a class="headerlink" href="#comparison-and-model-selection" title="Permalink to this heading">#</a></h3>
<p>We can now compare the performance of the different models using the table:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>Parameters</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Linear Regressor</td>
      <td></td>
      <td>0.533335</td>
      <td>0.529748</td>
      <td>0.727838</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Polynomial Regressor</td>
      <td>degree=2</td>
      <td>0.480448</td>
      <td>0.912976</td>
      <td>0.955498</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=300</td>
      <td>0.499228</td>
      <td>0.504155</td>
      <td>0.710039</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Polynomial Lasso Regressor</td>
      <td>degree=2, alpha=0.04</td>
      <td>0.567318</td>
      <td>0.590100</td>
      <td>0.768180</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cross-Validated Polynomial Ridge Regressor</td>
      <td>degree=2, alpha=250</td>
      <td>0.495570</td>
      <td>0.506287</td>
      <td>0.711538</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Neural Network</td>
      <td></td>
      <td>0.380225</td>
      <td>0.306755</td>
      <td>0.553855</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Alternatively, we can visualize the results graphically:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">california_housing_val_results</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Method&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/551aa4f0b274324789e5fe724bf5e22b9aa103d9944f6c9c1518fd293cda4e6c.png" src="../_images/551aa4f0b274324789e5fe724bf5e22b9aa103d9944f6c9c1518fd293cda4e6c.png" />
</div>
</div>
<p>From the analysis of the validation performance, it is clear that the neural network performs better. We can now compute the final performance on the test set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">mlp_regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">),</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">rmse</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.37814683358719486, 0.31599512987902556, 0.562134441107308)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="references">
<h2><span class="section-number">17.7. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">https://en.wikipedia.org/wiki/Empirical_risk_minimization</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="17_principal_component_analysis.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">16. </span>Principal Component Analysis</p>
      </div>
    </a>
    <a class="right-next"
       href="19_classification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">18. </span>Classification Task and Evaluation Measures</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">17.1. Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">17.1.1. Example 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2">17.1.2. Example 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-learning">17.2. Statistical Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">17.2.1. Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">17.2.2. Supervised vs Unsupervised Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-performance-of-a-predictive-model">17.3. Evaluating the Performance of a Predictive Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">17.3.1. Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalization-and-cross-validation">17.3.2. Generalization and Cross-Validation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#holdout-validation-or-single-split">17.3.2.1. Holdout Validation or Single Split</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">17.3.2.2. K-Fold Cross-Validation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#leave-one-out-cross-validation">17.3.2.3. Leave-One-Out Cross-Validation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-and-hyperparmeter-optimization">17.4. Model Selection and Hyperparmeter Optimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-predictive-models">17.5. Regression Predictive Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-measures">17.5.1. Evaluation Measures</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">17.5.1.1. Mean Squared Error (MSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">17.5.1.2. Root Mean Squared Error (RMSE)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">17.5.1.3. Mean Absolute Error (MAE)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-as-a-predictive-model-learned-with-empirical-risk-minimization">17.5.2. Linear Regression as a Predictive Model learned with Empirical Risk Minimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-regression">17.5.3. Multivariate Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">17.5.4. Non-linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-regression-problem">17.6. Example Regression Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#california-housing-dataset">17.6.1. California Housing Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-splitting">17.6.2. Data Splitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-normalization">17.6.3. Data Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regressor">17.6.4. Linear Regressor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">17.6.5. Non-Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regularization">17.6.6. Ridge Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">17.6.7. Lasso Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search">17.6.8. Grid Search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-pipelines">17.6.9. Scikit-Learn Pipelines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grid-search-with-cross-validation">17.6.10. Grid Search with Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-regression-algorithms">17.6.11. Other Regression Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-and-model-selection">17.6.12. Comparison and Model Selection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">17.7. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>