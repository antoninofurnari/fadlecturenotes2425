

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>26. Principal Component Analysis &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/17_principal_component_analysis';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="27. Exploratory/Inferential Analysis on the Heart Disease Dataset" href="../practicals/01_heart_disease_explorative_inferential_analysis.html" />
    <link rel="prev" title="25. Density Estimation" href="16_density_estimation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">1. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">2. Introduzione a Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_main_data_analysis_concepts.html">3. Introduction to Data Analysis and Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_probability.html">4. Probability for Data Manipulation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_common_distributions.html">5. Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_information_theory.html">6. Basic Elements of Information Theory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">7. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">8. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">9. Introduzione a Pandas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">10. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_misure_di_tendenza_centrale_dispersione_e_forma.html">11. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_associazione_variabili.html">12. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">13. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">14. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/08_associazione_variabili.html">15. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_statistical_inference.html">16. Statistical Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7 &amp; 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_linear_regression.html">17. Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">18. Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_causal_analysis.html">19. Causal Data Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/10_statistical_inference.html">20. Laboratory on Statistical Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/11_regressione_lineare.html">21. Laboratorio su Regressione Lineare</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/12_regressione_logistica.html">22. Laboratorio su regressione logistica</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="14_data_as_nd_points.html">23. Data as N-Dimensional Points</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_clustering.html">24. Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="16_density_estimation.html">25. Density Estimation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 13</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">26. Principal Component Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../practicals/01_heart_disease_explorative_inferential_analysis.html">27. Exploratory/Inferential Analysis on the Heart Disease Dataset</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 14</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="18_predictive_modeling.html">28. Introduction to Predictive Modelling and Regression Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 15</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="19_classification.html">29. Classification Task and Evaluation Measures</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_discriminative_models_for_classification.html">30. Discriminative Models for Classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 16</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="21_generative_models_for_classification.html">31. Generative Models for Classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/14_clustering_density_estimation_pca.html">32. Clustering, Density Estimation, and Principal Component Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/16_classificazione.html">33. Classificazione</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/17_principal_component_analysis.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/17_principal_component_analysis.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/17_principal_component_analysis.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Principal Component Analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-vs-feature-reduction">26.1. Feature Selection vs Feature Reduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-reduction-example">26.2. Feature Reduction Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formulation">26.3. General Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-our-example">26.4. Back to Our Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-for-data-whitening">26.5. PCA for data whitening</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-an-appropriate-number-of-components">26.6. Choosing an Appropriate Number of Components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-principal-components-load-plots">26.7. Interpretation of the Principal Components - Load Plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-pca">26.8. Applications of PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-and-visualization">26.8.1. Dimensionality Reduction and Visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-decorrelation-principal-component-regression">26.8.2. Data Decorrelation - Principal Component Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-compression">26.8.3. Data Compression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">26.9. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="principal-component-analysis">
<h1><span class="section-number">26. </span>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this heading">#</a></h1>
<p>So far, we have worked with relatively small datasets, with a limited set of interpretable variables. We have seen how models such as linear and logistic regression can help in understanding and interpreting the relationships between variables.</p>
<p>In practice, in many cases, we will work with larger sets of data, where individual variables are not always interpretable. <strong>Consider for instance a dataset of medical images with a resolution of <span class="math notranslate nohighlight">\(600 \times 600\)</span> pixels. We may want to be able to apply data analysis to such images, for instance, for automated diagnosis, but, if we think about it, we obtain a set of <span class="math notranslate nohighlight">\(360000\)</span> individual columns. Moreover, such columns are not directly interpretable (each one represent the gray value of a specific pixel in the image).</strong></p>
<p>In most of these cases, <strong>the data will be highly redundant, with different variables being dependent or reporting similar data in different forms</strong>. Think about it: if there is a large number of variables, it is more likely that some of them capture the same phenomenon from different points of view.</p>
<section id="feature-selection-vs-feature-reduction">
<h2><span class="section-number">26.1. </span>Feature Selection vs Feature Reduction<a class="headerlink" href="#feature-selection-vs-feature-reduction" title="Permalink to this heading">#</a></h2>
<p>We have seen how, when many variables are available, it makes sense to select a subset of such variables. In regression analysis, in particular, we have seen how, when features are highly correlated, we should <strong>discard some of them to reduce collinearity</strong>. Indeed, if two variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are highly correlated, one of the two is redundant to a given extent, and we can ignore it.</p>
<p>We have seen how variables can be selected in different ways:</p>
<ul class="simple">
<li><p>By looking at the correlation matrix, we can directly find those pairs of variables which are highly correlated;</p></li>
<li><p>When defining a linear or logistic regressor, we can remove those variables with a high p-value. We have seen that these are variables which do not contribute significantly to the prediction of the dependent variable, given all other variables present in the regressor;</p></li>
<li><p>Techniques such as <strong>Ridge and Lasso regression</strong> allow to perform some form of variable (or feature) selection, setting very low or zero coefficients for variables which do not contribute significantly to regression.</p></li>
</ul>
<p><strong>In general, however, when we discard a set of variables, we throw away some informative content, unless the variables we are removing can be perfectly reconstructed from the variables we are keeping, e.g., because they are linear combinations of other variables.</strong></p>
<p>Instead of selecting a subset of features to work with, <strong>feature reduction techniques</strong> aim to <strong>find a new set of features which summarize the original set of features losing a small amount of information, while maintaining a limited number of dimensions</strong>.</p>
</section>
<section id="feature-reduction-example">
<h2><span class="section-number">26.2. </span>Feature Reduction Example<a class="headerlink" href="#feature-reduction-example" title="Permalink to this heading">#</a></h2>
<p>Let us consider the Iris dataset. In particular, we will consider two features: <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">length</span></code> and <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span></code>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/ab7ccb1ca2c1eb8019b9acba5338ad4ed43cdfc230445dec8a04879aeacedfdf.png" src="../_images/ab7ccb1ca2c1eb8019b9acba5338ad4ed43cdfc230445dec8a04879aeacedfdf.png" />
</div>
</div>
<p>We can see from the plot above that the two features are highly correlated. Let us compute the Pearson coefficient and the related p-value:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PearsonRResult(statistic=0.8717537758865831, pvalue=1.0386674194498827e-47)
</pre></div>
</div>
</div>
</div>
<p>Let’s say we want to reduce the number of features (e.g., for computational reasons). We could think of applying <strong>feature selection</strong> and choose one fo the two, but we would surely <strong>loose some information</strong>. Instead, we could think of a <strong>linear projection of the data into a single dimension</strong> such that we loose the least possible amount of information.</p>
<p>To avoid any dependence on the units of measures and on the positioning of the points in the space, let’s first standardize the data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/700bafbf82c967b779cb62cd57f184200b9ec44ad917279d65a246bb3f20a879.png" src="../_images/700bafbf82c967b779cb62cd57f184200b9ec44ad917279d65a246bb3f20a879.png" />
</div>
</div>
<p>Transforming the 2D data to a single dimension with a linear transformation can be done with the following expression:</p>
<div class="math notranslate nohighlight">
\[z = u_1 x_1 + u_2 x_2\]</div>
<p>Note that this is equivalent to projecting point <span class="math notranslate nohighlight">\((x_1,x_y)\)</span> to the line passing from the origin with the following equation:</p>
<div class="math notranslate nohighlight">
\[y = \frac{u_2}{u_1}x\]</div>
<p>Once we choose a set of <span class="math notranslate nohighlight">\((u_1,u_2)\)</span>, we obtain a different projection, as in the example below, where we project the data to different random lines passing through the origin:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/635f1fb554c271d9690e4390f5f24ed938a2bf807ee77b6b63fb6864053e6e43.png" src="../_images/635f1fb554c271d9690e4390f5f24ed938a2bf807ee77b6b63fb6864053e6e43.png" />
</div>
</div>
<p>As can be seen, <strong>not all projections are the same</strong>. Indeed, some of them tend to project the points in a space in which they are very close to each other, while others allow for more “space” between the points.</p>
<p>Intuitively, we would like to project points in a way that <strong>they keep their distinctiveness as much as possible</strong>. We can reason on this also form the point of view of information loss. Let’s say we want to reconstruct the values of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(y_1\)</span> from <span class="math notranslate nohighlight">\(z\)</span>. Recall that the process by which we project <span class="math notranslate nohighlight">\((x_1,x_2)\)</span> onto <span class="math notranslate nohighlight">\((u_1,u_2)\)</span> is <strong>lossy</strong>:</p>
<div class="math notranslate nohighlight">
\[z = u_1 x_1 + u_2 x_2\]</div>
<p>In practice, <strong>we cannot reconstruct <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> with arbitrary precision</strong>. The best we can do is to approximate the point with its projection, hence by scaling vector <span class="math notranslate nohighlight">\((u_1,u_2)\)</span> by <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[(\hat{x}_1,\hat{x}_2)^T = z \cdot (u_1,u_2)^T\]</div>
<p>An example is shown in the following figure:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/be61f11c92318ede27533866637546d188a841c51e02723d258858af1ca7909c.png" src="../_images/be61f11c92318ede27533866637546d188a841c51e02723d258858af1ca7909c.png" />
</div>
</div>
<p>In the expressions:</p>
<div class="math notranslate nohighlight">
\[z = u_1 x_1 + u_2 x_2\]</div>
<div class="math notranslate nohighlight">
\[(\hat{x}_1,\hat{x}_2)^T = z \cdot (u_1,u_2)^T\]</div>
<p>Points <span class="math notranslate nohighlight">\((x_1,x_2)\)</span> are the blue circles, while points <span class="math notranslate nohighlight">\((\hat{x}_1,\hat{x}_2)\)</span> are the red ones in the plots shown above on random projection. Intuitively, to reduce the reconstruction error, we want to choose <strong>the line which best approximates the points</strong>, hence reducing the distance between the points and the line (as in linear regression where we tried to minimize RSS).</p>
<p><strong>It can be shown that this is equivalent to finding the projection such that <span class="math notranslate nohighlight">\(z\)</span> has the largest variance</strong>. Intuitively, this makes sense as highest variance means that the points are more distinguishable and hence their reconstructions will be more distinguishable as well.</p>
<p>While we will not see it formally, the plot below shows replicates the previous plot adding the mean squared error between original and reconstructed points:</p>
<div class="math notranslate nohighlight">
\[MSE = \frac{1}{N}\sum_{i}||({x}_1^{(i)},{x}_2^{(i)})-(\hat{x}_1^{(i)},\hat{x}_2^{(i)})||^2\]</div>
<p>Along with the variance of the projected data:</p>
<div class="math notranslate nohighlight">
\[Var = \frac{1}{N}\sum_{i} (z^{(i)}-\overline{z})^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{z}=\frac{1}{N} \sum_{i} z^{(i)}\)</span>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/917f9363342e3f1a4023af360e67acd13b24d79d63cee6d473fa96d9b339a7fa.png" src="../_images/917f9363342e3f1a4023af360e67acd13b24d79d63cee6d473fa96d9b339a7fa.png" />
</div>
</div>
<p>As can be noted, the MSE is smallest when the variance is highest, which corresponds to the condition in which the data is maximally distinguishable.</p>
</section>
<section id="general-formulation">
<h2><span class="section-number">26.3. </span>General Formulation<a class="headerlink" href="#general-formulation" title="Permalink to this heading">#</a></h2>
<p>We have seen a simple example in the case of two variables. Let us now discuss the general formulation in the case of <span class="math notranslate nohighlight">\(D\)</span>-dimensional variables.</p>
<p>Let <span class="math notranslate nohighlight">\(\{\mathbf{x}_n\}\)</span> be a set of <span class="math notranslate nohighlight">\(N\)</span> observations, where <span class="math notranslate nohighlight">\(n=1,\ldots,N\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> is a vector of dimensionality <span class="math notranslate nohighlight">\(D\)</span>. We will define Principal Component Analysis as a projection of the data to <span class="math notranslate nohighlight">\(M \lt D\)</span> dimensions such that the variance of the projected data is maximized.</p>
<blockquote>
<div><p><strong>NOTE</strong>: to avoid any influence by the individual variances along the original axes, we will assume that <strong>the data is standardized</strong>.</p>
</div></blockquote>
<p>We will first consider the case in which <span class="math notranslate nohighlight">\(M=1\)</span>. In this case, we want to find a D-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> such that, projecting the data to this vector, variance is maximized. Without loss of generality, we will choose <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{u}_1^T\mathbf{u}=1\)</span>. This corresponds to choosing <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> as a <strong>unit vector</strong>, which makes sense, as we are interested in the direction of the projection. We will find this assumption useful later.</p>
<p>Let <span class="math notranslate nohighlight">\(\overline{\mathbf{x}}\)</span> be the sample mean:</p>
<div class="math notranslate nohighlight">
\[\overline{\mathbf{x}} = \sum_{n=1}^N x_n\]</div>
<p>The data covariance matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S} = \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n - \overline{\mathbf{x}}) (\mathbf{x}_n - \overline{\mathbf{x}})^T\]</div>
<p>We know that projecting a data point to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is done simply by the dot product:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{u}_1^T \mathbf{x} \]</div>
<p>The mean of the projected data is hence <span class="math notranslate nohighlight">\(\mathbf{u}_1^T \overline{\mathbf{x}}\)</span>, and its variance is:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \sum_{n=1}^N (\mathbf{u}_1^T \mathbf{x}_n - \mathbf{u}_1^T \overline{\mathbf{x}}) (\mathbf{u}_1^T \mathbf{x}_n - \mathbf{u}_1^T \overline{\mathbf{x}})^T = \mathbf{u}_1^T \frac{1}{N} \sum_{n=1}^N (\mathbf{x}_n -  \overline{\mathbf{x}}) ( \mathbf{x}_n - \overline{\mathbf{x}})^T \mathbf{u}_1 = \mathbf{u}_1^T \mathbf{S}\mathbf{u}_1 \]</div>
<p>To find an appropriate <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>, we need to maximize the variance of the projected data with respect to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span>. The optimization problem can be formalized as follows:</p>
<div class="math notranslate nohighlight">
\[\arg\max \{\mathbf{u}_1^T\mathbf{S}\mathbf{u_1}\} \text{ subject to } \mathbf{u}_1^T\mathbf{u}_1=1\]</div>
<p>Note that the constraint <span class="math notranslate nohighlight">\(\mathbf{u}_1^T\mathbf{u}_1=1\)</span> is necessary. Without it, we could arbitrarily increase the variance by choosing vectors <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> with large modules, hence <span class="math notranslate nohighlight">\(\mathbf{u}_1^T\mathbf{u}_1 \to +\infty\)</span>.</p>
<p><strong>We will not see how this optimization problem is solved in details</strong>, but it can be shown that the variance is maximized when:</p>
<div class="math notranslate nohighlight">
\[\mathbf{u}_1^T \mathbf{S} \mathbf{u}_1 = \lambda_1\]</div>
<p>Note that this is equivalent to:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S} \mathbf{u}_1 = \lambda_1 \mathbf{u}_1\]</div>
<p>From the result above, we find out that:</p>
<ul class="simple">
<li><p>The solution <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{S}\)</span>. The related eigenvalue  <span class="math notranslate nohighlight">\(\lambda_1\)</span> is the variance along that dimension;</p></li>
<li><p>Since we want to maximize the variance, among all eigenvectors, we should choose the one with the <strong>largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span></strong>.</p></li>
</ul>
<p>The vector <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span> will be <strong>the first principal component</strong> of the data.</p>
<p><strong>We can proceed in an iterative fashion to find the other components</strong>. In practice, it can be shown that, to obtain uncorrelated variables, it is convenient to choose the next component <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{u}_2 \perp \mathbf{u}_1\)</span>. Doing so, we will form an <strong>orthonormal base</strong> of the data. We can hence identify the second principal component by choosing the vector <span class="math notranslate nohighlight">\(\mathbf{u}_2\)</span> with maximizes the variance <strong>among all vectors which are orthogonal to <span class="math notranslate nohighlight">\(\mathbf{u}_1\)</span></strong>.</p>
<p>In practice, it can be shown that the <span class="math notranslate nohighlight">\(M\)</span> principal components can be found by choosing the <span class="math notranslate nohighlight">\(M\)</span> eigenvectors <span class="math notranslate nohighlight">\(\mathbf{u}_1, \ldots, \mathbf{u}_M\)</span> of the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> corresponding to the largest eigenvectors <span class="math notranslate nohighlight">\(\lambda_1, \ldots, \lambda_M\)</span>.</p>
<p>Let us define the matrix <span class="math notranslate nohighlight">\(W\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{W} = \left(\begin{matrix}
\mathbf{u}_1 \\
\mathbf{u}_2 \\
\ldots \\
\mathbf{u}_M \\
\end{matrix}\right) = \left(\begin{matrix}
u_{11} &amp; u_{12} &amp; \ldots &amp; u_{1D} \\
u_{21} &amp; u_{22} &amp; \ldots &amp; u_{2D} \\
\ldots \\
u_{M1} &amp; u_{M2} &amp; \ldots &amp; u_{MD} \\
\end{matrix}\right)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(u_{ij}\)</span> is the <span class="math notranslate nohighlight">\(j^{th}\)</span> component of vector <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span>.</p>
<p>Note that, given the way we constructed this matrix, it will be <strong>a unit orthonormal matrix</strong>. Indeed:</p>
<ul class="simple">
<li><p>All rows are perpendicular, hence the dot product of any row with any other will be <span class="math notranslate nohighlight">\(\mathbf{u}_i^T \mathbf{u}_j=1\)</span>;</p></li>
<li><p>The scalar product of any row with itself will be: <span class="math notranslate nohighlight">\(\mathbf{u}_i^T \mathbf{u}_i\)</span>=1;</p></li>
<li><p>As a consequence the two conditions above will be valid also for columns;</p></li>
<li><p>The determinant of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> will be 1.</p></li>
</ul>
<p>If we choose <span class="math notranslate nohighlight">\(M=D\)</span>, we can hence see <strong><span class="math notranslate nohighlight">\(\mathbf{W}\)</span> as a rotation matrix, coherently with our previous interpretation of the PCA</strong>. If <span class="math notranslate nohighlight">\(M \lt D\)</span>, the matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> identifies a <strong>new base for the data</strong>.</p>
<p>Let:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \left(\begin{matrix}
\mathbf{x}_1 \\
\mathbf{x}_2 \\
\ldots \\
\mathbf{x}_N \\
\end{matrix}\right) = \left(\begin{matrix}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1D} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2D} \\
\ldots \\
x_{N1} &amp; x_{N2} &amp; \ldots &amp; x_{ND} \\
\end{matrix}\right)\end{split}\]</div>
<p>be the <span class="math notranslate nohighlight">\([N \times D]\)</span> data matrix.</p>
<p>We can project the data matrix <span class="math notranslate nohighlight">\(X\)</span> to the principal components with the following formula:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z} = \mathbf{X} \mathbf{W}^T\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> will be an <span class="math notranslate nohighlight">\([N \times M]\)</span> matrix in which the <span class="math notranslate nohighlight">\(i^{th}\)</span> row denotes the <span class="math notranslate nohighlight">\(i^{th}\)</span> element of the dataset projected to the PCA space.</p>
<p>In geometrical terms, the PCA <strong>performs a rotation of the D-dimensional data around the center of the data</strong> in a way that:</p>
<ul class="simple">
<li><p>The new axes are sorted by variance;</p></li>
<li><p>The projected data is uncorrelated.</p></li>
</ul>
</section>
<section id="back-to-our-example">
<h2><span class="section-number">26.4. </span>Back to Our Example<a class="headerlink" href="#back-to-our-example" title="Permalink to this heading">#</a></h2>
<p>We now have the computational tools to fully understand our example. Let us plot the data again. We will consider the mean-centered data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/0240b679d238c4eb92c70dee536f41d067311ea77a4ff91312ae23ecf2ef2178.png" src="../_images/0240b679d238c4eb92c70dee536f41d067311ea77a4ff91312ae23ecf2ef2178.png" />
</div>
</div>
<p>We have seen that the covariance matrix plays a central role in PCA. The covariance matrix will be as follows in our case:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.68569351, 1.27431544],
       [1.27431544, 3.11627785]])
</pre></div>
</div>
</div>
</div>
<p>The eigvenvalues and eigenvectors will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvalues: [0.1400726  3.66189877]

Eigenvectors:
 [[-0.9192793   0.39360585]
 [-0.39360585 -0.9192793 ]]
</pre></div>
</div>
</div>
</div>
<p>The rows of the eigenvector matrix identify two principal components. The first one is associated to a variance of about <span class="math notranslate nohighlight">\(0.14\)</span> (the first eigvenvalue), while the second component is associated to a variance of <span class="math notranslate nohighlight">\(3.66\)</span>. Let us re-order the principal components so that eigenvalues are descending (we want the first principal component to have the largest value):</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvalues: [3.66189877 0.1400726 ]

Eigenvectors:
 [[-0.39360585 -0.9192793 ]
 [-0.9192793   0.39360585]]
</pre></div>
</div>
</div>
</div>
<p>The two components identify two directions along which we can project our data, as shown in the following plot:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/21c336e006d0d9be86e9cc96b6590d64e937159d875ec84f0dc0f4ff027fb005.png" src="../_images/21c336e006d0d9be86e9cc96b6590d64e937159d875ec84f0dc0f4ff027fb005.png" />
</div>
</div>
<p>The eigenvectors can also be interpreted as vectors in the original space. Getting back to our 2D example, they would be shown as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/4010c0a68e1f423811cad62dde16688303835a44f5ddae2903b6b668e5e0f206.png" src="../_images/4010c0a68e1f423811cad62dde16688303835a44f5ddae2903b6b668e5e0f206.png" />
</div>
</div>
<p>The two principal components denote the <strong>main directions of variation within the data</strong>. In the plot above, the vector sizes are proportional to the related eigenvalues (hence the variance).</p>
<p>As we can see, the two directions are orthogonal. We can also project the data maintaining the original dimensionality with:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z} = \mathbf{X}\mathbf{W}^T\]</div>
<p>This process can also see as rotating the data in a way that the the first principal component becomes the new <span class="math notranslate nohighlight">\(x\)</span> axis, while the second principal component becomes the <span class="math notranslate nohighlight">\(y\)</span> axis. In this context, <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> can be seen as a <strong>rotation matrix</strong>. The plot below compares the original and rotated data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/72a6b60cb6723437193d56e12198673f7e2d4038bc4b32fc26af15e78b08b376.png" src="../_images/72a6b60cb6723437193d56e12198673f7e2d4038bc4b32fc26af15e78b08b376.png" />
</div>
</div>
<p>We can see the projection to the first principal component in two stages:</p>
<ol class="arabic simple">
<li><p>Rotating the data;</p></li>
<li><p>Dropping the <span class="math notranslate nohighlight">\(y\)</span> axis.</p></li>
</ol>
<p>Note that the same reasoning can be seen in 3D as shown in the following example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate 3D data with more distinct patterns</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create data clusters with clear elongation along specific axes</span>
<span class="n">cluster1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">cluster2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">cluster3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Combine the clusters into one dataset</span>
<span class="n">data_3d_distinct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">cluster1</span><span class="p">,</span> <span class="n">cluster2</span><span class="p">,</span> <span class="n">cluster3</span><span class="p">])</span>

<span class="c1"># Apply PCA to the more distinct 3D data</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_3d_distinct</span><span class="p">)</span>
<span class="n">data_rotated_distinct</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data_3d_distinct</span><span class="p">)</span>
<span class="n">data_projected_2d_distinct</span> <span class="o">=</span> <span class="n">data_rotated_distinct</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># Create the figure with 3 subplots</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot 1: Original 3D data with clear patterns</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_3d_distinct</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_3d_distinct</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">data_3d_distinct</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original 3D Data with Clear Patterns&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Z&#39;</span><span class="p">)</span>

<span class="c1"># Plot 2: 3D data after PCA rotation</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_rotated_distinct</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_rotated_distinct</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">data_rotated_distinct</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Rotated 3D Data (Aligned with PCA)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;PC3&#39;</span><span class="p">)</span>

<span class="c1"># Plot 3: 2D projection of the more distinct 3D data</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_projected_2d_distinct</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_projected_2d_distinct</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;2D Projection onto First Two Principal Components&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8337cfa8be0e1b79166a0c7bd684cde467aaae79cadd9221a69b0a98fe7ea674.png" src="../_images/8337cfa8be0e1b79166a0c7bd684cde467aaae79cadd9221a69b0a98fe7ea674.png" />
</div>
</div>
</section>
<section id="pca-for-data-whitening">
<h2><span class="section-number">26.5. </span>PCA for data whitening<a class="headerlink" href="#pca-for-data-whitening" title="Permalink to this heading">#</a></h2>
<p>It can also be seen that PCA transforms the original data with a given covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> to a new set of data (with new axes, the principal components), such that its covariance matrix <span class="math notranslate nohighlight">\(\Sigma'\)</span> is <strong>the identity matrix</strong>. This means that <strong>the new variables</strong> are now <strong>decorrelated</strong>.</p>
<p>Intuitively, this makes sense: <strong>we rotated the data so that the new axes are aligned to main directions of variation and we constructed principal components to be orthogonal to each other</strong>.</p>
<p>This process is also called <strong>whitening transformation</strong>. An example of this property is shown below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1b909159f74b46c41274fc82ed2c77d08926611e87c3f82c4e974390644d9787.png" src="../_images/1b909159f74b46c41274fc82ed2c77d08926611e87c3f82c4e974390644d9787.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[3.60543087, 1.84308349],
        [1.84308349, 2.83596216]]),
 array([[1.00000000e+00, 1.71554563e-17],
        [1.71554563e-17, 1.00000000e+00]]))
</pre></div>
</div>
</div>
</div>
<p>This whitening process can be useful when we need to make sure that the input variables are <strong>decorrelated</strong>. For instance, consider the case in which we need to fit a linear regression but we have multiple correlated variables. Pre-processing data with PCA would make sure that the input variables are decorrelated, hence removing any instability during fitting, but it should be also kept in mind that <strong>the linear regression would become less interpretable because of the PCA transformation</strong>.</p>
</section>
<section id="choosing-an-appropriate-number-of-components">
<h2><span class="section-number">26.6. </span>Choosing an Appropriate Number of Components<a class="headerlink" href="#choosing-an-appropriate-number-of-components" title="Permalink to this heading">#</a></h2>
<p>We have seen how PCA allows to reduce the dimensionality of the data. In short, this can be done by computing the first <span class="math notranslate nohighlight">\(M\)</span> eigenvalues and the associated eigenvectors.</p>
<p>In some cases, it is clear what is the value of <span class="math notranslate nohighlight">\(M\)</span> we need to set. For example, in the case of sorting Fisher’s Iris, we set <span class="math notranslate nohighlight">\(M=1\)</span> because we needed a scalar number. In other cases, we would like to reduce the dimensionality of the data, <strong>while keeping a reasonable amount of information about the original data</strong>.</p>
<p>We have seen that the variance is related to the MSE reprojection error and hence to the informative content. We can measure <strong>how much information we are retaining by selecting <span class="math notranslate nohighlight">\(M\)</span> components by measuring the cumulative variance of the first M components</strong>.</p>
<p>We will now consider as an example the whole Fisher Iris dataset, which has four features. The four Principal Components will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
    <tr>
      <th>components</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>z1</th>
      <td>0.361387</td>
      <td>0.656589</td>
      <td>-0.582030</td>
      <td>-0.315487</td>
    </tr>
    <tr>
      <th>z2</th>
      <td>-0.084523</td>
      <td>0.730161</td>
      <td>0.597911</td>
      <td>0.319723</td>
    </tr>
    <tr>
      <th>z3</th>
      <td>0.856671</td>
      <td>-0.173373</td>
      <td>0.076236</td>
      <td>0.479839</td>
    </tr>
    <tr>
      <th>z4</th>
      <td>0.358289</td>
      <td>-0.075481</td>
      <td>0.545831</td>
      <td>-0.753657</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The covariance matrix of the transformed data will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
</style>
<table id="T_533ec">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_533ec_level0_col0" class="col_heading level0 col0" >z1</th>
      <th id="T_533ec_level0_col1" class="col_heading level0 col1" >z2</th>
      <th id="T_533ec_level0_col2" class="col_heading level0 col2" >z3</th>
      <th id="T_533ec_level0_col3" class="col_heading level0 col3" >z4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_533ec_level0_row0" class="row_heading level0 row0" >z1</th>
      <td id="T_533ec_row0_col0" class="data row0 col0" >4.23</td>
      <td id="T_533ec_row0_col1" class="data row0 col1" >0.00</td>
      <td id="T_533ec_row0_col2" class="data row0 col2" >-0.00</td>
      <td id="T_533ec_row0_col3" class="data row0 col3" >0.00</td>
    </tr>
    <tr>
      <th id="T_533ec_level0_row1" class="row_heading level0 row1" >z2</th>
      <td id="T_533ec_row1_col0" class="data row1 col0" >0.00</td>
      <td id="T_533ec_row1_col1" class="data row1 col1" >0.24</td>
      <td id="T_533ec_row1_col2" class="data row1 col2" >-0.00</td>
      <td id="T_533ec_row1_col3" class="data row1 col3" >-0.00</td>
    </tr>
    <tr>
      <th id="T_533ec_level0_row2" class="row_heading level0 row2" >z3</th>
      <td id="T_533ec_row2_col0" class="data row2 col0" >-0.00</td>
      <td id="T_533ec_row2_col1" class="data row2 col1" >-0.00</td>
      <td id="T_533ec_row2_col2" class="data row2 col2" >0.08</td>
      <td id="T_533ec_row2_col3" class="data row2 col3" >0.00</td>
    </tr>
    <tr>
      <th id="T_533ec_level0_row3" class="row_heading level0 row3" >z4</th>
      <td id="T_533ec_row3_col0" class="data row3 col0" >0.00</td>
      <td id="T_533ec_row3_col1" class="data row3 col1" >-0.00</td>
      <td id="T_533ec_row3_col2" class="data row3 col2" >0.00</td>
      <td id="T_533ec_row3_col3" class="data row3 col3" >0.02</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>As we could expect, the covariance matrix is diagonal because the new features are decorrelated. We also note that the principal components are sorted by variance. We will define the variance of the data along the <span class="math notranslate nohighlight">\(z_i\)</span> component as:</p>
<div class="math notranslate nohighlight">
\[var(z_i)\]</div>
<p>The total variance is given by the sum of the variances:</p>
<div class="math notranslate nohighlight">
\[V = var(z_1) + var(z_2) + \ldots + var(z_D)\]</div>
<p>More in general, we will define:</p>
<div class="math notranslate nohighlight">
\[V(n) = var(z_1) + var(z_2) + \ldots + var(z_n)\]</div>
<p>Hence <span class="math notranslate nohighlight">\(V=V(D)\)</span>.</p>
<p>In our example, the total variance <span class="math notranslate nohighlight">\(V\)</span> will be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.57
</pre></div>
</div>
</div>
</div>
<p>We can quantify the fraction of variance explained by each component <span class="math notranslate nohighlight">\(n\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{var(n)}{V(N)}\]</div>
<p>If we compute such value for all <span class="math notranslate nohighlight">\(n\)</span>, we obtain the following vector in our example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.92461872, 0.05306648, 0.01710261, 0.00521218])
</pre></div>
</div>
</div>
</div>
<p>In practice, it is common to visualize the explained variance ratio in a <strong>scree plot</strong>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/24627c6defc30a31ec65a3f6561fc2b9e391204c18b353b2a6161bc75690f785.png" src="../_images/24627c6defc30a31ec65a3f6561fc2b9e391204c18b353b2a6161bc75690f785.png" />
</div>
</div>
<p>As we can see, the first component retains <span class="math notranslate nohighlight">\(92.46\%\)</span> of the variance, while the second one retains only <span class="math notranslate nohighlight">\(5.3\%\)</span> of the variance. It is common to also look at the cumulative variance ratio, defined as:</p>
<div class="math notranslate nohighlight">
\[\frac{V(n)}{V(N)}\]</div>
<p>In our case:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.92461872, 0.97768521, 0.99478782, 1.        ])
</pre></div>
</div>
</div>
</div>
<p>The vector above tell us that considering the first two components accounts for about <span class="math notranslate nohighlight">\(97.77\%\)</span> of the variance.</p>
<p>This approach can guide us in choosing the right number of dimensions to keep, selecting a budget of the variance we wish to be able to retain.</p>
</section>
<section id="interpretation-of-the-principal-components-load-plots">
<h2><span class="section-number">26.7. </span>Interpretation of the Principal Components - Load Plots<a class="headerlink" href="#interpretation-of-the-principal-components-load-plots" title="Permalink to this heading">#</a></h2>
<p>When we transform data with PCA, the new variables have a less direct interpretation. Indeed, if a given variable is a linear combination of other variables, it is not straightforward to assign a meaning to each component.</p>
<p>However, we know that the first components are the ones which contain most of the variance. Hence, inspecting the weights that each of these components given to the original features can give some insights into which of the original features are more relevant. In this context, <strong>the weights of the principal components are also called loadings</strong>. Loadings with <strong>large absolute values are more influential in determining the value of the principal components</strong>.</p>
<p>In practice, we can assess the relevance of features with a <strong>load plot</strong>, which represents each original variable as a 2D point of coordinates given by the loadings corresponding to the first two principal components. Let us show the load plot for our example:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/b0d4a732f3c4290d568daa9b219d8923342a54335498241602d0ca692ec029bd.png" src="../_images/b0d4a732f3c4290d568daa9b219d8923342a54335498241602d0ca692ec029bd.png" />
</div>
</div>
<p>In a load plot:</p>
<ul class="simple">
<li><p>variables that cluster together are positively correlated, while variables on opposite sides may be negatively correlated;</p></li>
<li><p>variables which are further from the origin have higher loadings, so they are more influential in the computation of the PCA.</p></li>
</ul>
<p>A loading plot can also be useful to perform feature selection. For instance, from the plot above all variables seem to be influential, but <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">width</span></code> and <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span></code> are correlated. We could think of using as a subset of variables <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">width</span></code>, <code class="docutils literal notranslate"><span class="pre">sepal</span> <span class="pre">length</span></code> and one of the other two (e.g., <code class="docutils literal notranslate"><span class="pre">petal</span> <span class="pre">length</span></code>).</p>
</section>
<section id="applications-of-pca">
<h2><span class="section-number">26.8. </span>Applications of PCA<a class="headerlink" href="#applications-of-pca" title="Permalink to this heading">#</a></h2>
<p>PCA has several applications in data analysis. We will summarize the main ones in the following.</p>
<section id="dimensionality-reduction-and-visualization">
<h3><span class="section-number">26.8.1. </span>Dimensionality Reduction and Visualization<a class="headerlink" href="#dimensionality-reduction-and-visualization" title="Permalink to this heading">#</a></h3>
<p>We have seen that data can be interpreted as a set of D-dimensional points. When <span class="math notranslate nohighlight">\(D=2\)</span>, it is often useful to visualize the data through a scatterplot. This allows us to see how the data distributes in the space. When <span class="math notranslate nohighlight">\(D&gt;2\)</span>, we can show a series a scatterplot (a pairplot or scattermatrix). However, when <span class="math notranslate nohighlight">\(D\)</span> is very large, it is usually unfeasible to visualize data with all possible scatterplots.</p>
<p>In these cases, it can be useful to transform the data with PCA and then visualize the data points as 2D points in the space identified by the first two principal components.</p>
<p>We will show an example on the multidimensional dataset <strong>DIGITS</strong>. The dataset contains small images of resolution <span class="math notranslate nohighlight">\(8 \times 8 pixels\)</span> representing handwritten digits from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(9\)</span>. We can see the dataset as containing <span class="math notranslate nohighlight">\(64\)</span> variables. Each variable indicates the pixel value at a specific location.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/7fa83c4651307a7b1a39cee4c5a164ff6a1480d6b8797e1f8f0e416ef98b591f.png" src="../_images/7fa83c4651307a7b1a39cee4c5a164ff6a1480d6b8797e1f8f0e416ef98b591f.png" />
</div>
</div>
<p>We can visualize the data with PCA by first projecting the data to <span class="math notranslate nohighlight">\(M=2\)</span> principal components, then plotting the transformed data as 2D points.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/bf14fd7d56c2e85b1b880667fa182a77e1e2ff92e5f54ba4eacb1df26a11564f.png" src="../_images/bf14fd7d56c2e85b1b880667fa182a77e1e2ff92e5f54ba4eacb1df26a11564f.png" />
</div>
</div>
</section>
<section id="data-decorrelation-principal-component-regression">
<h3><span class="section-number">26.8.2. </span>Data Decorrelation - Principal Component Regression<a class="headerlink" href="#data-decorrelation-principal-component-regression" title="Permalink to this heading">#</a></h3>
<p>PCA is also useful when we need to fit a linear regressor and we are in the presence of a dataset with multi-collinearity.</p>
<p>Let us consider the <code class="docutils literal notranslate"><span class="pre">auto_mpg</span></code> dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>displacement</th>
      <th>cylinders</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model_year</th>
      <th>origin</th>
      <th>mpg</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>307.0</td>
      <td>8</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>350.0</td>
      <td>8</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>318.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>304.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>16.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>302.0</td>
      <td>8</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>17.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>393</th>
      <td>140.0</td>
      <td>4</td>
      <td>86.0</td>
      <td>2790</td>
      <td>15.6</td>
      <td>82</td>
      <td>1</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>394</th>
      <td>97.0</td>
      <td>4</td>
      <td>52.0</td>
      <td>2130</td>
      <td>24.6</td>
      <td>82</td>
      <td>2</td>
      <td>44.0</td>
    </tr>
    <tr>
      <th>395</th>
      <td>135.0</td>
      <td>4</td>
      <td>84.0</td>
      <td>2295</td>
      <td>11.6</td>
      <td>82</td>
      <td>1</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>396</th>
      <td>120.0</td>
      <td>4</td>
      <td>79.0</td>
      <td>2625</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>119.0</td>
      <td>4</td>
      <td>82.0</td>
      <td>2720</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
      <td>31.0</td>
    </tr>
  </tbody>
</table>
<p>398 rows × 8 columns</p>
</div></div></div>
</div>
<p>If we try to fit a linear regressor with all variables, we will find multicollinearity:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -17.2184</td> <td>    4.644</td> <td>   -3.707</td> <td> 0.000</td> <td>  -26.350</td> <td>   -8.087</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0199</td> <td>    0.008</td> <td>    2.647</td> <td> 0.008</td> <td>    0.005</td> <td>    0.035</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.4934</td> <td>    0.323</td> <td>   -1.526</td> <td> 0.128</td> <td>   -1.129</td> <td>    0.142</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0170</td> <td>    0.014</td> <td>   -1.230</td> <td> 0.220</td> <td>   -0.044</td> <td>    0.010</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0065</td> <td>    0.001</td> <td>   -9.929</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.005</td>
</tr>
<tr>
  <th>acceleration</th> <td>    0.0806</td> <td>    0.099</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.114</td> <td>    0.275</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7508</td> <td>    0.051</td> <td>   14.729</td> <td> 0.000</td> <td>    0.651</td> <td>    0.851</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4261</td> <td>    0.278</td> <td>    5.127</td> <td> 0.000</td> <td>    0.879</td> <td>    1.973</td>
</tr>
</table></div></div>
</div>
<p>Indeed, some variables have a large p-value, probably because they are correlated with the others.</p>
<p>To perform Principal Component Regression, we can first compute PCA <strong>on the independent variables</strong>, then fit a linear regressor. We will choose <span class="math notranslate nohighlight">\(M=4\)</span> principal components:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/6f5d6b7f98c17dba153fbbb86ba4a6907980465977828212c83a3cf85ec6a978.png" src="../_images/6f5d6b7f98c17dba153fbbb86ba4a6907980465977828212c83a3cf85ec6a978.png" />
<div class="output text_html"><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.492</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.487</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   92.20</td>
</tr>
<tr>
  <th>Date:</th>             <td>Thu, 14 Dec 2023</td> <th>  Prob (F-statistic):</th> <td>9.24e-55</td>
</tr>
<tr>
  <th>Time:</th>                 <td>21:38:13</td>     <th>  Log-Likelihood:    </th> <td> -1207.7</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   386</td>      <th>  AIC:               </th> <td>   2425.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   381</td>      <th>  BIC:               </th> <td>   2445.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   23.3607</td> <td>    0.283</td> <td>   82.480</td> <td> 0.000</td> <td>   22.804</td> <td>   23.918</td>
</tr>
<tr>
  <th>z1</th>        <td>   -0.0051</td> <td>    0.000</td> <td>  -15.346</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.004</td>
</tr>
<tr>
  <th>z2</th>        <td>   -0.0320</td> <td>    0.007</td> <td>   -4.373</td> <td> 0.000</td> <td>   -0.046</td> <td>   -0.018</td>
</tr>
<tr>
  <th>z3</th>        <td>   -0.0552</td> <td>    0.018</td> <td>   -3.138</td> <td> 0.002</td> <td>   -0.090</td> <td>   -0.021</td>
</tr>
<tr>
  <th>z4</th>        <td>   -0.8815</td> <td>    0.086</td> <td>  -10.234</td> <td> 0.000</td> <td>   -1.051</td> <td>   -0.712</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 3.702</td> <th>  Durbin-Watson:     </th> <td>   1.125</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.157</td> <th>  Jarque-Bera (JB):  </th> <td>   3.435</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.214</td> <th>  Prob(JB):          </th> <td>   0.179</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.174</td> <th>  Cond. No.          </th> <td>    860.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div></div>
</div>
<p>While the result is not as interpretable as it was before, this technique may be useful in the case of predictive analysis.</p>
</section>
<section id="data-compression">
<h3><span class="section-number">26.8.3. </span>Data Compression<a class="headerlink" href="#data-compression" title="Permalink to this heading">#</a></h3>
<p>Now let’s see a simple example of data compression using PCA. In particular, we will consider the case of compressing images. An image can be seen as high-dimensional data, where the number of dimensions is equal to the number of pixels. For example, an RGB image of size <span class="math notranslate nohighlight">\(640 \times 480\)</span> pixels has <span class="math notranslate nohighlight">\(3 \cdot 640 \cdot 480=921600\)</span> dimensions.</p>
<p>We expect that there will be redundant information in all these dimensions. One method to compress images is to divide them into fixed-size blocks (e.g., <span class="math notranslate nohighlight">\(8 \times 8\)</span>). Each of these blocks will be an element belonging to a population (the population of <span class="math notranslate nohighlight">\(8 \times 8\)</span> blocks of the image).</p>
<p>Assuming that the information in the blocks is highly correlated, we can try to compress it by applying PCA to the sample of blocks extracted from our image and choosing only a few principal components to represent the content of the blocks.</p>
<p>We will consider the following example image:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image Size: (427, 640, 3)
Number of dimensions: 819840
</pre></div>
</div>
<img alt="../_images/43ea5685c00917a41d0d26f7633cd991ec238b2015e71fe2ac4628ed48c82581.png" src="../_images/43ea5685c00917a41d0d26f7633cd991ec238b2015e71fe2ac4628ed48c82581.png" />
</div>
</div>
<p>We will divide the image into RGB blocks of size <span class="math notranslate nohighlight">\(8 \times 8 \times 3\)</span> (these are <span class="math notranslate nohighlight">\(8 \times 8\)</span> RGB images). These will look as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/385115d390dfb52aaffac60ab237330e7f48e6e2c9a963c37779e772e0ccc4dd.png" src="../_images/385115d390dfb52aaffac60ab237330e7f48e6e2c9a963c37779e772e0ccc4dd.png" />
</div>
</div>
<p>If we compute the PCA of all blocks, we will obtain the following cumulative variance ratio vector:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.72033322, 0.8054948 , 0.88473857, 0.91106289, 0.92773705,
       0.94002709, 0.94742984, 0.95262262, 0.95724086, 0.96138203,
       0.96493482, 0.96813026, 0.97074765, 0.97304417, 0.97502487,
       0.97676849, 0.97827393, 0.97961611, 0.98080084, 0.98195037,
       0.98298693, 0.98399279, 0.98491245, 0.98570784, 0.98649079,
       0.98719466, 0.98779928, 0.98835152, 0.98888528, 0.98937606,
       0.98983658, 0.99027875])
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a46dcc263e4e417cdad0e9c38f72e9d1f2cd074717504160eb500ff01641b03b.png" src="../_images/a46dcc263e4e417cdad0e9c38f72e9d1f2cd074717504160eb500ff01641b03b.png" />
</div>
</div>
<p>As we can see, truncating at the first component allows us to retain about <span class="math notranslate nohighlight">\(72%\)</span> of the information, truncating at the second allows us to retain about <span class="math notranslate nohighlight">\(80%\)</span>, and so on, up to <span class="math notranslate nohighlight">\(32\)</span> components, which allow us to retain about <span class="math notranslate nohighlight">\(99%\)</span> of the information. Now let’s see how to compress and reconstruct the image. We will choose the first <span class="math notranslate nohighlight">\(32\)</span> components, preserving <span class="math notranslate nohighlight">\(99%\)</span> of the information.</p>
<p>If we do so, and then project the tiles to the compressed space each tile will be represented by only <span class="math notranslate nohighlight">\(32\)</span> numbers. This will lead to the following savings in space:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Space saved in the compression: 83.33%
</pre></div>
</div>
</div>
</div>
<p>We can reconstruct the original image by applying the inverse PCA transformation to the compressed patches. The result will be as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e987571bbeaef48bfb8df7986ca617bfebf01f45ec1b7087a5e8f84fae956832.png" src="../_images/e987571bbeaef48bfb8df7986ca617bfebf01f45ec1b7087a5e8f84fae956832.png" />
</div>
</div>
<p>The plot below shows how the reconstruction quality increases when more components are used:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/493ba064aa5904dcb214be0ef37650f837dd30ef9d42fe7a4cfe06be8efa5df6.png" src="../_images/493ba064aa5904dcb214be0ef37650f837dd30ef9d42fe7a4cfe06be8efa5df6.png" />
</div>
</div>
</section>
</section>
<section id="references">
<h2><span class="section-number">26.9. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Parts of Chapter 12 of [1]</p></li>
</ul>
<p>[1] Bishop, Christopher M., and Nasser M. Nasrabadi. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="16_density_estimation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">25. </span>Density Estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="../practicals/01_heart_disease_explorative_inferential_analysis.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">27. </span>Exploratory/Inferential Analysis on the Heart Disease Dataset</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-vs-feature-reduction">26.1. Feature Selection vs Feature Reduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-reduction-example">26.2. Feature Reduction Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formulation">26.3. General Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-our-example">26.4. Back to Our Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-for-data-whitening">26.5. PCA for data whitening</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-an-appropriate-number-of-components">26.6. Choosing an Appropriate Number of Components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-principal-components-load-plots">26.7. Interpretation of the Principal Components - Load Plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-pca">26.8. Applications of PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction-and-visualization">26.8.1. Dimensionality Reduction and Visualization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-decorrelation-principal-component-regression">26.8.2. Data Decorrelation - Principal Component Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-compression">26.8.3. Data Compression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">26.9. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>