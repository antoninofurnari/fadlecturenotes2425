

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>18. Linear Regression &#8212; Lecture Notes on Fundamentals of Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/11_linear_regression';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="19. Logistic Regression" href="12_logistic_regression.html" />
    <link rel="prev" title="16. Statistical Inference" href="10_statistical_inference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Lecture Notes on Fundamentals of Data Analysis - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Lecture Notes on Fundamental of Data Analysis
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/01_setup.html">1. Introduzione ai laboratori e Installazione dell’Ambiente di Lavoro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/02_intro_python.html">2. Introduzione a Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03_main_data_analysis_concepts.html">3. Introduction to Data Analysis and Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_probability.html">4. Probability for Data Manipulation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="08_common_distributions.html">5. Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_information_theory.html">6. Basic Elements of Information Theory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/03_intro_numpy.html">7. Introduzione a Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/04_intro_matplotlib.html">8. Introduzione a Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/05_intro_pandas.html">9. Introduzione a Pandas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="04_misure_di_frequenze_e_rappresentazione_grafica_dei_dati.html">10. Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_misure_di_tendenza_centrale_dispersione_e_forma.html">11. Misure di Tendenza Centrale, Dispersione e Forma</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06_associazione_variabili.html">12. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Laboratory 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../laboratories/06_misure_di_frequenze_e_rappresentazioni_grafiche_dei_dati.html">13. Laboratorio su Misure di Frequenze e Rappresentazione Grafica dei Dati</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/07_misure_di_tendenza_centrale_dispersione_e_forma.html">14. Laboratorio su Misure di Tendenza Centrale, Dispersione e Forma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../laboratories/08_associazione_variabili.html">15. Associazione tra Variabili</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_statistical_inference.html">16. Statistical Inference</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 7 &amp; 8</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">18. Linear Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_logistic_regression.html">19. Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_causal_analysis.html">20. Causal Data Analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/antoninofurnari/fadlecturenotes/blob/master/lecturenotes/lectures/11_linear_regression.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/antoninofurnari/fadlecturenotes/issues/new?title=Issue%20on%20page%20%2Flectures/11_linear_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/11_linear_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-auto-mpg-dataset">18.1. The Auto MPG Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-models">18.2. Regression Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">18.3. Simple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-coefficients-ordinary-least-squares-ols">18.3.1. Estimating the Coefficients - Ordinary Least Squares (OLS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-coefficients-of-linear-regression">18.3.2. Interpretation of the Coefficients of Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-of-the-coefficient-estimates">18.3.3. Accuracy of the Coefficient Estimates</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-errors-of-the-regression-coefficients">18.3.3.1. Standard Errors of the Regression Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-of-the-regression-coefficients">18.3.3.2. Confidence Intervals of the Regression Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-tests-for-the-significance-of-coefficients">18.3.3.3. Statistical Tests for the Significance of Coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-of-the-model">18.3.4. Accuracy of the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-standard-error">18.3.4.1. Residual Standard Error</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#r-2-statistic">18.3.4.2. <span class="math notranslate nohighlight">\(R^2\)</span> Statistic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">18.4. Multiple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-interpretation">18.4.1. Geometrical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation">18.4.2. Statistical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-regression-coefficients">18.4.3. Estimating the Regression Coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f-test">18.4.4. The F-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">18.4.5. Variable Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-2">18.4.6. Adjusted <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-predictors">18.5. Qualitative Predictors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-only-two-levels">18.5.1. Predictors with Only Two Levels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-more-than-two-levels">18.5.2. Predictors with More than Two Levels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extensions-of-the-linear-model">18.6. Extensions of the Linear Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-terms">18.6.1. Interaction Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-relationships-quadratic-and-polynomial-regression">18.6.2. Non-linear Relationships: Quadratic and Polynomial Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-plots-and-residual-q-q-plots">18.7. Residual Plots and Residual Q-Q Plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-regularization-techniques">18.8. Collinearity and Regularization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">18.8.1. Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-ridge-regression-coefficients">18.8.1.1. Interpretation of the ridge regression coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">18.8.2. Lasso Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">18.9. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><span class="section-number">18. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h1>
<p>Linear regression is a fundamental and widely used statistical technique in data analysis and machine learning. It is a powerful tool for <strong>modeling and understanding the relationships between variables</strong>. At its core, linear regression aims to establish a linear relationship between a <strong>dependent variable</strong> (the one you want to predict) and <strong>one or more independent variables</strong> (the ones used for prediction). This technique allows us to make predictions, infer associations, and gain insights into how changes in independent variables influence the target variable. Linear regression is both intuitive and versatile, making it a valuable tool for tasks ranging from simple trend analysis to more complex predictive modeling and hypothesis testing.</p>
<p>In this context, we will explore the concepts and applications of linear regression, its different types, and how to implement it using Python.</p>
<section id="the-auto-mpg-dataset">
<h2><span class="section-number">18.1. </span>The Auto MPG Dataset<a class="headerlink" href="#the-auto-mpg-dataset" title="Permalink to this heading">#</a></h2>
<p>We will consider the <a class="reference external" href="https://archive.ics.uci.edu/dataset/9/auto+mpg">Auto MPG</a> dataset, which contains <span class="math notranslate nohighlight">\(398\)</span> measurements of <span class="math notranslate nohighlight">\(8\)</span> different properties of different cars:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>displacement</th>
      <th>cylinders</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model_year</th>
      <th>origin</th>
      <th>mpg</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>307.0</td>
      <td>8</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>350.0</td>
      <td>8</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>318.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>304.0</td>
      <td>8</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>16.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>302.0</td>
      <td>8</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>17.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>393</th>
      <td>140.0</td>
      <td>4</td>
      <td>86.0</td>
      <td>2790</td>
      <td>15.6</td>
      <td>82</td>
      <td>1</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>394</th>
      <td>97.0</td>
      <td>4</td>
      <td>52.0</td>
      <td>2130</td>
      <td>24.6</td>
      <td>82</td>
      <td>2</td>
      <td>44.0</td>
    </tr>
    <tr>
      <th>395</th>
      <td>135.0</td>
      <td>4</td>
      <td>84.0</td>
      <td>2295</td>
      <td>11.6</td>
      <td>82</td>
      <td>1</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>396</th>
      <td>120.0</td>
      <td>4</td>
      <td>79.0</td>
      <td>2625</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>119.0</td>
      <td>4</td>
      <td>82.0</td>
      <td>2720</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
      <td>31.0</td>
    </tr>
  </tbody>
</table>
<p>398 rows × 8 columns</p>
</div></div></div>
</div>
<p>Here is a description of the different variables:</p>
<ul class="simple">
<li><p><strong>Displacement</strong>: The engine’s displacement (in cubic inches), which indicates the engine’s size and power.</p></li>
<li><p><strong>Cylinders</strong>: The number of cylinders in the engine of the car. This is a categorical variable.</p></li>
<li><p><strong>Horsepower</strong>: The engine’s horsepower, a measure of the engine’s performance.</p></li>
<li><p><strong>Weight</strong>: The weight of the car in pounds.</p></li>
<li><p><strong>Acceleration</strong>: The car’s acceleration (in seconds) from 0 to 60 miles per hour.</p></li>
<li><p><strong>Model Year</strong>: The year the car was manufactured. This is often converted into a categorical variable representing the car’s age.</p></li>
<li><p><strong>Origin</strong>: The car’s country of origin or manufacturing.
Car Name: The name or identifier of the car model.</p></li>
<li><p><strong>MPG (Miles per Gallon)</strong>: The fuel efficiency of the car in miles per gallon. It is the variable to be predicted in regression analysis.</p></li>
</ul>
<p>We will start by exploring the relationship between the variables <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">MPG</span></code>. Let’s visualize the related scatterplot:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1fb0c4e965f84bea87a10dff24242ac884f289a50476243e5c973efb3ba80747.png" src="../_images/1fb0c4e965f84bea87a10dff24242ac884f289a50476243e5c973efb3ba80747.png" />
</div>
</div>
</section>
<section id="regression-models">
<h2><span class="section-number">18.2. </span>Regression Models<a class="headerlink" href="#regression-models" title="Permalink to this heading">#</a></h2>
<p>Regression models, in general, aim to <strong>study the relationship between two variables</strong>, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, by defining a mathematical model <span class="math notranslate nohighlight">\(f\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[Y=f(X) + \epsilon\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f\)</span> is a deterministic function which can be used to <strong>predict the values of <span class="math notranslate nohighlight">\(Y\)</span> from the values of <span class="math notranslate nohighlight">\(X\)</span></strong>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is an <strong>error term</strong>, i.e., a variable capturing everything that is not captured by the deterministic function <span class="math notranslate nohighlight">\(f\)</span>. It can be due to different reasons, the main of which are:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f\)</span> is not an accurate deterministic function of the process. Since we don’t know the “true” function <span class="math notranslate nohighlight">\(f\)</span> and we are only estimating it, we may obtain a suboptimal <span class="math notranslate nohighlight">\(f\)</span> for which <span class="math notranslate nohighlight">\(Y \neq f(X)\)</span>. The error term captures the differences between our predictions and the true values.</p></li>
<li><p><span class="math notranslate nohighlight">\(Y\)</span> cannot only be predicted from <span class="math notranslate nohighlight">\(X\)</span>, but some other variable is needed to correctly predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X\)</span>. For instance, <span class="math notranslate nohighlight">\(X\)</span> could be “years of education” and <span class="math notranslate nohighlight">\(Y\)</span> can be “income”. While may expect that “income” is not completely predicted from “years of education”. This can happen also because we don’t always have observations for all relevant variables.</p></li>
<li><p>the problem has inherent stochasticity which cannot be entirely modeled within the deterministic function <span class="math notranslate nohighlight">\(f\)</span>. For instance, consider the problem of predicting the rate of wins in poker based on the expertise of the player. The expertise surely allows to predict the rate of wins, but wins partially depend also on random factors, such as how the deck was shuffled.</p></li>
</ul>
</li>
</ul>
<p>Note that, often, we model <span class="math notranslate nohighlight">\(f\)</span> in a way that we have its <strong>analytical form</strong>. This is very powerful. If we have the analytical form of the function <span class="math notranslate nohighlight">\(f\)</span> which <strong>explains</strong> how <span class="math notranslate nohighlight">\(Y\)</span> is influenced from <span class="math notranslate nohighlight">\(X\)</span> (<strong>can be predicted from <span class="math notranslate nohighlight">\(X\)</span></strong>), then we can really understand deeply the connection between the two variables!</p>
<p>The function <span class="math notranslate nohighlight">\(f\)</span> can take different forms. The most common one is the <strong>linear form</strong> that we will see in the next section. While the linear form is very simple (and hence we can anticipate it will be a limited model in many cases), it has the great advantage to be <strong>easy to interpret</strong>.</p>
</section>
<section id="simple-linear-regression">
<h2><span class="section-number">18.3. </span>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Simple linear regression aims to model the <strong>linear relationship</strong> between two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. In our example dataset, we will consider <span class="math notranslate nohighlight">\(X=\text{horsepower}\)</span> and <span class="math notranslate nohighlight">\(Y=\text{mpg}\)</span>.</p>
<p>Since we are trying to model a linear relationship, we can imagine <strong>a line passing through the data</strong>. The simple linear regression model is defined as:</p>
<div class="math notranslate nohighlight">
\[Y \approx \beta_0 + \beta_1X\]</div>
<p>In our example:</p>
<div class="math notranslate nohighlight">
\[\text{mpg} \approx \beta_0 + \beta_1 \text{horsepower}\]</div>
<p>It is often common to introduce a <strong>“noise” variable</strong> which captures the randomness due to which the expression above is approximated and write:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1X + \epsilon\]</div>
<p>As we will see later, we expect <span class="math notranslate nohighlight">\(\epsilon\)</span> to be <strong>small and randomly distributed</strong>.</p>
<p>Given the model above, we will call:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span>, the <strong>dependent variable</strong> or <strong>regressor</strong>;</p></li>
<li><p><span class="math notranslate nohighlight">\(Y\)</span>, the <strong>independent variable</strong> or <strong>regressed variable</strong>.</p></li>
</ul>
<p>The values <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are called <strong>coefficients</strong> or <strong>parameters</strong> of the model.</p>
<p>The mathematical model above has a geometrical interpretation. Indeed, specific values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> identify a given line in the 2D plane, as shown in the plot below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/2a666893895881e826c843f1cc97b4d2e6292b30af6f5084f9d4aeb041ec1fb6.png" src="../_images/2a666893895881e826c843f1cc97b4d2e6292b30af6f5084f9d4aeb041ec1fb6.png" />
</div>
</div>
<p>We hence aim to estimate two appropriate values <span class="math notranslate nohighlight">\(\hat \beta_0\)</span> and <span class="math notranslate nohighlight">\(\hat \beta_1\)</span> from data in a way that they provide a model which represents well our data. In the case of our example, we expect the geometrical model to have this aspect:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/528afcac8e04ed0389165862a00453351b8a1980d8f1adb134e6947820a6de46.png" src="../_images/528afcac8e04ed0389165862a00453351b8a1980d8f1adb134e6947820a6de46.png" />
</div>
</div>
<p>This line will also be called the <strong>regression line</strong>.</p>
<section id="estimating-the-coefficients-ordinary-least-squares-ols">
<h3><span class="section-number">18.3.1. </span>Estimating the Coefficients - Ordinary Least Squares (OLS)<a class="headerlink" href="#estimating-the-coefficients-ordinary-least-squares-ols" title="Permalink to this heading">#</a></h3>
<p>To estimate the coefficients of our optimal model, we should first define what is a good model. We will say that <strong>a good model is one that predicts well the <span class="math notranslate nohighlight">\(Y\)</span> variable from the <span class="math notranslate nohighlight">\(X\)</span> one</strong>. We already know from the example above that, since the relationship is not perfectly linear, the model will make some mistakes.</p>
<p>Let <span class="math notranslate nohighlight">\(\{(x_i,y_i)\}\)</span> be our set of observations. Let</p>
<div class="math notranslate nohighlight">
\[\hat y_i = \hat \beta_0 + \hat \beta_1 x_i\]</div>
<p>be the prediction of the model for the observation <span class="math notranslate nohighlight">\(x_i\)</span>. For each data point <span class="math notranslate nohighlight">\((x_i,y_i)\)</span>, we will define the deviation of the prediction from the <span class="math notranslate nohighlight">\(y_i\)</span> value as follows:</p>
<div class="math notranslate nohighlight">
\[e_i = y_i - \hat y_i\]</div>
<p>These numbers will be positive or negative based on whether we underestimate or overestimate the <span class="math notranslate nohighlight">\(y_i\)</span> values. As a global error indicator for the model, given the data, we will define the <strong>residual sum of squares (RSS)</strong> as:</p>
<div class="math notranslate nohighlight">
\[RSS = e_1^2 + e_2^2 + \ldots + e_n^2 \]</div>
<p>or equivalently:</p>
<div class="math notranslate nohighlight">
\[RSS = (y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2 + \ldots + (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2\]</div>
<p>This number will be the sum of the square values of the dashed segments in the plot below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/f46d194f5a10e9b2e959291a111eb37f0c9614f92203318a61c27a0b269ed329.png" src="../_images/f46d194f5a10e9b2e959291a111eb37f0c9614f92203318a61c27a0b269ed329.png" />
</div>
</div>
<p>Intuitively, if we minimize these numbers, we will find the line which <strong>best fits the data</strong>.</p>
<p>We can obtain estimates for <span class="math notranslate nohighlight">\(\hat \beta_0\)</span> and <span class="math notranslate nohighlight">\(\hat \beta_1\)</span> by minimizing the RSS using an approach called <strong>ordinary least squares</strong>.</p>
<p>We can write the RSS as a function of the parameters to estimate:</p>
<div class="math notranslate nohighlight">
\[RSS(\beta_0, \beta_1) = \sum_{i=1}^n(y_i - \beta_0 -  \beta_1 x_i)^2 \]</div>
<p>This is also called a <strong>cost function</strong> or <strong>loss function</strong>.</p>
<p>We aim to find:</p>
<div class="math notranslate nohighlight">
\[(\hat \beta_0, \hat \beta_1) = \arg \min_{\beta_0, \beta_1} RSS(\beta_0, \beta_1)\]</div>
<p>The minimum can be found setting:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS(\beta_0, \beta_1)}{\partial \beta_0} = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS(\beta_0, \beta_1)}{\partial \beta_1} = 0\]</div>
<p>Doing the math, it can be shown that:</p>
<div class="math notranslate nohighlight">
\[\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^n(x_i - \overline x)}\]</div>
<div class="math notranslate nohighlight">
\[\hat \beta_0 = \overline y - \hat \beta_1 \overline x\]</div>
</section>
<section id="interpretation-of-the-coefficients-of-linear-regression">
<h3><span class="section-number">18.3.2. </span>Interpretation of the Coefficients of Linear Regression<a class="headerlink" href="#interpretation-of-the-coefficients-of-linear-regression" title="Permalink to this heading">#</a></h3>
<p>Using the formulas above, we find the following values for the example above:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>beta_0: 39.94
beta_1: -0.16
</pre></div>
</div>
</div>
</div>
<p>These parameters identify the following line:</p>
<div class="math notranslate nohighlight">
\[y = 39.94 - 0.15 x\]</div>
<p>The plot below shows the line on the data:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/2eadbea9271f07cbcd11ded62deaf40105d594525ded3a7b279610e5c4adcb15.png" src="../_images/2eadbea9271f07cbcd11ded62deaf40105d594525ded3a7b279610e5c4adcb15.png" />
</div>
</div>
<p>Apart from the geometric interpretation, the coefficients of a linear regressor have an important <strong>statistical interpretation</strong>. In particular:</p>
<p><strong>The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> is the value of <span class="math notranslate nohighlight">\(y\)</span> that we get when the input value <span class="math notranslate nohighlight">\(x\)</span> is equal to zero <span class="math notranslate nohighlight">\(x=0\)</span> (i.e., <span class="math notranslate nohighlight">\(f(0)\)</span>)</strong>. This value <strong>may not always make sense</strong>. For instance, in the example above, we have: <span class="math notranslate nohighlight">\(\beta_0 = 39.94\)</span>, which means that, <strong>when the horsepower is <span class="math notranslate nohighlight">\(0\)</span>, then the consumption in mpg is equal to <span class="math notranslate nohighlight">\(39.94\)</span></strong>.</p>
<p><strong>The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> indicates the steepness of the curve</strong>. If <span class="math notranslate nohighlight">\(\beta_1\)</span> is large, then the curve is steep. This indicates that a small change in <span class="math notranslate nohighlight">\(x\)</span> is associates to a large change in <span class="math notranslate nohighlight">\(y\)</span>. In general, we can see that:</p>
<div class="math notranslate nohighlight">
\[f(x+1)-f(x)=\beta_0+\beta_1 (x+1)-\beta_0-\beta_1 x=\beta_1 (x+1-x)=\beta_1\]</div>
<p>which reveals that <strong>when we observe an increment of one unit of x, we observe an increment of <span class="math notranslate nohighlight">\(\beta_1\)</span> units in y</strong>. In our example, <span class="math notranslate nohighlight">\(\beta_1=-0.15\)</span>, hence we can say that, for cars with one additional unit of horsepower, we observe an drop in mps <span class="math notranslate nohighlight">\(-0.15\)</span> units.</p>
</section>
<section id="accuracy-of-the-coefficient-estimates">
<h3><span class="section-number">18.3.3. </span>Accuracy of the Coefficient Estimates<a class="headerlink" href="#accuracy-of-the-coefficient-estimates" title="Permalink to this heading">#</a></h3>
<p>Recall that we are trying to model the relationship between two random variables <span class="math notranslate nohighlight">\(X\)</span> as <span class="math notranslate nohighlight">\(Y\)</span> with a simple linear model:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1X+\epsilon\]</div>
<p>This means that, once we find appropriate values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, we expect these to summarize the <strong>linear relationship in the population</strong> or the <strong>population regression line</strong>. Also, recall that these values are obtained using two formulas which are based on realizations of <span class="math notranslate nohighlight">\(X\)</span> of <span class="math notranslate nohighlight">\(Y\)</span> and can be hence seen as <strong>estimators</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^n(x_i - \overline x)^2}\]</div>
<div class="math notranslate nohighlight">
\[\hat \beta_0 = \overline y - \hat \beta_1 \overline x\]</div>
<p>We now recall that, being estimates, they provide values related to a given <strong>realization of the random variables</strong>.</p>
<p>Let us consider an ideal population for which:</p>
<div class="math notranslate nohighlight">
\[Y=2x+1\]</div>
<p>Ideally, given a sample from the population, we expect to obtain <span class="math notranslate nohighlight">\(\hat \beta_0 \approx 1\)</span> and <span class="math notranslate nohighlight">\(\hat \beta_1 \approx 2\)</span>. In practice, different samples may lead to different estimates and hence different regression lines, as shown in the plot below:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c4fd62442231f408f86ccbad547786768adda153ce75381ddf95623eb21c5f08.png" src="../_images/c4fd62442231f408f86ccbad547786768adda153ce75381ddf95623eb21c5f08.png" />
</div>
</div>
<p>Each of the first three subplots shows a different sample drawn from the population, with its corresponding estimated regression line, along with the true population regression line. The last subplot compares the different estimated lines with the population regression line and the average regression line (in red). Coefficient estimates are shown in the subplot titles.</p>
<p>As can be noted, each estimate can be inaccurate, while the average regression line is very close to the population regression line. This is due to the fact that our estimators for the parameters of the regression coefficients <strong>have non-zero variance</strong>. In practice, in can be shown that these estimators are <strong>unbiased</strong> (hence the average regression line is close to the population line).</p>
<section id="standard-errors-of-the-regression-coefficients">
<h4><span class="section-number">18.3.3.1. </span>Standard Errors of the Regression Coefficients<a class="headerlink" href="#standard-errors-of-the-regression-coefficients" title="Permalink to this heading">#</a></h4>
<p>A natural question is hence <strong>“how do we assess how good our estimates of the regression coefficients are”</strong>. We resort here to <strong>statistical inference</strong>. It can be shown that the squared standard errors associated to the coefficient estimates are:</p>
<div class="math notranslate nohighlight">
\[SE(\hat \beta_0)^2 = \sigma^2 \left[\frac{1}{n} + \frac{\overline x^2}{\sum_{i=1}^{n}(x_i-\overline x)^2}\right], SE(\hat \beta_1)^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\overline x)^2}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = Var(\epsilon)\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\sigma^2\)</span> is generally unknown, but it can be estimated as the <strong>residual standard error</strong>:</p>
<div class="math notranslate nohighlight">
\[RSE = \sqrt{\frac{RSS}{n-2}}\]</div>
<p>In the formulas above, we see that:</p>
<ul class="simple">
<li><p>The standard errors are proportional to <span class="math notranslate nohighlight">\(\sigma^2\)</span>. This is expected, as we will have more uncertainty when the variance of the error term is high, hence when the points are more <strong>scattered around the population regression line</strong>.</p></li>
<li><p>The standard errors depend inversely on <span class="math notranslate nohighlight">\(n\sigma_x^2 = \sum_{i=1}^n(x_i-\overline x)\)</span> (the variance of <span class="math notranslate nohighlight">\(x\)</span> multiplied by the sample size). This means that we will have more uncertainty in the estimates if <span class="math notranslate nohighlight">\(x\)</span> concentrate in a narrow range.</p></li>
</ul>
</section>
<section id="confidence-intervals-of-the-regression-coefficients">
<h4><span class="section-number">18.3.3.2. </span>Confidence Intervals of the Regression Coefficients<a class="headerlink" href="#confidence-intervals-of-the-regression-coefficients" title="Permalink to this heading">#</a></h4>
<p>As we have seen, standard errors allow to compute confidence intervals. We will not see it into details, but it turns out that the true values will be in the following intervals with a <span class="math notranslate nohighlight">\(95\%\)</span> confidence:</p>
<div class="math notranslate nohighlight">
\[[\hat \beta_0 - 1.96 \cdot SE(\hat \beta_0), \hat \beta_0 + 1.96 \cdot SE(\hat \beta_0)]\]</div>
<div class="math notranslate nohighlight">
\[[\hat \beta_1 - 1.96 \cdot SE(\hat \beta_1), \hat \beta_1 + 1.96 \cdot SE(\hat \beta_1)]\]</div>
<p>In practice, it is common to compute confidence intervals with a <span class="math notranslate nohighlight">\(95\%\)</span> confidence. In the case of our model:</p>
<div class="math notranslate nohighlight">
\[horsepower \approx = \beta_0 + mpg \cdot beta_1\]</div>
<p>We will have:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>COEFFICIENT</p></th>
<th class="head"><p>STD ERROR</p></th>
<th class="head"><p>CONFIDENCE INTERVAL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p>39.94</p></td>
<td><p>0.717</p></td>
<td><p><span class="math notranslate nohighlight">\([38.53, 41.35]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p>-0.1578</p></td>
<td><p>0.006</p></td>
<td><p><span class="math notranslate nohighlight">\([-0.17, -0.15]\)</span></p></td>
</tr>
</tbody>
</table>
<p>From the table above, we can say that:</p>
<ul class="simple">
<li><p>The value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> for <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> lies somewhere between <span class="math notranslate nohighlight">\(38.53\)</span> and <span class="math notranslate nohighlight">\(41.35\)</span>;</p></li>
<li><p>An increase of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> by one unit is associated to an decrease of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> between <span class="math notranslate nohighlight">\(-0.17\)</span> and <span class="math notranslate nohighlight">\(.015\)</span>.</p></li>
</ul>
<p>It is also common to see plots like the following one:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/99a1dc89c41e1e1f1fe741d100d9b713250994651e2042407614d2a881468af8.png" src="../_images/99a1dc89c41e1e1f1fe741d100d9b713250994651e2042407614d2a881468af8.png" />
</div>
</div>
<p>In the plot above, the shading around the line illustrates the variability induced by the confidence intervals estimated for the coefficients.</p>
</section>
<section id="statistical-tests-for-the-significance-of-coefficients">
<h4><span class="section-number">18.3.3.3. </span>Statistical Tests for the Significance of Coefficients<a class="headerlink" href="#statistical-tests-for-the-significance-of-coefficients" title="Permalink to this heading">#</a></h4>
<p>Standard errors are also used to perform <strong>hypothesis tests</strong> on the coefficients. In practice, it is common to perform a statistical test to assess whether the coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> is significantly different from zero. It is interesting to check this because, if <span class="math notranslate nohighlight">\(\beta_1\)</span> was equal to zero, then there would not be any correlation between the variables (and hence the linear regressor would not be useful). Indeed, if <span class="math notranslate nohighlight">\(\beta-1=0\)</span>:</p>
<div class="math notranslate nohighlight">
\[Y=\beta_0 + \epsilon\]</div>
<p>Hence <span class="math notranslate nohighlight">\(Y\)</span> cannot be predicted from <span class="math notranslate nohighlight">\(X\)</span> and the two variables are not associated.</p>
<p>The <strong>null hypothesis</strong> of the test is as follows:</p>
<div class="math notranslate nohighlight">
\[H_0: \text{There is no association between } X \text{ and } Y \Leftrightarrow \beta_1=0\]</div>
<p>While the <strong>alternative hypothesis</strong> is formulated as follows:</p>
<div class="math notranslate nohighlight">
\[H_1: \text{There is some association between } X \text{ and } Y \Leftrightarrow \beta_1 \neq 0\]</div>
<p>To conduct the test, the following t-statistic is computed from the estimate of <span class="math notranslate nohighlight">\(\beta_1\)</span> and the standard error:</p>
<div class="math notranslate nohighlight">
\[t=\frac{\hat \beta_1 - 0}{SE(\hat \beta_1)}\]</div>
<p>Where the <span class="math notranslate nohighlight">\(-0\)</span> term indicates that we are subtracting the value assumed by the null hypothesis (<span class="math notranslate nohighlight">\(\beta_1=0\)</span>). The statistic will follow a t-Student distribution with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom (<span class="math notranslate nohighlight">\(n\)</span> being the number of data points). If <span class="math notranslate nohighlight">\(n&gt;30\)</span> the distribution is approximately Gaussian. Using this statistic, a <strong><span class="math notranslate nohighlight">\(p-value\)</span></strong> indicating the <strong>probability of observing a t statistic more extreme than this one if there is no associated between the two variables</strong> is computed. Chosen a significance level <span class="math notranslate nohighlight">\(\alpha\)</span> (often <span class="math notranslate nohighlight">\(\alpha=0.05\)</span>), we will reject the null hypothesis if <span class="math notranslate nohighlight">\(p&lt;\alpha\)</span>.</p>
<p>A similar test is conducted to check that <span class="math notranslate nohighlight">\(\beta_0\)</span> is significantly different from zero.</p>
<p>Let’s see the updated table from the same example:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>COEFFICIENT</p></th>
<th class="head"><p>STD ERROR</p></th>
<th class="head"><p>t</p></th>
<th class="head"><p>P&gt;|t|</p></th>
<th class="head"><p>CONFIDENCE INTERVAL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(39.94\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.717\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(55.66\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([38.53, 41.35]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.1578\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.006\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-24.49\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([-0.17, -0.15]\)</span></p></td>
</tr>
</tbody>
</table>
<p>From the table above, we can conclude that both <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are significantly different than <span class="math notranslate nohighlight">\(0\)</span> (p-value is equal to zero). This can also be noted by the fact that the confidence intervals do not contain the zero number.</p>
</section>
</section>
<section id="accuracy-of-the-model">
<h3><span class="section-number">18.3.4. </span>Accuracy of the Model<a class="headerlink" href="#accuracy-of-the-model" title="Permalink to this heading">#</a></h3>
<p>The tests performed above will tell us if there is a relationship between the variables, but they will not tell us <strong>how well does the model fit the data</strong>. For instance, if the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is not linear, we would expect the model not to fit the data very well. In practice, we can use different measures of accuracy of the model.</p>
<section id="residual-standard-error">
<h4><span class="section-number">18.3.4.1. </span>Residual Standard Error<a class="headerlink" href="#residual-standard-error" title="Permalink to this heading">#</a></h4>
<p>One way to measure how well the model fits the data is to check the variance of the residuals <span class="math notranslate nohighlight">\(\epsilon\)</span>. Recall that our model is:</p>
<div class="math notranslate nohighlight">
\[Y=\beta_0 + \beta_1 X + \epsilon\]</div>
<p>If the model fits the data well, then the values of <span class="math notranslate nohighlight">\(\epsilon\)</span> will be close to zero and their variance will be small. We have already seen that the <strong>Residual Sum of Squares (RSS)</strong> is defined as:</p>
<div class="math notranslate nohighlight">
\[RSE = \sqrt{\frac{RSS}{n-2}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n (y_i-\hat y_i)^2}\]</div>
<p>The residual standard error is a measure of the <strong>lack of fit</strong>. Large values will indicate that the model is not a good fit. For instance, in our example we have:</p>
<div class="math notranslate nohighlight">
\[RSE = 4.91\]</div>
<p>This value has to be interpreted depending on the scale of the <span class="math notranslate nohighlight">\(Y\)</span> variable. The average value of <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\overline{mpg} = 23.52\]</div>
<p>So the percentage error will be <span class="math notranslate nohighlight">\(4.91/23.52 \approx 20\%\)</span>.</p>
</section>
<section id="r-2-statistic">
<h4><span class="section-number">18.3.4.2. </span><span class="math notranslate nohighlight">\(R^2\)</span> Statistic<a class="headerlink" href="#r-2-statistic" title="Permalink to this heading">#</a></h4>
<p>The RSE value is an absolute measure, which is measured in the units of <span class="math notranslate nohighlight">\(Y\)</span>. Indeed, to interpret it, we had to check the range of the <span class="math notranslate nohighlight">\(Y\)</span> values. An alternative way to check if the model is fitting the data well, would be to compare the performance of our model with the performance of a <strong>baseline model</strong> which assumes no association between the <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> variables. This model would be:</p>
<div class="math notranslate nohighlight">
\[Y = k\]</div>
<p>This model has a single parameter <span class="math notranslate nohighlight">\(k\)</span>. The <strong>Residual Sum of Squares</strong> of this model would be:</p>
<div class="math notranslate nohighlight">
\[RSS(k)=\sum_{i=1}^n{(k-y_i)^2}\]</div>
<p>To find the optimal value <span class="math notranslate nohighlight">\(k\)</span>, we can compute the derivative of the RSS and set it to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial RSS(k)}{\partial k}=2\sum_{i=1}^n{(k-y_i)} = 2(nk - \sum_{i=1}^n y_i) = 2(nk - n\overline y_i)\]</div>
<div class="math notranslate nohighlight">
\[2n(k - \overline y_i) = 0 \Leftrightarrow k=\overline y_i\]</div>
<p>Hence, the optimal estimator, <strong>when there is not relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is the average value of <span class="math notranslate nohighlight">\(Y\)</span></strong>. We will call its RSS value the <strong>total sum of squares</strong>:</p>
<div class="math notranslate nohighlight">
\[TSS = \sum_{i=1}^n(y_i-\overline y)^2\]</div>
<p>We can compare the RSS value obtained by our model to the TSS, which is the error of the baseline method:</p>
<div class="math notranslate nohighlight">
\[\frac{RSS}{TSS}\]</div>
<p>This number will be <strong>comprised between 0 and 1</strong>, and in particular:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{RSS}{TSS}=0\)</span> when <span class="math notranslate nohighlight">\(RSS=0\)</span>, i.e., we have a perfect model;</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{RSS}{TSS}=1\)</span> when <span class="math notranslate nohighlight">\(RSS=TSS\)</span>, i.e., we are not doing any better than the baseline model (so our model is poor).</p></li>
</ul>
<p>Note that the RSS measures <strong>the variability in <span class="math notranslate nohighlight">\(Y\)</span> left unexplained after regression</strong> (the one that the model could not capture), while the TSS measures <strong>the total variability in <span class="math notranslate nohighlight">\(Y\)</span></strong>. The fraction hence explains the <strong>proportion of variability which the model could not explain</strong>.</p>
<p>We define the <span class="math notranslate nohighlight">\(R^2\)</span> statistic as:</p>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \frac{RSS}{TSS}\]</div>
<p>Inverting by the <span class="math notranslate nohighlight">\(1-\)</span> subtraction, this number measures the <strong>proportion of variability in <span class="math notranslate nohighlight">\(Y\)</span> that can be explained using <span class="math notranslate nohighlight">\(X\)</span></strong></p>
<p>The plot below shows examples of linear regression fits with different <span class="math notranslate nohighlight">\(R^2\)</span> values.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/3d62b26713ecd8bf2fc4bb00fdd1e4f749a313553f209878086fc881d42a9656.png" src="../_images/3d62b26713ecd8bf2fc4bb00fdd1e4f749a313553f209878086fc881d42a9656.png" />
</div>
</div>
<p>The interpretation of the <span class="math notranslate nohighlight">\(R^2\)</span> is related to the one of the Pearson’s <span class="math notranslate nohighlight">\(\rho\)</span> correlation coefficient. Indeed, both scores are independent of the slope of the regression line, but quantify how liner the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is. In practice, it can be shown that:</p>
<div class="math notranslate nohighlight">
\[R^2 = \rho^2\]</div>
<p>The <span class="math notranslate nohighlight">\(R^2\)</span> value in our example of regressing <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is:</p>
<div class="math notranslate nohighlight">
\[R^2 = 0.61\]</div>
<p>Which indicates that <span class="math notranslate nohighlight">\(61\%\)</span> of the variance of the <span class="math notranslate nohighlight">\(Y\)</span> variable is explained by the model.</p>
</section>
</section>
</section>
<section id="multiple-linear-regression">
<h2><span class="section-number">18.4. </span>Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this heading">#</a></h2>
<p>In the example above, we have seen that about <span class="math notranslate nohighlight">\(61\%\)</span> of the variance in <span class="math notranslate nohighlight">\(Y\)</span> was explained by the model. One may wonder why about <span class="math notranslate nohighlight">\(39\%\)</span> of the variance could not be explained. Some common reasons may be:</p>
<ul class="simple">
<li><p>There is stochasticity in the data which prevents us to learn an accurate function to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X\)</span>;</p></li>
<li><p>The relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is far from linear, so we cannot predict <span class="math notranslate nohighlight">\(Y\)</span> accurately;</p></li>
<li><p>The prediction of <span class="math notranslate nohighlight">\(Y\)</span> also depends on other variables.</p></li>
</ul>
<p>While in general the unexplained variance is due to a combination of the aforementioned factors, the third one is often very common and relatively easy to fix. In our case, we are trying to predict <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>. However, we can easily imagine how other variables may contribute to the estimation of <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. For instance, two cars with the same <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> but different <code class="docutils literal notranslate"><span class="pre">weight</span></code> may have different values of <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. We can hence try to find a model with <strong>also uses <code class="docutils literal notranslate"><span class="pre">weight</span></code> to predict <code class="docutils literal notranslate"><span class="pre">mpg</span></code></strong>. This is simply done by adding an additional coefficient for the new variable:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\]</div>
<p>The obtained model is called <strong>multiple linear regression</strong>. If we fit this model (we will see how to estimate coefficients in this case), we obtain the following <span class="math notranslate nohighlight">\(R^2\)</span> value:</p>
<div class="math notranslate nohighlight">
\[R^2=0.71\]</div>
<p>An increment of <span class="math notranslate nohighlight">\(+0.1\)</span>!</p>
<p>In general, we can include as many variables as we think is relevant to add and define the following model:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_n X_n\]</div>
<p>For instance, the following model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight + \beta_3 model\_year\]</div>
<p>Has an <span class="math notranslate nohighlight">\(R^2\)</span> value of:</p>
<div class="math notranslate nohighlight">
\[R^2=0.808\]</div>
<section id="geometrical-interpretation">
<h3><span class="section-number">18.4.1. </span>Geometrical Interpretation<a class="headerlink" href="#geometrical-interpretation" title="Permalink to this heading">#</a></h3>
<p>The multiple regression model based on two variable has a geometrical interpretation. Indeed, the equation:</p>
<div class="math notranslate nohighlight">
\[Z = \beta_0 + \beta_1 X + \beta_2 Y\]</div>
<p>identifies a plane in the 3D space. We can visualize the plane identified by the <span class="math notranslate nohighlight">\(mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\)</span> model as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/a9a15334cb0d1ea09bbf96f4ab8ac2ec1d66efe826bd3ec4736af6d7eb2e9f86.png" src="../_images/a9a15334cb0d1ea09bbf96f4ab8ac2ec1d66efe826bd3ec4736af6d7eb2e9f86.png" />
</div>
</div>
<p>The dashed lines indicate the residuals. The best fit of the model minimizes again the sum of squared residuals. This model makes predictions selecting the <span class="math notranslate nohighlight">\(Z\)</span> value which intersect the plane for given values of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>In general, when we consider <span class="math notranslate nohighlight">\(n\)</span> variables, the linear regressor will be <strong>a (n-1)-dimensional hyperplane in the n-dimensional space</strong>.</p>
</section>
<section id="statistical-interpretation">
<h3><span class="section-number">18.4.2. </span>Statistical Interpretation<a class="headerlink" href="#statistical-interpretation" title="Permalink to this heading">#</a></h3>
<p>The statistical interpretation of a multiple linear regression model is very similar to the interpretation of a simple linear regression model. Given the general model:</p>
<div class="math notranslate nohighlight">
\[y=\beta_0 + \beta_1 x_1 + \ldots + \beta_i x_i + \ldots + \beta_n x_n\]</div>
<p>we can interpret the coefficients as follows:</p>
<ul class="simple">
<li><p>The value of <span class="math notranslate nohighlight">\(\beta_0\)</span> indicates the value of <span class="math notranslate nohighlight">\(y\)</span> when all independent variables are set to zero;</p></li>
<li><p>The value of <span class="math notranslate nohighlight">\(\beta_i\)</span> indicates the increment of <span class="math notranslate nohighlight">\(y\)</span> that we expect to see when <span class="math notranslate nohighlight">\(x_i\)</span> increments by one unit, <strong>provided that all other values <span class="math notranslate nohighlight">\(x_j | j\neq i\)</span> are constant</strong>.</p></li>
</ul>
<p>In the considered example:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\]</div>
<p>we obtain the following estimates for the coefficients:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_2\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(45.64\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.05\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.01\)</span></p></td>
</tr>
</tbody>
</table>
<p>We can interpret these estimates as follows:</p>
<ul class="simple">
<li><p>Cars with zero <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and zero <code class="docutils literal notranslate"><span class="pre">weight</span></code> will have an <code class="docutils literal notranslate"><span class="pre">mpg</span></code> of <span class="math notranslate nohighlight">\(45.64\)</span> (<span class="math notranslate nohighlight">\(\approx 19.4 Km/l\)</span>).</p></li>
<li><p>An increment of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is associated to a decrement of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> of <span class="math notranslate nohighlight">\(-0.05\)</span> units, provided that <code class="docutils literal notranslate"><span class="pre">weight</span></code> is constant. This makes sense: cars with more <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> will probably consume more fuel.</p></li>
<li><p>An increment of one unit of <code class="docutils literal notranslate"><span class="pre">weight</span></code> is associated to a decrement of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> of <code class="docutils literal notranslate"><span class="pre">-0.01</span></code> units, provided that <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is constant. This makes sense: heavier cars will consume more fuel.</p></li>
</ul>
<p>Let’s compare the estimates above with the estimates of our previous model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower\]</div>
<p>In that case, we obtained:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\hat \beta_1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(39.94\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.16\)</span></p></td>
</tr>
</tbody>
</table>
<p>We can note that the coefficients are different. This happens because, when we add more variables, <strong>the model explains variance in a different way</strong>. If we think more about it, this is coherent with the interpretation of the coefficients. Indeed:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(39.94\)</span> is the expected value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code>, but all other variables have unknown values. <span class="math notranslate nohighlight">\(45.64\)</span> is the expected value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> and <code class="docutils literal notranslate"><span class="pre">weight=0</span></code>. This is different, as in the second case we are (virtually) looking at a subset of data for which both horsepower and weight are zero, while in the first case, we are only looking at data for which <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code>, but <code class="docutils literal notranslate"><span class="pre">weight</span></code> can be any value. In some sense, we can see <span class="math notranslate nohighlight">\(39.94\)</span> as an average value for different values of <code class="docutils literal notranslate"><span class="pre">weight</span></code> (and all other unobserved variables).</p></li>
<li><p><span class="math notranslate nohighlight">\(-0.16\)</span> is the expected increment of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when we observe an increment of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and we don’t know anything about the values of the other variables. <span class="math notranslate nohighlight">\(-0.05\)</span> is the expected increment of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> are held constant, so, again, we are (virtually) looking at a different subset of the data in which the relationship between <code class="docutils literal notranslate"><span class="pre">mpg</span></code> and <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> may be a bit different.</p></li>
</ul>
<p>Note that, also in the case of multiple regression, we can estimate confidence intervals and perform statistical tests. In our example, we will get this table:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>COEFFICIENT</p></th>
<th class="head"><p>STD ERROR</p></th>
<th class="head"><p>t</p></th>
<th class="head"><p>P&gt;|t|</p></th>
<th class="head"><p>CONFIDENCE INTERVAL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(45.64\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.793\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(57.54\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([44.08, 47.20]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.05\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.011\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-4.26\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([-0.07, -0.03]\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-0.01\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0.001\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-11.53\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([-0.007, -0.005]\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="estimating-the-regression-coefficients">
<h3><span class="section-number">18.4.3. </span>Estimating the Regression Coefficients<a class="headerlink" href="#estimating-the-regression-coefficients" title="Permalink to this heading">#</a></h3>
<p>Given the general model:</p>
<div class="math notranslate nohighlight">
\[y=\beta_0 + \beta_1 x_1 + \ldots + \beta_i x_i + \ldots + \beta_n x_n\]</div>
<p>We can define our <strong>cost function</strong> again as the residual sum of squares:</p>
<div class="math notranslate nohighlight">
\[RSS(\beta_0,\ldots,\beta_n) = \sum_{i=1}^n (y_i - \beta_0+\beta_1x_i + \ldots + \beta_n x_n)^2\]</div>
<p>The values <span class="math notranslate nohighlight">\(\hat \beta_0,\ldots,\hat \beta_n\)</span> values that minimize the loss function above are the <strong>multiple least square coefficient estimates</strong>.</p>
<p>To find these optimal values, it is convenient to use matrix notation. Given <span class="math notranslate nohighlight">\(m\)</span> observations, we will have <span class="math notranslate nohighlight">\(m\)</span> equations:</p>
<div class="math notranslate nohighlight">
\[y^{(1)} = \beta_0 + \beta_1 x_1^{(1)} + \ldots + \beta_n x_n^{(1)} + e^{(1)}\]</div>
<div class="math notranslate nohighlight">
\[y^{(2)} = \beta_0 + \beta_1 x_1^{(2)} + \ldots + \beta_n x_n^{(2)} + e^{(2)}\]</div>
<div class="math notranslate nohighlight">
\[\ldots\]</div>
<div class="math notranslate nohighlight">
\[y^{(m)} = \beta_0 + \beta_1 x_1^{(m)} + \ldots + \beta_n x_n^{(m)} + e^{(m)}\]</div>
<p>We can write the <span class="math notranslate nohighlight">\(m\)</span> equations in matrix form as follows:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{e}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{y} = \begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix},
\mathbf{X}=
\begin{bmatrix}
1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \ldots &amp; x_n^{(1)} \\
1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \ldots &amp;  x_n^{(2)} \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_1^{(m)} &amp; x_2^{(m)} &amp; \ldots &amp;  x_n^{(m)} \\
\end{bmatrix},
\mathbf{\beta} = \begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{n}
\end{bmatrix},
\mathbf{e} = \begin{bmatrix}
e^{(1)} \\
e^{(2)} \\
\vdots \\
e^{(m)}
\end{bmatrix}
\end{split}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is called the <strong>design matrix</strong>.</p>
<p>In the notation above, we want to minimize:</p>
<div class="math notranslate nohighlight">
\[RSS(\mathbf{\beta}) = \sum_{i=1}^m (e^{(i)})^2 = \mathbf{e}^T \mathbf{e}\]</div>
<p>It can be shown that, by the <strong>least squares method</strong>, the RSS is minimized by the estimate:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\hat \beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</div>
</section>
<section id="the-f-test">
<h3><span class="section-number">18.4.4. </span>The F-Test<a class="headerlink" href="#the-f-test" title="Permalink to this heading">#</a></h3>
<p>When fitting a multiple linear regressor, it is common to perform a statistical test to check whether at least one of the regression coefficients is significantly different from zero (in the population). This test is called an <span class="math notranslate nohighlight">\(F-test\)</span>. We define the <strong>null and alternative hypotheses</strong> as follows:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1=\beta_2=\ldots=\beta_n=0\]</div>
<div class="math notranslate nohighlight">
\[H_a: \exists j\ s.t.\ \beta_j \neq 0\]</div>
<p>The null hypothesis (the one we want to reject with this test) is that all coefficients are zero in the population. If this is true, than the multiple regressor is not reliable and we should discard it. The alternative hypothesis is that at least one of the coefficients is different from zero.</p>
<p>The test is performed by computing the following F-statistic:</p>
<div class="math notranslate nohighlight">
\[F=\frac{(TSS-RSS)/n}{RSS(m-n-1)}\]</div>
<p>Where recall that <span class="math notranslate nohighlight">\(n\)</span> is the number of variables and <span class="math notranslate nohighlight">\(m\)</span> is the number of observations.</p>
<p>In practice, the F-statistic will be:</p>
<ul class="simple">
<li><p>Close to <span class="math notranslate nohighlight">\(1\)</span> if there is no relationship between the response and the predictors (<span class="math notranslate nohighlight">\(H_0\)</span> is true);</p></li>
<li><p>Greater than <span class="math notranslate nohighlight">\(1\)</span> if <span class="math notranslate nohighlight">\(H_a\)</span> is true.</p></li>
</ul>
<p>The test is carried out as usual, finding a p-value which indicates the probability to observe a statistic larger than the observed one if all regression coefficients are zero in the population.</p>
<p>In our example of regressing <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code>, we will find:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>Prob(F-statistic)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.706</p></td>
<td><p>467.9</p></td>
<td><p>3.06e-104</p></td>
</tr>
</tbody>
</table>
<p>This indicates that the regressor is statistically relevant. The <span class="math notranslate nohighlight">\(F-statistic\)</span> is much larger than <span class="math notranslate nohighlight">\(1\)</span> and the p-value (Prob(F-statistic)) is very small (under the significance level <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>).</p>
</section>
<section id="variable-selection">
<h3><span class="section-number">18.4.5. </span>Variable Selection<a class="headerlink" href="#variable-selection" title="Permalink to this heading">#</a></h3>
<p>Let’s now try to fit a multiple linear regressor on our dataset by including all variables. Our dependent variable will be <code class="docutils literal notranslate"><span class="pre">mpg</span></code>, while the set of dependent variables will be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;displacement&#39; &#39;cylinders&#39; &#39;horsepower&#39; &#39;weight&#39; &#39;acceleration&#39;
 &#39;model_year&#39; &#39;origin&#39;]
</pre></div>
</div>
</div>
</div>
<p>We obtain the following measures of fit:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>Prob(F-statistic)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.821</p></td>
<td><p>252.4</p></td>
<td><p>2.04e-139</p></td>
</tr>
</tbody>
</table>
<p>The regressor has a good <span class="math notranslate nohighlight">\(R^2\)</span> and the p-value of the F-test is very small. We can conclude that there is some relationship between the independent variables and the dependent one.</p>
<p>The estimates of the regression coefficients will be:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -17.2184</td> <td>    4.644</td> <td>   -3.707</td> <td> 0.000</td> <td>  -26.350</td> <td>   -8.087</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0170</td> <td>    0.014</td> <td>   -1.230</td> <td> 0.220</td> <td>   -0.044</td> <td>    0.010</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0065</td> <td>    0.001</td> <td>   -9.929</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.005</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0199</td> <td>    0.008</td> <td>    2.647</td> <td> 0.008</td> <td>    0.005</td> <td>    0.035</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.4934</td> <td>    0.323</td> <td>   -1.526</td> <td> 0.128</td> <td>   -1.129</td> <td>    0.142</td>
</tr>
<tr>
  <th>acceleration</th> <td>    0.0806</td> <td>    0.099</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.114</td> <td>    0.275</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7508</td> <td>    0.051</td> <td>   14.729</td> <td> 0.000</td> <td>    0.651</td> <td>    0.851</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4261</td> <td>    0.278</td> <td>    5.127</td> <td> 0.000</td> <td>    0.879</td> <td>    1.973</td>
</tr>
</table></div></div>
</div>
<p>From the table, we can see that not all predictors have a p-value below the significance level <span class="math notranslate nohighlight">\(\alpha=0.05\)</span>. In particular:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">horsepower</span></code> has a large p-value of <span class="math notranslate nohighlight">\(0.22\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cylinders</span></code> has a large p-value of <span class="math notranslate nohighlight">\(0.128\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">acceleration</span></code> has a large p-value of <span class="math notranslate nohighlight">\(0.415\)</span>.</p></li>
</ul>
<p>This means that, within the current regressor, there is no meaningful relationship between these variables and <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. A legitimate question is</p>
<blockquote>
<div><p>How is it possible that <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is not associated to <code class="docutils literal notranslate"><span class="pre">mpg</span></code> in this regressor if it was associated to it before?!</p>
</div></blockquote>
<p>However, we should recall that, when we consider a different set of variables, the interpretation of the coefficients changes. So, even if in the previous models, <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> was correlated to <code class="docutils literal notranslate"><span class="pre">mpg</span></code>, now it is not correlated anymore. We can imagine that the relationship between these variables is now explained by the other variables which we have introduced.</p>
<p>Even if the model is statistically significant, it does make sense to get rid of the variables with poor relationships with <code class="docutils literal notranslate"><span class="pre">mpg</span></code>. After all, if we remove a variable, the estimates of the other coefficients may change.</p>
<p>A common way to remove these variables is by <strong>backward selection</strong> or <strong>backward elimination</strong>. This consists in iteratively removing the variable with the largest p-value. We remove one variable at a time and re-compute the results, iterating until all variables have a small p-value.</p>
<p>let’s start by removing <code class="docutils literal notranslate"><span class="pre">acceleration</span></code>. This is the result:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -15.5635</td> <td>    4.175</td> <td>   -3.728</td> <td> 0.000</td> <td>  -23.773</td> <td>   -7.354</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0239</td> <td>    0.011</td> <td>   -2.205</td> <td> 0.028</td> <td>   -0.045</td> <td>   -0.003</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0062</td> <td>    0.001</td> <td>  -10.883</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.005</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0193</td> <td>    0.007</td> <td>    2.579</td> <td> 0.010</td> <td>    0.005</td> <td>    0.034</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.5067</td> <td>    0.323</td> <td>   -1.570</td> <td> 0.117</td> <td>   -1.141</td> <td>    0.128</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7475</td> <td>    0.051</td> <td>   14.717</td> <td> 0.000</td> <td>    0.648</td> <td>    0.847</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4282</td> <td>    0.278</td> <td>    5.138</td> <td> 0.000</td> <td>    0.882</td> <td>    1.975</td>
</tr>
</table></div></div>
</div>
<p>Note that the coefficients have changed. We now remove <code class="docutils literal notranslate"><span class="pre">cylinders</span></code>, which has the largest p-value of <span class="math notranslate nohighlight">\(0.117\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>  -16.6939</td> <td>    4.120</td> <td>   -4.051</td> <td> 0.000</td> <td>  -24.795</td> <td>   -8.592</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0219</td> <td>    0.011</td> <td>   -2.033</td> <td> 0.043</td> <td>   -0.043</td> <td>   -0.001</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0063</td> <td>    0.001</td> <td>  -11.124</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.005</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0114</td> <td>    0.006</td> <td>    2.054</td> <td> 0.041</td> <td>    0.000</td> <td>    0.022</td>
</tr>
<tr>
  <th>model_year</th>   <td>    0.7484</td> <td>    0.051</td> <td>   14.707</td> <td> 0.000</td> <td>    0.648</td> <td>    0.848</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.3853</td> <td>    0.277</td> <td>    4.998</td> <td> 0.000</td> <td>    0.840</td> <td>    1.930</td>
</tr>
</table></div></div>
</div>
<p>All variables now have an acceptable p-value (<span class="math notranslate nohighlight">\(\alpha=0.05\)</span>). We are done. Note that, by removing the two variables, <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> now has an acceptable p-value. This indicates that one of the removed variables was redundant with respect to <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>.</p>
</section>
<section id="adjusted-r-2">
<h3><span class="section-number">18.4.6. </span>Adjusted <span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#adjusted-r-2" title="Permalink to this heading">#</a></h3>
<p>While in the case of simple regression we saw that <span class="math notranslate nohighlight">\(R^2=\rho(x,y)^2\)</span> (where <span class="math notranslate nohighlight">\(\rho\)</span> is the correlation coefficient), in the case of multiple regression, it turns out that:</p>
<div class="math notranslate nohighlight">
\[R^2 = \rho(Y, \hat Y)^2\]</div>
<p>In general, having more variables in the linear regressor will reduce the error term and improve the covariance between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat Y\)</span>, hence increasing the <span class="math notranslate nohighlight">\(R^2\)</span>. However, in general having a small increase in <span class="math notranslate nohighlight">\(R^2\)</span> when we add a new variable may not be good. Indeed, we could prefer a simpler model with a slightly smaller <span class="math notranslate nohighlight">\(R^2\)</span> value.</p>
<p>To express this, we can compute the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\overline R^2 = 1- \frac{m-1}{m-n-1} R^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(m\)</span> is the number of data points and <span class="math notranslate nohighlight">\(n\)</span> is the number of independent variables. The <span class="math notranslate nohighlight">\(\overline R^2\)</span> re-balances the <span class="math notranslate nohighlight">\(R^2\)</span> accounting for the introduction of additional variables.</p>
<p>For instance the last model we fit has an <span class="math notranslate nohighlight">\(R^2=0.820\)</span> and <span class="math notranslate nohighlight">\(\overline R^2=0.818\)</span>. We will see more in details how to use the <span class="math notranslate nohighlight">\(\overline R^2\)</span> in the laboratory.</p>
</section>
</section>
<section id="qualitative-predictors">
<h2><span class="section-number">18.5. </span>Qualitative Predictors<a class="headerlink" href="#qualitative-predictors" title="Permalink to this heading">#</a></h2>
<p>So far, we have studied relationships between continuous variables. In practice, linear regression allows to also study relationship between <strong>continuous dependent variables</strong> and <strong>qualitative independent variables</strong>. We will consider another dataset similar to the <strong>Auto MPG</strong> dataset:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mpg</th>
      <th>horsepower</th>
      <th>fuelsystem</th>
      <th>fueltype</th>
      <th>length</th>
      <th>cylinders</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>21</td>
      <td>111.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>168.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21</td>
      <td>111.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>168.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19</td>
      <td>154.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>171.2</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>24</td>
      <td>102.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>176.6</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>18</td>
      <td>115.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>176.6</td>
      <td>5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>200</th>
      <td>23</td>
      <td>114.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>201</th>
      <td>19</td>
      <td>160.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>4</td>
    </tr>
    <tr>
      <th>202</th>
      <td>18</td>
      <td>134.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>203</th>
      <td>26</td>
      <td>106.0</td>
      <td>idi</td>
      <td>diesel</td>
      <td>188.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>204</th>
      <td>19</td>
      <td>114.0</td>
      <td>mpfi</td>
      <td>gas</td>
      <td>188.8</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>205 rows × 6 columns</p>
</div></div></div>
</div>
<p>In this case, besides having numerical variables, we also have qualitative ones such as <code class="docutils literal notranslate"><span class="pre">fuelsystem</span></code> and <code class="docutils literal notranslate"><span class="pre">fueltype</span></code>. Let’s see what are their unique values:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fuel System: [&#39;mpfi&#39; &#39;2bbl&#39; &#39;mfi&#39; &#39;1bbl&#39; &#39;spfi&#39; &#39;4bbl&#39; &#39;idi&#39; &#39;spdi&#39;]
Fuel Type: [&#39;gas&#39; &#39;diesel&#39;]
</pre></div>
</div>
</div>
</div>
<p>We will not see the meaning of all the values of <code class="docutils literal notranslate"><span class="pre">fuelsystem</span></code>, while the values of <code class="docutils literal notranslate"><span class="pre">fueltype</span></code> are self-explanatory.</p>
<section id="predictors-with-only-two-levels">
<h3><span class="section-number">18.5.1. </span>Predictors with Only Two Levels<a class="headerlink" href="#predictors-with-only-two-levels" title="Permalink to this heading">#</a></h3>
<p>We will first see the case in which qualitative predictors only have two levels. To handle these as independent variables, we can define a new <strong>dummy variable</strong> which will encode <span class="math notranslate nohighlight">\(1\)</span> as one of the two levels and <span class="math notranslate nohighlight">\(0\)</span> as the other one. For instance, we can introduce a <code class="docutils literal notranslate"><span class="pre">fueltype[T.gas]</span></code> variable defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}fueltype[T.gas] = \begin{cases} 1 &amp; \text{if } fueltype=gas \\ 0 &amp; \text{otherwise}\end{cases}\end{split}\]</div>
<p>If we fit the model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_1 fueltype[T.gas]\]</div>
<p>We obtain an <span class="math notranslate nohighlight">\(R^2=0.661\)</span> with <span class="math notranslate nohighlight">\(Prob(F-statistic) \approx 0\)</span> and the following estimates for the regression parameters:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   41.2379</td> <td>    1.039</td> <td>   39.705</td> <td> 0.000</td> <td>   39.190</td> <td>   43.286</td>
</tr>
<tr>
  <th>fueltype[T.gas]</th> <td>   -2.7658</td> <td>    0.918</td> <td>   -3.013</td> <td> 0.003</td> <td>   -4.576</td> <td>   -0.956</td>
</tr>
<tr>
  <th>horsepower</th>      <td>   -0.1295</td> <td>    0.007</td> <td>  -18.758</td> <td> 0.000</td> <td>   -0.143</td> <td>   -0.116</td>
</tr>
</table></div></div>
</div>
<p>How do we interpret this result?</p>
<ul class="simple">
<li><p>The value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> <code class="docutils literal notranslate"><span class="pre">fueltype=diesel</span></code> (i.e., <code class="docutils literal notranslate"><span class="pre">fueltype[T.gas]=0</span></code>) is <span class="math notranslate nohighlight">\(41.2379\)</span>;</p></li>
<li><p>An increase of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is associated to a decrease of <span class="math notranslate nohighlight">\(0.1295\)</span> units of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> provided that <code class="docutils literal notranslate"><span class="pre">fueltype=diesel</span></code>;</p></li>
<li><p>For gas vehicles we expect to see a decrease of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> equal to <span class="math notranslate nohighlight">\(2.7658\)</span> with respect to diesel vehicles.</p></li>
</ul>
</section>
<section id="predictors-with-more-than-two-levels">
<h3><span class="section-number">18.5.2. </span>Predictors with More than Two Levels<a class="headerlink" href="#predictors-with-more-than-two-levels" title="Permalink to this heading">#</a></h3>
<p>When predictors have <span class="math notranslate nohighlight">\(n\)</span> levels, we need to introduce <strong>multiple dummy variables</strong>. Specifically, we need to introduce <span class="math notranslate nohighlight">\(n-1\)</span> binary variables. For instance, if the levels of the variable <code class="docutils literal notranslate"><span class="pre">income</span></code> are <code class="docutils literal notranslate"><span class="pre">low</span></code>, <code class="docutils literal notranslate"><span class="pre">medium</span></code> and <code class="docutils literal notranslate"><span class="pre">high</span></code>, we could introduce two variables <code class="docutils literal notranslate"><span class="pre">income[T.low]</span></code> and <code class="docutils literal notranslate"><span class="pre">income[T.medium]</span></code>. These are sufficient to express all possible values of <code class="docutils literal notranslate"><span class="pre">income</span></code> as shown in the table below:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><code class="docutils literal notranslate"><span class="pre">income</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">income[T.low]</span></code></p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">income[T.medium]</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">low</span></code></p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">medium</span></code></p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">high</span></code></p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Note that we could have introduced a new variable <code class="docutils literal notranslate"><span class="pre">income[T.high]</span></code> but this would have been redundant and so <strong>correlated to the other two variables</strong>, which is something we know we have to avoid in linear regression.</p>
<p>If we fit the model which predicts <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">fuelsystem</span></code> we obtain <span class="math notranslate nohighlight">\(R^2=0.734\)</span>, <span class="math notranslate nohighlight">\(Prob(F-statistic) \approx 0\)</span> and the following estimates for the regression coefficients:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>          <td>   38.8638</td> <td>    1.234</td> <td>   31.504</td> <td> 0.000</td> <td>   36.431</td> <td>   41.297</td>
</tr>
<tr>
  <th>fuelsystem[T.2bbl]</th> <td>   -1.6374</td> <td>    1.127</td> <td>   -1.453</td> <td> 0.148</td> <td>   -3.860</td> <td>    0.585</td>
</tr>
<tr>
  <th>fuelsystem[T.4bbl]</th> <td>  -12.0875</td> <td>    2.263</td> <td>   -5.341</td> <td> 0.000</td> <td>  -16.551</td> <td>   -7.624</td>
</tr>
<tr>
  <th>fuelsystem[T.idi]</th>  <td>   -0.3894</td> <td>    1.300</td> <td>   -0.299</td> <td> 0.765</td> <td>   -2.954</td> <td>    2.175</td>
</tr>
<tr>
  <th>fuelsystem[T.mfi]</th>  <td>   -5.8285</td> <td>    3.661</td> <td>   -1.592</td> <td> 0.113</td> <td>  -13.049</td> <td>    1.392</td>
</tr>
<tr>
  <th>fuelsystem[T.mpfi]</th> <td>   -5.4942</td> <td>    1.202</td> <td>   -4.570</td> <td> 0.000</td> <td>   -7.865</td> <td>   -3.123</td>
</tr>
<tr>
  <th>fuelsystem[T.spdi]</th> <td>   -5.2446</td> <td>    1.612</td> <td>   -3.254</td> <td> 0.001</td> <td>   -8.423</td> <td>   -2.066</td>
</tr>
<tr>
  <th>fuelsystem[T.spfi]</th> <td>   -6.1522</td> <td>    3.615</td> <td>   -1.702</td> <td> 0.090</td> <td>  -13.282</td> <td>    0.978</td>
</tr>
<tr>
  <th>horsepower</th>         <td>   -0.0968</td> <td>    0.009</td> <td>  -11.248</td> <td> 0.000</td> <td>   -0.114</td> <td>   -0.080</td>
</tr>
</table></div></div>
</div>
<p>As we can see, we have added different correlation coefficients in order to deal with the different levels. Not all predictors have a low p-value, so we can remove those with backward elimination. We will see some more examples in the laboratory.</p>
</section>
</section>
<section id="extensions-of-the-linear-model">
<h2><span class="section-number">18.6. </span>Extensions of the Linear Model<a class="headerlink" href="#extensions-of-the-linear-model" title="Permalink to this heading">#</a></h2>
<p>The linear model makes the very restrictive assumption that the the nature of the relationship between the variables is linear. However, in many cases, it is common to find relationships which deviate from this assumption. In the following sections, we will see some simple ways to deviate from these assumption within a linear regression model.</p>
<section id="interaction-terms">
<h3><span class="section-number">18.6.1. </span>Interaction Terms<a class="headerlink" href="#interaction-terms" title="Permalink to this heading">#</a></h3>
<p>A linear regression model assumes that <strong>the effect of the different independent variables to the prediction of the dependent variable is additive</strong>:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n + \epsilon\]</div>
<p>In some cases, however, it makes sense to assume the presence of <strong>interaction terms</strong>, i.e., terms in the model which account for the interactions between some variables. For instance:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1x_2\]</div>
<p>As a concrete example, consider the problem of regressing <code class="docutils literal notranslate"><span class="pre">mpg</span></code> from <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code>. A simple model of the kind:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_1 weight\]</div>
<p>assumes that the <span class="math notranslate nohighlight">\(\beta_1\)</span>, the increment that we observe in <code class="docutils literal notranslate"><span class="pre">mpg</span></code> when <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> increments by one unit is <strong>constant even when other variables change their values</strong>. However, we can imagine how, for light vehicles <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> affects <code class="docutils literal notranslate"><span class="pre">mpg</span></code> in a way, while for heavy vehicles <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> affects <code class="docutils literal notranslate"><span class="pre">mpg</span></code> in a different way. For example, we expect light vehicles with big <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> to be more efficient than heavy vehicles with small <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>. To account for this, we could consider the following model instead:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight + \beta_3 horsepower \times weight\]</div>
<p>Note that the model above is nonlinear in <span class="math notranslate nohighlight">\(horsepower\)</span> and <span class="math notranslate nohighlight">\(weight\)</span>, but it is still linear if we introduce a variable <span class="math notranslate nohighlight">\(hw = horsepower \times weight\)</span>. We can easily do this by adding a new column to our design matrix computed as the product of the two variables. Then we can fit the model using the same exact estimators seen before.</p>
<p>This new regressor obtained <span class="math notranslate nohighlight">\(R^2=0.748\)</span>, which is larger as compared to <span class="math notranslate nohighlight">\(R^2=0.706\)</span> obtained for the <span class="math notranslate nohighlight">\(mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\)</span> regressor. Both regressor have a large F-statistic and an associated p-value close to zero. The new regressor explains more variance than the previous one.</p>
<p>The estimated coefficients are:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>         <td>   63.5579</td> <td>    2.343</td> <td>   27.127</td> <td> 0.000</td> <td>   58.951</td> <td>   68.164</td>
</tr>
<tr>
  <th>horsepower</th>        <td>   -0.2508</td> <td>    0.027</td> <td>   -9.195</td> <td> 0.000</td> <td>   -0.304</td> <td>   -0.197</td>
</tr>
<tr>
  <th>weight</th>            <td>   -0.0108</td> <td>    0.001</td> <td>  -13.921</td> <td> 0.000</td> <td>   -0.012</td> <td>   -0.009</td>
</tr>
<tr>
  <th>horsepower:weight</th> <td> 5.355e-05</td> <td> 6.65e-06</td> <td>    8.054</td> <td> 0.000</td> <td> 4.05e-05</td> <td> 6.66e-05</td>
</tr>
</table></div></div>
</div>
<p>Let’s compare these with the ones obtained for the <span class="math notranslate nohighlight">\(mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight\)</span> regressor:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>  <td>   45.6402</td> <td>    0.793</td> <td>   57.540</td> <td> 0.000</td> <td>   44.081</td> <td>   47.200</td>
</tr>
<tr>
  <th>horsepower</th> <td>   -0.0473</td> <td>    0.011</td> <td>   -4.267</td> <td> 0.000</td> <td>   -0.069</td> <td>   -0.026</td>
</tr>
<tr>
  <th>weight</th>     <td>   -0.0058</td> <td>    0.001</td> <td>  -11.535</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.005</td>
</tr>
</table></div></div>
</div>
<p>We can note that:</p>
<ul class="simple">
<li><p>The p-value of the product <span class="math notranslate nohighlight">\(horsepower \times weight\)</span> is almost zero. This is a strong evidence that the true relationship is not merely additive, but an interaction between the two variables actually happens.</p></li>
<li><p>The coefficients of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> have changed. This makes sense, also their interpretation has changed. For instance, in the new regressor an increase of one unit of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is associated to an decrease of <span class="math notranslate nohighlight">\(0.2508\)</span> units of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> if <strong>the product between <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> does not change</strong>, i.e., if the way the two quantities interact does not change.</p></li>
</ul>
<p>How should we interpret the coefficient <span class="math notranslate nohighlight">\(\beta_3\)</span>? Let’s rewrite our model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 weight + \beta_3 horsepower \times weight\]</div>
<p>as follows:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + horsepower (\beta_1 + \beta_3 weight) + \beta_2 weight \]</div>
<p>Hence we can see <span class="math notranslate nohighlight">\(\beta_3\)</span> as the increase in the coefficient of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> when <code class="docutils literal notranslate"><span class="pre">weight</span></code> increases by one unit. In the example above <span class="math notranslate nohighlight">\(\beta_3\)</span> is very small, this is due to the fact that weight is measured in pounds, so increasing the weight by one pound has a very small effect. It easy to see that, <strong>an increase in <code class="docutils literal notranslate"><span class="pre">weight</span></code> by <span class="math notranslate nohighlight">\(1000\)</span> pounds increases the coefficient of <span class="math notranslate nohighlight">\(horsepower\)</span> by <span class="math notranslate nohighlight">\(1000 \beta_3 \approx = 0.05\)</span></strong>. This is a relevant increase, considering that the coefficient of <span class="math notranslate nohighlight">\(horsepower\)</span> is <span class="math notranslate nohighlight">\(-0.2508\)</span>. Hence, when we see an increase in <code class="docutils literal notranslate"><span class="pre">weight</span></code> of <span class="math notranslate nohighlight">\(1000\)</span> units, the coefficient of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> will be larger, meaning that <strong>in heavier cars, large <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> will let <code class="docutils literal notranslate"><span class="pre">mpg</span></code> decrease more gently</strong>.</p>
<p>Let us consider the computed coefficients again:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>         <td>   63.5579</td> <td>    2.343</td> <td>   27.127</td> <td> 0.000</td> <td>   58.951</td> <td>   68.164</td>
</tr>
<tr>
  <th>horsepower</th>        <td>   -0.2508</td> <td>    0.027</td> <td>   -9.195</td> <td> 0.000</td> <td>   -0.304</td> <td>   -0.197</td>
</tr>
<tr>
  <th>weight</th>            <td>   -0.0108</td> <td>    0.001</td> <td>  -13.921</td> <td> 0.000</td> <td>   -0.012</td> <td>   -0.009</td>
</tr>
<tr>
  <th>horsepower:weight</th> <td> 5.355e-05</td> <td> 6.65e-06</td> <td>    8.054</td> <td> 0.000</td> <td> 4.05e-05</td> <td> 6.66e-05</td>
</tr>
</table></div></div>
</div>
<p>We can interpret the results as follows:</p>
<ul class="simple">
<li><p><strong>Intercept</strong>: the value of <code class="docutils literal notranslate"><span class="pre">mpg</span></code> is <span class="math notranslate nohighlight">\(63.5579\)</span> when <code class="docutils literal notranslate"><span class="pre">horsepower=0</span></code> and <code class="docutils literal notranslate"><span class="pre">weight=0</span></code> (also their product will be zero);</p></li>
<li><p><strong>horsepower</strong>: an increase of one unit corresponds to a decrement of <span class="math notranslate nohighlight">\(-0.2508\)</span> in <code class="docutils literal notranslate"><span class="pre">mpg</span></code> provided that <code class="docutils literal notranslate"><span class="pre">weight</span></code> is held constant and the way <code class="docutils literal notranslate"><span class="pre">weight</span></code> influences <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> does not change. Note that, if we change the value of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> and keep <code class="docutils literal notranslate"><span class="pre">weight</span></code> constant, the interaction term will change. Here we assume that, even if the ratio change, we assume that the way <code class="docutils literal notranslate"><span class="pre">weight</span></code> affects <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> does not change.</p></li>
<li><p><strong>weight</strong>: an increase of one unit corresponds to a decrement of <span class="math notranslate nohighlight">\(-0.0108\)</span> in <code class="docutils literal notranslate"><span class="pre">mpg</span></code> provided that <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> is held constant and the way <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> affects <code class="docutils literal notranslate"><span class="pre">weight</span></code> does not change.</p></li>
<li><p><strong>horsepower:weight</strong>: an increase in one unit of <code class="docutils literal notranslate"><span class="pre">weight</span></code> increases the coefficient regulating the effect of <code class="docutils literal notranslate"><span class="pre">horsepower</span></code> on <code class="docutils literal notranslate"><span class="pre">mpg</span></code> by <span class="math notranslate nohighlight">\(5.05e-5\)</span>.</p></li>
</ul>
</section>
<section id="non-linear-relationships-quadratic-and-polynomial-regression">
<h3><span class="section-number">18.6.2. </span>Non-linear Relationships: Quadratic and Polynomial Regression<a class="headerlink" href="#non-linear-relationships-quadratic-and-polynomial-regression" title="Permalink to this heading">#</a></h3>
<p>In many cases, the relationship between two variables is not linear. Let’s visualize the scatterplot between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/00f2868fa38957995abd2a520a22ea2428ba3dceb185f71054c3de10fb434315.png" src="../_images/00f2868fa38957995abd2a520a22ea2428ba3dceb185f71054c3de10fb434315.png" />
</div>
</div>
<p>This relationship does not look linear. It looks instead as a quadratic, which would be fit by the following model:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2\]</div>
<p>Again, the model above is nonlinear in <code class="docutils literal notranslate"><span class="pre">horsepower</span></code>, but we can still fit it with a linear regressor if we add a new variable <span class="math notranslate nohighlight">\(z=horsepower\)</span>.</p>
<p>The fit model will obtain <span class="math notranslate nohighlight">\(R^2=0.688\)</span>, larger than <span class="math notranslate nohighlight">\(R^2=608\)</span> obtained by the base model (<span class="math notranslate nohighlight">\(mpg = \beta_0 + \beta_1 horsepower\)</span>). Both have a large F-statistic.</p>
<p>The estimated coefficients are:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<tr>
           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>          <td>   56.9001</td> <td>    1.800</td> <td>   31.604</td> <td> 0.000</td> <td>   53.360</td> <td>   60.440</td>
</tr>
<tr>
  <th>horsepower</th>         <td>   -0.4662</td> <td>    0.031</td> <td>  -14.978</td> <td> 0.000</td> <td>   -0.527</td> <td>   -0.405</td>
</tr>
<tr>
  <th>I(horsepower ** 2)</th> <td>    0.0012</td> <td>    0.000</td> <td>   10.080</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>
</tr>
</table></div></div>
</div>
<p>The coefficients now describe the quadratic:</p>
<div class="math notranslate nohighlight">
\[y = 58.9001 - 0.4662 x + 0.0012 x^2\]</div>
<p>If we plot it on the data, we obtain the following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e738b8873300d9c7544d5c714998aa9325b362d565b7e27ed58babb98791b19b.png" src="../_images/e738b8873300d9c7544d5c714998aa9325b362d565b7e27ed58babb98791b19b.png" />
</div>
</div>
<p>In general, we can fit a polynomial model to the data, choosing a suitable degree <span class="math notranslate nohighlight">\(d\)</span>. For instance, for <span class="math notranslate nohighlight">\(d=4\)</span> we have:</p>
<div class="math notranslate nohighlight">
\[mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2 + \beta_3 horsepower^3 + \beta_4 horsepower^4\]</div>
<p>which identifies the following fit:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c633f07afcf8225e73d306c1a82a5cc2ba213db2ffbce86f01d0fb2426676b44.png" src="../_images/c633f07afcf8225e73d306c1a82a5cc2ba213db2ffbce86f01d0fb2426676b44.png" />
</div>
</div>
<p>This approach is known as <strong>polynomial regression</strong> and allows to turn the linear regression into a nonlinear model. Note that, when we have more variables, polynomial regression also includes interaction terms. For instance, the linear model:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x + \beta_2y\]</div>
<p>becomes the following polynomial model of degree <span class="math notranslate nohighlight">\(2\)</span>:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x + \beta_2y + \beta_3x^2 + \beta_4y^2 + \beta_5 xy\]</div>
<p>As usual, we only have to add new variables for the squared and interaction term and solve the problem as a linear regression one. This is easily handled by libraries. Note that, as the number of variables increases, the number of terms to add for a given degree also increases.</p>
</section>
</section>
<section id="residual-plots-and-residual-q-q-plots">
<h2><span class="section-number">18.7. </span>Residual Plots and Residual Q-Q Plots<a class="headerlink" href="#residual-plots-and-residual-q-q-plots" title="Permalink to this heading">#</a></h2>
<p>Residual plots are a way to diagnose if the linear model we are fitting on the data actually describes the relationship between the variables. For instance, if the relationship is not linear, and we try to force a linear model, the model will not be accurate.</p>
<p>If the relation is accurate, we expect the residuals of the model (i.e., the <span class="math notranslate nohighlight">\(e_i=\hat y_i - y_i\)</span> terms) to be random and approximately Gaussian.</p>
<p><strong>To check if the residuals are random, it is common to show a residual plot</strong>, which plots the residuals on the <span class="math notranslate nohighlight">\(y\)</span> axis and the true <span class="math notranslate nohighlight">\(y\)</span> values on the y axis. We expect to observe a cloud of points centered around the zero  with no specific patters.</p>
<p><strong>To check if the residuals are Gaussian, we can show Q-Q plots</strong></p>
<p>The graph below compares residual plots and Q-Q Plots of the residuals of different regression models:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/f65885ac9e0db799bbbcb7812e268cc6347ca6459af19c90e5a0ecaebb21ed58.png" src="../_images/f65885ac9e0db799bbbcb7812e268cc6347ca6459af19c90e5a0ecaebb21ed58.png" />
</div>
</div>
<p>From the plot, we can see that some models have residuals less correlated with the predicted variable and quantiles closer to the normal distribution. This happens when the model explains better the variance of the predicted variable. In particular, we can note that the quadratic model (second row) and the one with the interaction term (last row) are a much better fit than the purely linear models (other two rows).</p>
</section>
<section id="collinearity-and-regularization-techniques">
<h2><span class="section-number">18.8. </span>Collinearity and Regularization Techniques<a class="headerlink" href="#collinearity-and-regularization-techniques" title="Permalink to this heading">#</a></h2>
<p>When fitting a multiple linear regressor, we should bear in mind that having two very correlated variables can lead to a poor model. For instance, in the case of perfectly correlated variables, two rows of the <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> matrix will be linearly dependent (one can be computed as a linear function of the other), hence the determinant of <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> will be zero, the matrix cannot be inverted, and the coefficients cannot be estimated.</p>
<p>Even if two variables are nearly collinear (strongly correlated), the <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> matrix will be ill conditioned and the parameters will be estimated with high numerical imprecision.</p>
<p>In general, when there is collinearity between two variables (or even <strong>multicollinearity</strong>, involving more than two variables), <strong>the results of the multiple linear regressor cannot be trusted</strong>. In such cases, we can try to remove the correlated variables (we can identify correlated variables with the correlation matrix), apply regularization techniques (discussed later) or perform Principal Component Analysis (PCA), which aims to provide a set of decorrelated features from <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. The PCA method will be presented later.</p>
<section id="ridge-regression">
<h3><span class="section-number">18.8.1. </span>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this heading">#</a></h3>
<p>One way to remove collinearity is to <strong>remove those variables which are highly correlated with other variables</strong>. This will effectively remove collinearity and solve the instability issues of linear regression. However, when there is a large number of predictors, this approach may not be very convenient. Also, it is not always clear which predictors to remove and which ones to keep, especially in the case of <strong>multicollinearity</strong> (sets of more than two variables which are highly correlated).</p>
<p>A “<strong>soft approach</strong>” would be to feed all variables to the model and <strong>encourage it to set some of the coefficients to zero</strong>. If we could do this, the model would <strong>effectively select which variables to exclude from the model</strong>, hence releasing us from making a hard decision.</p>
<p>This can be done by changing our cost function, the RSS. Recall that the RSS of linear regression (Ordinary Least Squares) can be written as:</p>
<div class="math notranslate nohighlight">
\[RSS = \sum_{i=1}^m (y_i - \beta_0 - \sum_{j=1}^n \beta_j x_{ij})^2\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{ij}\)</span> is the value of the <span class="math notranslate nohighlight">\(x_j\)</span> variable in the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation.</p>
<p>We can write the cost function as follows:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^m (y_i - \beta_0 - \sum_{j=1}^n \beta_j x_{ij})^2 + \lambda \sum_{j=1}^n \beta_j^2 = RSS + \lambda \sum_{j=1}^n \beta_j^2\]</div>
<p>In practice, we are adding the following term:</p>
<p><span class="math notranslate nohighlight">\(\sum_{j=1}^n \beta_j^2\)</span></p>
<p>weighted by a parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. The term above <strong>penalizes solutions with large values for the coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>, hence encouraging the model to put some of the variables to zero, or close to zero</strong>. This term is called “regularization term” and a linear regressor fit by minimizing this new cost function is called a <strong>ridge regressor</strong>.</p>
<p>The cost function has a parameter <span class="math notranslate nohighlight">\(\lambda\)</span> which must be set as a constant. These kinds of parameters which we need to specify manually and are not computed from the data, are called <strong>hyperparameters</strong>. In practice, <span class="math notranslate nohighlight">\(\lambda\)</span> controls <strong>how much we want to regularize the linear regressor</strong>. The higher the value of <span class="math notranslate nohighlight">\(\lambda\)</span>, the stronger the regularization will be, hence encouraging the model to select small values of <span class="math notranslate nohighlight">\(\beta_j\)</span> or set some of them to zero.</p>
<p>To avoid <span class="math notranslate nohighlight">\(\lambda\)</span> having different effects depending on the unit measures of the given variables, it is common to <strong>z-score all variables before computing a ridge regressor</strong>.</p>
<p>This is shown in the following plot, which shows the values of the parameters <span class="math notranslate nohighlight">\(\beta_j\)</span> when a ridge regression model is fit with different values of <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/fd3880ceafc33f42cf09494bcb5ed81c6e71e85916ac64e3be39f6dea62356e3.png" src="../_images/fd3880ceafc33f42cf09494bcb5ed81c6e71e85916ac64e3be39f6dea62356e3.png" />
</div>
</div>
<p>As can be seen, the coefficient values are gradually shrunk towards zero as the value of <span class="math notranslate nohighlight">\(\lambda\)</span> increases. Interestingly, the model <strong>“decides”</strong> which coefficients to shrink. For instance, for low values of <span class="math notranslate nohighlight">\(\lambda\)</span>, <code class="docutils literal notranslate"><span class="pre">weight</span></code> is shrunk, while <code class="docutils literal notranslate"><span class="pre">acceleration</span></code> is first shrunk, then allowed to be larger than zero. This is due to the fact that, the regularization term acts as a sort of “soft constraint” encouraging the model to find smaller weights, while still finding a good solution.</p>
<p>It can be shown (but we will not see it formally), that ridge regression <strong>reduces the variance of coefficient estimates</strong>. At the same time, <strong>the bias is increased</strong>. So, finding a good value of <span class="math notranslate nohighlight">\(\lambda\)</span> allows to <strong>control the bias-variance trade-off</strong>.</p>
<section id="interpretation-of-the-ridge-regression-coefficients">
<h4><span class="section-number">18.8.1.1. </span>Interpretation of the ridge regression coefficients<a class="headerlink" href="#interpretation-of-the-ridge-regression-coefficients" title="Permalink to this heading">#</a></h4>
<p>Let us compare the parameters obtained through a ridge regressor with those obtained with a linear regressor (OLS):</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ridge_params</th>
      <th>ols_params</th>
    </tr>
    <tr>
      <th>variables</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>displacement</th>
      <td>-0.998445</td>
      <td>2.079303</td>
    </tr>
    <tr>
      <th>cylinders</th>
      <td>-0.969852</td>
      <td>-0.840519</td>
    </tr>
    <tr>
      <th>horsepower</th>
      <td>-1.027231</td>
      <td>-0.651636</td>
    </tr>
    <tr>
      <th>weight</th>
      <td>-1.401530</td>
      <td>-5.492050</td>
    </tr>
    <tr>
      <th>acceleration</th>
      <td>0.199621</td>
      <td>0.222014</td>
    </tr>
    <tr>
      <th>model_year</th>
      <td>1.375600</td>
      <td>2.762119</td>
    </tr>
    <tr>
      <th>origin</th>
      <td>0.830411</td>
      <td>1.147316</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As we can see, the ridge parameters has a smaller scale. This is due to the regularization term. As a result, the parameters <strong>cannot be interpreted statistically as the ones of a linear regressor</strong>. Instead, we can interpret them as denoting the relative importance of the variable to the prediction.</p>
</section>
</section>
<section id="lasso-regression">
<h3><span class="section-number">18.8.2. </span>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this heading">#</a></h3>
<p>The ridge regressor will in general set coefficients exactly to zero only for very large values of <span class="math notranslate nohighlight">\(\lambda\)</span>. An alternative model which has been shown to set coefficients more likely to zero is the <strong>lasso regressor</strong>. The main difference with ridge regression is in the regularization term. The new cost function of a lasso regressor is as follows:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^m (y_i - \beta_0 - \sum_{j=1}^n \beta_j x_{ij})^2 + \lambda \sum_{j=1}^n |\beta_j| = RSS + \lambda \sum_{j=1}^n |\beta_j|\]</div>
<p>We basically replaced the norm of the coefficients with the absolute values (this is called an L1 norm), which has the effect to encourage the model to set values to zero.</p>
<p>The figure below shows how the coefficient estimates change for different values of <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1de2a52c820bc6c4b1de5bd7c81ccce61b4e45a2ff1d88ceff546b121bee77db.png" src="../_images/1de2a52c820bc6c4b1de5bd7c81ccce61b4e45a2ff1d88ceff546b121bee77db.png" />
</div>
</div>
<p>As can be seen, the model now makes “hard choices” on whether a value should be set to zero or not, thus performing variable selection.</p>
</section>
</section>
<section id="references">
<h2><span class="section-number">18.9. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Chapter 3 of [1]</p></li>
<li><p>Parts of chapter 11 of [2]</p></li>
</ul>
<p>[1] Heumann, Christian, and Michael Schomaker Shalabh. Introduction to statistics and data analysis. Springer International Publishing Switzerland, 2016.</p>
<p>[2] James, Gareth Gareth Michael. An introduction to statistical learning: with applications in Python, 2023.<a class="reference external" href="https://www.statlearning.com">https://www.statlearning.com</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_statistical_inference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">16. </span>Statistical Inference</p>
      </div>
    </a>
    <a class="right-next"
       href="12_logistic_regression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Logistic Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-auto-mpg-dataset">18.1. The Auto MPG Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-models">18.2. Regression Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">18.3. Simple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-coefficients-ordinary-least-squares-ols">18.3.1. Estimating the Coefficients - Ordinary Least Squares (OLS)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-coefficients-of-linear-regression">18.3.2. Interpretation of the Coefficients of Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-of-the-coefficient-estimates">18.3.3. Accuracy of the Coefficient Estimates</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-errors-of-the-regression-coefficients">18.3.3.1. Standard Errors of the Regression Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-of-the-regression-coefficients">18.3.3.2. Confidence Intervals of the Regression Coefficients</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-tests-for-the-significance-of-coefficients">18.3.3.3. Statistical Tests for the Significance of Coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-of-the-model">18.3.4. Accuracy of the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-standard-error">18.3.4.1. Residual Standard Error</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#r-2-statistic">18.3.4.2. <span class="math notranslate nohighlight">\(R^2\)</span> Statistic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">18.4. Multiple Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-interpretation">18.4.1. Geometrical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-interpretation">18.4.2. Statistical Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-regression-coefficients">18.4.3. Estimating the Regression Coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-f-test">18.4.4. The F-Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection">18.4.5. Variable Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-2">18.4.6. Adjusted <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-predictors">18.5. Qualitative Predictors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-only-two-levels">18.5.1. Predictors with Only Two Levels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictors-with-more-than-two-levels">18.5.2. Predictors with More than Two Levels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extensions-of-the-linear-model">18.6. Extensions of the Linear Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-terms">18.6.1. Interaction Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-relationships-quadratic-and-polynomial-regression">18.6.2. Non-linear Relationships: Quadratic and Polynomial Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-plots-and-residual-q-q-plots">18.7. Residual Plots and Residual Q-Q Plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collinearity-and-regularization-techniques">18.8. Collinearity and Regularization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">18.8.1. Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-the-ridge-regression-coefficients">18.8.1.1. Interpretation of the ridge regression coefficients</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">18.8.2. Lasso Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">18.9. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonino Furnari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>